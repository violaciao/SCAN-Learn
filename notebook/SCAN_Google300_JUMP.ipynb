{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCAN Add-Prim JUMP Experiment\n",
    "*************************************************************\n",
    "\n",
    "Reference: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Python 3.6\n",
    "* PyTorch 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Device is using\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "TASK_NAME = \"addprim-jump\"\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False, trainOrtest='train'):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines        \n",
    "    lines = open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/processed/{}-{}_{}-{}.txt'.\\\n",
    "                 format(trainOrtest, TASK_NAME, lang1, lang2), encoding='utf-8').\\\n",
    "                 read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "# PRED_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 37046 sentence pairs\n",
      "Trimmed to 37046 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['run opposite right and turn around left thrice', 'I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataFrom='train'):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=False, trainOrtest=dataFrom)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('in', 'out', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "=================\n",
    "\n",
    "The model we are using is a GRU encoder-decoder seq2seq model with attention mechanism. In order to solve the zero-shot generalization task, we embed the encoder networks with pre-trained embeddings, from GloVe and Google Word2Vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_SOURCE = 'google'\n",
    "hidden_size = 300\n",
    "\n",
    "if EMBEDDEING_SOURCE == 'google':\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_GoogleNews300Negative.pkl', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "else:\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_raw{}d.pkl'.format(hidden_size), 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "\n",
    "pretrained_emb = np.zeros((input_lang.n_words, hidden_size))\n",
    "for k, v in input_lang.index2word.items():\n",
    "    if v == 'SOS':\n",
    "        pretrained_emb[k] = np.zeros(hidden_size)\n",
    "    elif (v == 'EOS') and (EMBEDDEING_SOURCE != 'google'):\n",
    "        pretrained_emb[k] = b['.']\n",
    "    elif (v == 'and') and (EMBEDDEING_SOURCE == 'google'):\n",
    "        pretrained_emb[k] = b['AND']\n",
    "    else:\n",
    "        pretrained_emb[k] = b[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of this seq2seq network is a GRU netword. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_PRETRAINED = True\n",
    "WEIGHT_UPDATE = False\n",
    "\n",
    "MODEL_VERSION = 'T0.4_gg300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        if EMBEDDEING_PRETRAINED:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "            self.embedding.weight.requires_grad = WEIGHT_UPDATE\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is a GRU network with attention mechanism that takes the last output of the encoder and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "First we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "We use teacher forcing to help converge faster with a delay fashion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, \n",
    "          max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for timing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, eval_every=1000, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "        encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "        decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "        \n",
    "    best_test_acc = evaluateAccuracy(encoder, decoder, 500)\n",
    "    print(\"Best evaluation accuracy: {0:.2f}%\".format(best_test_acc * 100))\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        \n",
    "    encoder_optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg), end=' ')\n",
    "            \n",
    "            if iter % eval_every == 0:\n",
    "                test_acc = evaluateAccuracy(encoder, decoder, 200)\n",
    "                print('{0:.2f}%'.format(test_acc * 100))\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    with open(\"saved_models/encoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(encoder, f)\n",
    "                    with open(\"saved_models/decoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(decoder, f)\n",
    "                    print(\"New best test accuracy! Model Updated!\")\n",
    "                    best_test_acc = test_acc\n",
    "#                 elif test_acc < best_test_acc - 0.001:\n",
    "#                     encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "#                     decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "                    \n",
    "            else:\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 15412 sentence pairs\n",
      "Trimmed to 15412 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['jump around right and walk opposite right thrice', 'I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_WALK']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs_eval = prepareData('in', 'out', True, dataFrom='test')\n",
    "print(random.choice(pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateAccuracy(encoder, decoder, n=10):\n",
    "    ACCs = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        \n",
    "        if output_words[-1] == '<EOS>':\n",
    "            output_words = output_words[:-1]\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        if output_sentence == pair[1]:\n",
    "            ACCs.append(1)\n",
    "        else:\n",
    "            ACCs.append(0)\n",
    "    return np.array(ACCs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initially trained with a higher teacher aid, and relatively large learning rate. Both teacher forcing effect and the learning rate decay over iterations when the model approaches the optimum.  \n",
    "\n",
    "#### The model achieves 97% accuracy rate for the best test sample evaluation, and is 94% correct on average for the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 0.00%\n",
      "0m 10s (- 17m 53s) (50 1%) 1.8409 \n",
      "0m 16s (- 13m 17s) (100 2%) 1.3678 \n",
      "0m 20s (- 11m 13s) (150 3%) 1.1563 \n",
      "0m 25s (- 10m 14s) (200 4%) 1.4303 \n",
      "0m 30s (- 9m 34s) (250 5%) 1.3190 \n",
      "0m 34s (- 9m 3s) (300 6%) 1.1561 \n",
      "0m 39s (- 8m 44s) (350 7%) 1.0983 \n",
      "0m 43s (- 8m 16s) (400 8%) 1.0989 \n",
      "0m 47s (- 7m 57s) (450 9%) 1.1244 \n",
      "0m 50s (- 7m 38s) (500 10%) 1.0068 0.00%\n",
      "1m 1s (- 8m 18s) (550 11%) 1.0164 \n",
      "1m 6s (- 8m 7s) (600 12%) 0.9592 \n",
      "1m 11s (- 7m 59s) (650 13%) 0.9208 \n",
      "1m 18s (- 8m 3s) (700 14%) 0.8341 \n",
      "1m 23s (- 7m 54s) (750 15%) 1.0649 \n",
      "1m 29s (- 7m 50s) (800 16%) 0.9913 \n",
      "1m 35s (- 7m 46s) (850 17%) 0.9482 \n",
      "1m 42s (- 7m 45s) (900 18%) 0.8757 \n",
      "1m 46s (- 7m 35s) (950 19%) 0.7944 \n",
      "1m 51s (- 7m 27s) (1000 20%) 0.7529 0.50%\n",
      "New best test accuracy! Model Updated!\n",
      "2m 2s (- 7m 40s) (1050 21%) 0.9433 \n",
      "2m 7s (- 7m 31s) (1100 22%) 0.8243 \n",
      "2m 12s (- 7m 23s) (1150 23%) 0.8185 \n",
      "2m 17s (- 7m 14s) (1200 24%) 0.8771 \n",
      "2m 22s (- 7m 8s) (1250 25%) 0.8414 \n",
      "2m 26s (- 6m 58s) (1300 26%) 0.8757 \n",
      "2m 32s (- 6m 51s) (1350 27%) 0.7904 \n",
      "2m 36s (- 6m 43s) (1400 28%) 0.7015 \n",
      "2m 41s (- 6m 35s) (1450 28%) 0.8232 \n",
      "2m 46s (- 6m 28s) (1500 30%) 0.7058 0.00%\n",
      "2m 59s (- 6m 39s) (1550 31%) 0.5292 \n",
      "3m 4s (- 6m 32s) (1600 32%) 0.7495 \n",
      "3m 10s (- 6m 26s) (1650 33%) 0.6000 \n",
      "3m 15s (- 6m 19s) (1700 34%) 0.5988 \n",
      "3m 20s (- 6m 11s) (1750 35%) 0.6379 \n",
      "3m 24s (- 6m 3s) (1800 36%) 0.6823 \n",
      "3m 29s (- 5m 56s) (1850 37%) 0.5871 \n",
      "3m 34s (- 5m 50s) (1900 38%) 0.5158 \n",
      "3m 38s (- 5m 42s) (1950 39%) 0.6679 \n",
      "3m 43s (- 5m 34s) (2000 40%) 0.5612 0.50%\n",
      "3m 52s (- 5m 34s) (2050 41%) 0.4955 \n",
      "3m 56s (- 5m 26s) (2100 42%) 0.5727 \n",
      "4m 0s (- 5m 18s) (2150 43%) 0.5738 \n",
      "4m 4s (- 5m 10s) (2200 44%) 0.5245 \n",
      "4m 7s (- 5m 2s) (2250 45%) 0.5757 \n",
      "4m 11s (- 4m 55s) (2300 46%) 0.4836 \n",
      "4m 15s (- 4m 47s) (2350 47%) 0.4901 \n",
      "4m 19s (- 4m 40s) (2400 48%) 0.5018 \n",
      "4m 23s (- 4m 34s) (2450 49%) 0.5153 \n",
      "4m 29s (- 4m 29s) (2500 50%) 0.4521 2.00%\n",
      "New best test accuracy! Model Updated!\n",
      "4m 38s (- 4m 27s) (2550 51%) 0.4003 \n",
      "4m 42s (- 4m 21s) (2600 52%) 0.4074 \n",
      "4m 47s (- 4m 15s) (2650 53%) 0.5755 \n",
      "4m 52s (- 4m 9s) (2700 54%) 0.5914 \n",
      "4m 57s (- 4m 3s) (2750 55%) 0.6039 \n",
      "5m 2s (- 3m 57s) (2800 56%) 0.6046 \n",
      "5m 6s (- 3m 51s) (2850 56%) 0.4919 \n",
      "5m 10s (- 3m 44s) (2900 57%) 0.4291 \n",
      "5m 15s (- 3m 39s) (2950 59%) 0.3830 \n",
      "5m 19s (- 3m 32s) (3000 60%) 0.4394 1.00%\n",
      "5m 27s (- 3m 29s) (3050 61%) 0.4667 \n",
      "5m 31s (- 3m 23s) (3100 62%) 0.4426 \n",
      "5m 35s (- 3m 17s) (3150 63%) 0.4550 \n",
      "5m 40s (- 3m 11s) (3200 64%) 0.3793 \n",
      "5m 44s (- 3m 5s) (3250 65%) 0.2858 \n",
      "5m 48s (- 2m 59s) (3300 66%) 0.5251 \n",
      "5m 52s (- 2m 53s) (3350 67%) 0.4435 \n",
      "5m 57s (- 2m 48s) (3400 68%) 0.3687 \n",
      "6m 1s (- 2m 42s) (3450 69%) 0.3985 \n",
      "6m 5s (- 2m 36s) (3500 70%) 0.4877 3.50%\n",
      "New best test accuracy! Model Updated!\n",
      "6m 13s (- 2m 32s) (3550 71%) 0.3885 \n",
      "6m 18s (- 2m 27s) (3600 72%) 0.4619 \n",
      "6m 22s (- 2m 21s) (3650 73%) 0.3466 \n",
      "6m 27s (- 2m 16s) (3700 74%) 0.4014 \n",
      "6m 32s (- 2m 10s) (3750 75%) 0.3845 \n",
      "6m 37s (- 2m 5s) (3800 76%) 0.4113 \n",
      "6m 43s (- 2m 0s) (3850 77%) 0.5576 \n",
      "6m 49s (- 1m 55s) (3900 78%) 0.4051 \n",
      "6m 53s (- 1m 49s) (3950 79%) 0.3683 \n",
      "6m 59s (- 1m 44s) (4000 80%) 0.3687 10.00%\n",
      "New best test accuracy! Model Updated!\n",
      "7m 10s (- 1m 40s) (4050 81%) 0.4126 \n",
      "7m 14s (- 1m 35s) (4100 82%) 0.3149 \n",
      "7m 18s (- 1m 29s) (4150 83%) 0.3345 \n",
      "7m 23s (- 1m 24s) (4200 84%) 0.3904 \n",
      "7m 27s (- 1m 18s) (4250 85%) 0.4244 \n",
      "7m 31s (- 1m 13s) (4300 86%) 0.4573 \n",
      "7m 35s (- 1m 8s) (4350 87%) 0.3184 \n",
      "7m 40s (- 1m 2s) (4400 88%) 0.3619 \n",
      "7m 44s (- 0m 57s) (4450 89%) 0.3341 \n",
      "7m 48s (- 0m 52s) (4500 90%) 0.3988 14.50%\n",
      "New best test accuracy! Model Updated!\n",
      "7m 56s (- 0m 47s) (4550 91%) 0.3853 \n",
      "8m 1s (- 0m 41s) (4600 92%) 0.3007 \n",
      "8m 6s (- 0m 36s) (4650 93%) 0.2480 \n",
      "8m 10s (- 0m 31s) (4700 94%) 0.2687 \n",
      "8m 15s (- 0m 26s) (4750 95%) 0.3162 \n",
      "8m 20s (- 0m 20s) (4800 96%) 0.2771 \n",
      "8m 25s (- 0m 15s) (4850 97%) 0.2597 \n",
      "8m 29s (- 0m 10s) (4900 98%) 0.2960 \n",
      "8m 34s (- 0m 5s) (4950 99%) 0.4603 \n",
      "8m 38s (- 0m 0s) (5000 100%) 0.3054 16.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 18.00%\n",
      "0m 16s (- 5m 11s) (50 5%) 0.4941 \n",
      "0m 20s (- 3m 8s) (100 10%) 0.4029 \n",
      "0m 24s (- 2m 17s) (150 15%) 0.4301 \n",
      "0m 27s (- 1m 51s) (200 20%) 0.5234 \n",
      "0m 31s (- 1m 35s) (250 25%) 0.3886 \n",
      "0m 35s (- 1m 23s) (300 30%) 0.4076 \n",
      "0m 39s (- 1m 13s) (350 35%) 0.4116 \n",
      "0m 43s (- 1m 5s) (400 40%) 0.3716 \n",
      "0m 47s (- 0m 57s) (450 45%) 0.3658 \n",
      "0m 50s (- 0m 50s) (500 50%) 0.2636 23.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 57s (- 0m 46s) (550 55%) 0.3923 \n",
      "0m 59s (- 0m 39s) (600 60%) 0.3438 \n",
      "1m 3s (- 0m 34s) (650 65%) 0.3711 \n",
      "1m 7s (- 0m 28s) (700 70%) 0.3209 \n",
      "1m 11s (- 0m 23s) (750 75%) 0.4423 \n",
      "1m 15s (- 0m 18s) (800 80%) 0.4426 \n",
      "1m 18s (- 0m 13s) (850 85%) 0.4209 \n",
      "1m 22s (- 0m 9s) (900 90%) 0.3264 \n",
      "1m 25s (- 0m 4s) (950 95%) 0.2829 \n",
      "1m 28s (- 0m 0s) (1000 100%) 0.3117 19.50%\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 16.40%\n",
      "0m 13s (- 4m 17s) (50 5%) 0.3768 \n",
      "0m 17s (- 2m 36s) (100 10%) 0.3642 \n",
      "0m 20s (- 1m 58s) (150 15%) 0.2782 \n",
      "0m 24s (- 1m 39s) (200 20%) 0.3501 \n",
      "0m 28s (- 1m 25s) (250 25%) 0.4310 \n",
      "0m 31s (- 1m 13s) (300 30%) 0.3726 \n",
      "0m 34s (- 1m 4s) (350 35%) 0.3641 \n",
      "0m 37s (- 0m 56s) (400 40%) 0.4038 \n",
      "0m 41s (- 0m 50s) (450 45%) 0.3018 \n",
      "0m 45s (- 0m 45s) (500 50%) 0.3922 13.00%\n",
      "0m 53s (- 0m 43s) (550 55%) 0.4571 \n",
      "0m 57s (- 0m 38s) (600 60%) 0.2379 \n",
      "1m 1s (- 0m 33s) (650 65%) 0.3253 \n",
      "1m 5s (- 0m 27s) (700 70%) 0.3545 \n",
      "1m 9s (- 0m 23s) (750 75%) 0.3274 \n",
      "1m 14s (- 0m 18s) (800 80%) 0.3917 \n",
      "1m 18s (- 0m 13s) (850 85%) 0.3644 \n",
      "1m 23s (- 0m 9s) (900 90%) 0.3851 \n",
      "1m 28s (- 0m 4s) (950 95%) 0.4378 \n",
      "1m 32s (- 0m 0s) (1000 100%) 0.3949 20.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 20.40%\n",
      "0m 15s (- 4m 52s) (50 5%) 0.3883 \n",
      "0m 19s (- 2m 56s) (100 10%) 0.3780 \n",
      "0m 23s (- 2m 10s) (150 15%) 0.4593 \n",
      "0m 26s (- 1m 46s) (200 20%) 0.3323 \n",
      "0m 30s (- 1m 32s) (250 25%) 0.3254 \n",
      "0m 34s (- 1m 20s) (300 30%) 0.3944 \n",
      "0m 38s (- 1m 11s) (350 35%) 0.4481 \n",
      "0m 42s (- 1m 3s) (400 40%) 0.4148 \n",
      "0m 46s (- 0m 57s) (450 45%) 0.3838 \n",
      "0m 50s (- 0m 50s) (500 50%) 0.3132 22.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 58s (- 0m 47s) (550 55%) 0.2395 \n",
      "1m 2s (- 0m 41s) (600 60%) 0.3002 \n",
      "1m 5s (- 0m 35s) (650 65%) 0.2523 \n",
      "1m 10s (- 0m 30s) (700 70%) 0.3210 \n",
      "1m 14s (- 0m 24s) (750 75%) 0.2427 \n",
      "1m 17s (- 0m 19s) (800 80%) 0.3313 \n",
      "1m 21s (- 0m 14s) (850 85%) 0.3432 \n",
      "1m 25s (- 0m 9s) (900 90%) 0.3127 \n",
      "1m 29s (- 0m 4s) (950 95%) 0.2886 \n",
      "1m 33s (- 0m 0s) (1000 100%) 0.2132 23.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 24.00%\n",
      "0m 14s (- 4m 35s) (50 5%) 0.2055 \n",
      "0m 18s (- 2m 47s) (100 10%) 0.2915 \n",
      "0m 22s (- 2m 8s) (150 15%) 0.3316 \n",
      "0m 26s (- 1m 47s) (200 20%) 0.3369 \n",
      "0m 31s (- 1m 33s) (250 25%) 0.3257 \n",
      "0m 35s (- 1m 22s) (300 30%) 0.3602 \n",
      "0m 38s (- 1m 11s) (350 35%) 0.2130 \n",
      "0m 42s (- 1m 4s) (400 40%) 0.2990 \n",
      "0m 46s (- 0m 56s) (450 45%) 0.3939 \n",
      "0m 50s (- 0m 50s) (500 50%) 0.3390 25.50%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 0s (- 0m 49s) (550 55%) 0.3873 \n",
      "1m 4s (- 0m 42s) (600 60%) 0.2341 \n",
      "1m 7s (- 0m 36s) (650 65%) 0.3750 \n",
      "1m 11s (- 0m 30s) (700 70%) 0.3211 \n",
      "1m 15s (- 0m 25s) (750 75%) 0.3049 \n",
      "1m 19s (- 0m 19s) (800 80%) 0.2484 \n",
      "1m 24s (- 0m 14s) (850 85%) 0.2174 \n",
      "1m 28s (- 0m 9s) (900 90%) 0.2939 \n",
      "1m 32s (- 0m 4s) (950 95%) 0.2548 \n",
      "1m 36s (- 0m 0s) (1000 100%) 0.2174 30.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 27.40%\n",
      "0m 14s (- 4m 28s) (50 5%) 0.2138 \n",
      "0m 18s (- 2m 43s) (100 10%) 0.3305 \n",
      "0m 22s (- 2m 6s) (150 15%) 0.1774 \n",
      "0m 26s (- 1m 44s) (200 20%) 0.2059 \n",
      "0m 29s (- 1m 29s) (250 25%) 0.1545 \n",
      "0m 34s (- 1m 19s) (300 30%) 0.3385 \n",
      "0m 38s (- 1m 11s) (350 35%) 0.1867 \n",
      "0m 42s (- 1m 4s) (400 40%) 0.2761 \n",
      "0m 47s (- 0m 58s) (450 45%) 0.2276 \n",
      "0m 51s (- 0m 51s) (500 50%) 0.2262 39.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 59s (- 0m 48s) (550 55%) 0.2733 \n",
      "1m 3s (- 0m 42s) (600 60%) 0.2091 \n",
      "1m 7s (- 0m 36s) (650 65%) 0.2506 \n",
      "1m 10s (- 0m 30s) (700 70%) 0.2209 \n",
      "1m 14s (- 0m 24s) (750 75%) 0.2089 \n",
      "1m 17s (- 0m 19s) (800 80%) 0.1850 \n",
      "1m 21s (- 0m 14s) (850 85%) 0.1786 \n",
      "1m 25s (- 0m 9s) (900 90%) 0.2017 \n",
      "1m 29s (- 0m 4s) (950 95%) 0.1565 \n",
      "1m 33s (- 0m 0s) (1000 100%) 0.1601 45.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 46.80%\n",
      "0m 13s (- 4m 8s) (50 5%) 0.2971 \n",
      "0m 16s (- 2m 25s) (100 10%) 0.1704 \n",
      "0m 19s (- 1m 53s) (150 15%) 0.2576 \n",
      "0m 23s (- 1m 35s) (200 20%) 0.0989 \n",
      "0m 27s (- 1m 23s) (250 25%) 0.2374 \n",
      "0m 32s (- 1m 16s) (300 30%) 0.1630 \n",
      "0m 38s (- 1m 10s) (350 35%) 0.2024 \n",
      "0m 43s (- 1m 4s) (400 40%) 0.1583 \n",
      "0m 46s (- 0m 56s) (450 45%) 0.1418 \n",
      "0m 50s (- 0m 50s) (500 50%) 0.1388 52.50%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 2s (- 0m 51s) (550 55%) 0.2873 \n",
      "1m 7s (- 0m 45s) (600 60%) 0.1489 \n",
      "1m 11s (- 0m 38s) (650 65%) 0.2218 \n",
      "1m 16s (- 0m 32s) (700 70%) 0.1651 \n",
      "1m 20s (- 0m 26s) (750 75%) 0.1274 \n",
      "1m 24s (- 0m 21s) (800 80%) 0.2083 \n",
      "1m 28s (- 0m 15s) (850 85%) 0.2011 \n",
      "1m 33s (- 0m 10s) (900 90%) 0.1716 \n",
      "1m 37s (- 0m 5s) (950 95%) 0.1793 \n",
      "1m 41s (- 0m 0s) (1000 100%) 0.1482 48.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 51.40%\n",
      "0m 13s (- 4m 9s) (50 5%) 0.1805 \n",
      "0m 16s (- 2m 31s) (100 10%) 0.1852 \n",
      "0m 20s (- 1m 58s) (150 15%) 0.1226 \n",
      "0m 24s (- 1m 39s) (200 20%) 0.1361 \n",
      "0m 29s (- 1m 27s) (250 25%) 0.1556 \n",
      "0m 32s (- 1m 16s) (300 30%) 0.1276 \n",
      "0m 36s (- 1m 8s) (350 35%) 0.1370 \n",
      "0m 40s (- 1m 1s) (400 40%) 0.1657 \n",
      "0m 44s (- 0m 54s) (450 45%) 0.0982 \n",
      "0m 48s (- 0m 48s) (500 50%) 0.0849 47.50%\n",
      "0m 55s (- 0m 45s) (550 55%) 0.1171 \n",
      "0m 59s (- 0m 39s) (600 60%) 0.0808 \n",
      "1m 3s (- 0m 34s) (650 65%) 0.1705 \n",
      "1m 7s (- 0m 28s) (700 70%) 0.1722 \n",
      "1m 10s (- 0m 23s) (750 75%) 0.0980 \n",
      "1m 14s (- 0m 18s) (800 80%) 0.1280 \n",
      "1m 18s (- 0m 13s) (850 85%) 0.1184 \n",
      "1m 21s (- 0m 9s) (900 90%) 0.1153 \n",
      "1m 25s (- 0m 4s) (950 95%) 0.1021 \n",
      "1m 29s (- 0m 0s) (1000 100%) 0.1574 69.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 60.80%\n",
      "0m 11s (- 3m 39s) (50 5%) 0.0860 \n",
      "0m 15s (- 2m 23s) (100 10%) 0.1414 \n",
      "0m 20s (- 1m 56s) (150 15%) 0.0955 \n",
      "0m 26s (- 1m 44s) (200 20%) 0.0981 \n",
      "0m 29s (- 1m 29s) (250 25%) 0.0834 \n",
      "0m 34s (- 1m 19s) (300 30%) 0.0793 \n",
      "0m 38s (- 1m 11s) (350 35%) 0.1010 \n",
      "0m 42s (- 1m 3s) (400 40%) 0.0812 \n",
      "0m 47s (- 0m 57s) (450 45%) 0.1487 \n",
      "0m 51s (- 0m 51s) (500 50%) 0.1070 56.50%\n",
      "0m 58s (- 0m 47s) (550 55%) 0.1221 \n",
      "1m 2s (- 0m 41s) (600 60%) 0.1193 \n",
      "1m 7s (- 0m 36s) (650 65%) 0.0785 \n",
      "1m 13s (- 0m 31s) (700 70%) 0.0816 \n",
      "1m 17s (- 0m 25s) (750 75%) 0.0872 \n",
      "1m 21s (- 0m 20s) (800 80%) 0.1353 \n",
      "1m 27s (- 0m 15s) (850 85%) 0.1284 \n",
      "1m 32s (- 0m 10s) (900 90%) 0.1308 \n",
      "1m 36s (- 0m 5s) (950 95%) 0.0787 \n",
      "1m 40s (- 0m 0s) (1000 100%) 0.0898 68.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 65.60%\n",
      "0m 13s (- 4m 14s) (50 5%) 0.0937 \n",
      "0m 18s (- 2m 45s) (100 10%) 0.1445 \n",
      "0m 22s (- 2m 4s) (150 15%) 0.1132 \n",
      "0m 26s (- 1m 44s) (200 20%) 0.0943 \n",
      "0m 30s (- 1m 30s) (250 25%) 0.0708 \n",
      "0m 33s (- 1m 18s) (300 30%) 0.1276 \n",
      "0m 37s (- 1m 10s) (350 35%) 0.0563 \n",
      "0m 41s (- 1m 1s) (400 40%) 0.0629 \n",
      "0m 45s (- 0m 55s) (450 45%) 0.0756 \n",
      "0m 49s (- 0m 49s) (500 50%) 0.0897 69.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 56s (- 0m 46s) (550 55%) 0.0810 \n",
      "1m 0s (- 0m 40s) (600 60%) 0.0591 \n",
      "1m 4s (- 0m 34s) (650 65%) 0.0702 \n",
      "1m 8s (- 0m 29s) (700 70%) 0.0613 \n",
      "1m 12s (- 0m 24s) (750 75%) 0.0727 \n",
      "1m 16s (- 0m 19s) (800 80%) 0.0796 \n",
      "1m 19s (- 0m 14s) (850 85%) 0.1010 \n",
      "1m 23s (- 0m 9s) (900 90%) 0.0593 \n",
      "1m 27s (- 0m 4s) (950 95%) 0.0596 \n",
      "1m 31s (- 0m 0s) (1000 100%) 0.0536 66.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 68.60%\n",
      "0m 14s (- 4m 38s) (50 5%) 0.0849 \n",
      "0m 17s (- 2m 40s) (100 10%) 0.0758 \n",
      "0m 21s (- 2m 4s) (150 15%) 0.0879 \n",
      "0m 25s (- 1m 42s) (200 20%) 0.0722 \n",
      "0m 29s (- 1m 28s) (250 25%) 0.0556 \n",
      "0m 34s (- 1m 20s) (300 30%) 0.0585 \n",
      "0m 39s (- 1m 12s) (350 35%) 0.0524 \n",
      "0m 43s (- 1m 5s) (400 40%) 0.1150 \n",
      "0m 47s (- 0m 58s) (450 45%) 0.0724 \n",
      "0m 51s (- 0m 51s) (500 50%) 0.0517 68.50%\n",
      "0m 59s (- 0m 48s) (550 55%) 0.1545 \n",
      "1m 3s (- 0m 42s) (600 60%) 0.1066 \n",
      "1m 7s (- 0m 36s) (650 65%) 0.0618 \n",
      "1m 11s (- 0m 30s) (700 70%) 0.0642 \n",
      "1m 15s (- 0m 25s) (750 75%) 0.0864 \n",
      "1m 19s (- 0m 19s) (800 80%) 0.0425 \n",
      "1m 23s (- 0m 14s) (850 85%) 0.0785 \n",
      "1m 27s (- 0m 9s) (900 90%) 0.0559 \n",
      "1m 31s (- 0m 4s) (950 95%) 0.0853 \n",
      "1m 35s (- 0m 0s) (1000 100%) 0.0525 71.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 70.20%\n",
      "0m 13s (- 4m 24s) (50 5%) 0.0530 \n",
      "0m 17s (- 2m 38s) (100 10%) 0.0907 \n",
      "0m 21s (- 2m 1s) (150 15%) 0.0591 \n",
      "0m 25s (- 1m 43s) (200 20%) 0.1120 \n",
      "0m 29s (- 1m 28s) (250 25%) 0.0422 \n",
      "0m 33s (- 1m 18s) (300 30%) 0.0514 \n",
      "0m 37s (- 1m 9s) (350 35%) 0.0513 \n",
      "0m 41s (- 1m 2s) (400 40%) 0.1138 \n",
      "0m 45s (- 0m 56s) (450 45%) 0.0414 \n",
      "0m 52s (- 0m 52s) (500 50%) 0.0438 78.00%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 1s (- 0m 50s) (550 55%) 0.0459 \n",
      "1m 6s (- 0m 44s) (600 60%) 0.1263 \n",
      "1m 10s (- 0m 37s) (650 65%) 0.0487 \n",
      "1m 14s (- 0m 31s) (700 70%) 0.0616 \n",
      "1m 19s (- 0m 26s) (750 75%) 0.0501 \n",
      "1m 24s (- 0m 21s) (800 80%) 0.0688 \n",
      "1m 28s (- 0m 15s) (850 85%) 0.0602 \n",
      "1m 33s (- 0m 10s) (900 90%) 0.1060 \n",
      "1m 37s (- 0m 5s) (950 95%) 0.0538 \n",
      "1m 41s (- 0m 0s) (1000 100%) 0.0445 73.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 74.80%\n",
      "0m 13s (- 4m 12s) (50 5%) 0.0505 \n",
      "0m 17s (- 2m 35s) (100 10%) 0.0744 \n",
      "0m 21s (- 2m 1s) (150 15%) 0.0636 \n",
      "0m 25s (- 1m 41s) (200 20%) 0.0548 \n",
      "0m 29s (- 1m 27s) (250 25%) 0.0513 \n",
      "0m 33s (- 1m 17s) (300 30%) 0.0540 \n",
      "0m 37s (- 1m 9s) (350 35%) 0.0458 \n",
      "0m 41s (- 1m 2s) (400 40%) 0.0362 \n",
      "0m 45s (- 0m 55s) (450 45%) 0.0345 \n",
      "0m 49s (- 0m 49s) (500 50%) 0.0548 77.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 57s (- 0m 46s) (550 55%) 0.0518 \n",
      "1m 1s (- 0m 40s) (600 60%) 0.0460 \n",
      "1m 6s (- 0m 35s) (650 65%) 0.0567 \n",
      "1m 11s (- 0m 30s) (700 70%) 0.0495 \n",
      "1m 16s (- 0m 25s) (750 75%) 0.0334 \n",
      "1m 21s (- 0m 20s) (800 80%) 0.0573 \n",
      "1m 25s (- 0m 15s) (850 85%) 0.0735 \n",
      "1m 30s (- 0m 10s) (900 90%) 0.0422 \n",
      "1m 33s (- 0m 4s) (950 95%) 0.0425 \n",
      "1m 37s (- 0m 0s) (1000 100%) 0.0292 81.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 76.60%\n",
      "0m 11s (- 3m 41s) (50 5%) 0.0727 \n",
      "0m 16s (- 2m 25s) (100 10%) 0.1336 \n",
      "0m 19s (- 1m 48s) (150 15%) 0.0386 \n",
      "0m 22s (- 1m 29s) (200 20%) 0.0393 \n",
      "0m 25s (- 1m 16s) (250 25%) 0.0275 \n",
      "0m 29s (- 1m 7s) (300 30%) 0.0736 \n",
      "0m 32s (- 1m 0s) (350 35%) 0.0563 \n",
      "0m 35s (- 0m 53s) (400 40%) 0.0417 \n",
      "0m 40s (- 0m 48s) (450 45%) 0.0793 \n",
      "0m 44s (- 0m 44s) (500 50%) 0.0277 74.50%\n",
      "0m 51s (- 0m 42s) (550 55%) 0.0318 \n",
      "0m 55s (- 0m 36s) (600 60%) 0.0476 \n",
      "0m 58s (- 0m 31s) (650 65%) 0.0403 \n",
      "1m 1s (- 0m 26s) (700 70%) 0.0817 \n",
      "1m 5s (- 0m 21s) (750 75%) 0.0423 \n",
      "1m 9s (- 0m 17s) (800 80%) 0.1327 \n",
      "1m 14s (- 0m 13s) (850 85%) 0.0357 \n",
      "1m 17s (- 0m 8s) (900 90%) 0.0563 \n",
      "1m 21s (- 0m 4s) (950 95%) 0.0510 \n",
      "1m 27s (- 0m 0s) (1000 100%) 0.0600 82.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 79.60%\n",
      "0m 16s (- 5m 7s) (50 5%) 0.0429 \n",
      "0m 20s (- 3m 2s) (100 10%) 0.0530 \n",
      "0m 24s (- 2m 19s) (150 15%) 0.0408 \n",
      "0m 28s (- 1m 55s) (200 20%) 0.0511 \n",
      "0m 33s (- 1m 40s) (250 25%) 0.0575 \n",
      "0m 37s (- 1m 27s) (300 30%) 0.0441 \n",
      "0m 41s (- 1m 17s) (350 35%) 0.0480 \n",
      "0m 46s (- 1m 9s) (400 40%) 0.0342 \n",
      "0m 50s (- 1m 1s) (450 45%) 0.0501 \n",
      "0m 53s (- 0m 53s) (500 50%) 0.0641 78.00%\n",
      "1m 1s (- 0m 50s) (550 55%) 0.0650 \n",
      "1m 5s (- 0m 43s) (600 60%) 0.0786 \n",
      "1m 9s (- 0m 37s) (650 65%) 0.0369 \n",
      "1m 13s (- 0m 31s) (700 70%) 0.0408 \n",
      "1m 17s (- 0m 25s) (750 75%) 0.0369 \n",
      "1m 21s (- 0m 20s) (800 80%) 0.0800 \n",
      "1m 25s (- 0m 15s) (850 85%) 0.0403 \n",
      "1m 29s (- 0m 9s) (900 90%) 0.0319 \n",
      "1m 33s (- 0m 4s) (950 95%) 0.0458 \n",
      "1m 37s (- 0m 0s) (1000 100%) 0.0421 80.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 78.60%\n",
      "0m 16s (- 5m 18s) (50 5%) 0.0785 \n",
      "0m 20s (- 3m 7s) (100 10%) 0.0238 \n",
      "0m 26s (- 2m 27s) (150 15%) 0.0617 \n",
      "0m 31s (- 2m 4s) (200 20%) 0.0397 \n",
      "0m 35s (- 1m 47s) (250 25%) 0.0374 \n",
      "0m 40s (- 1m 34s) (300 30%) 0.0416 \n",
      "0m 44s (- 1m 23s) (350 35%) 0.0544 \n",
      "0m 51s (- 1m 16s) (400 40%) 0.0360 \n",
      "0m 55s (- 1m 8s) (450 45%) 0.0401 \n",
      "1m 1s (- 1m 1s) (500 50%) 0.0513 86.00%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 13s (- 1m 0s) (550 55%) 0.0346 \n",
      "1m 21s (- 0m 54s) (600 60%) 0.0399 \n",
      "1m 26s (- 0m 46s) (650 65%) 0.0295 \n",
      "1m 32s (- 0m 39s) (700 70%) 0.0319 \n",
      "1m 40s (- 0m 33s) (750 75%) 0.0415 \n",
      "1m 46s (- 0m 26s) (800 80%) 0.0389 \n",
      "1m 50s (- 0m 19s) (850 85%) 0.0583 \n",
      "1m 54s (- 0m 12s) (900 90%) 0.0611 \n",
      "1m 59s (- 0m 6s) (950 95%) 0.0272 \n",
      "2m 4s (- 0m 0s) (1000 100%) 0.0366 83.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 83.80%\n",
      "0m 21s (- 6m 45s) (50 5%) 0.0402 \n",
      "0m 25s (- 3m 52s) (100 10%) 0.0407 \n",
      "0m 30s (- 2m 50s) (150 15%) 0.0595 \n",
      "0m 34s (- 2m 17s) (200 20%) 0.0267 \n",
      "0m 38s (- 1m 56s) (250 25%) 0.0465 \n",
      "0m 42s (- 1m 39s) (300 30%) 0.0226 \n",
      "0m 47s (- 1m 27s) (350 35%) 0.0381 \n",
      "0m 51s (- 1m 16s) (400 40%) 0.0456 \n",
      "0m 55s (- 1m 7s) (450 45%) 0.0304 \n",
      "0m 59s (- 0m 59s) (500 50%) 0.0757 85.50%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 6s (- 0m 54s) (550 55%) 0.0267 \n",
      "1m 10s (- 0m 47s) (600 60%) 0.0794 \n",
      "1m 15s (- 0m 40s) (650 65%) 0.0294 \n",
      "1m 19s (- 0m 33s) (700 70%) 0.0502 \n",
      "1m 22s (- 0m 27s) (750 75%) 0.0293 \n",
      "1m 27s (- 0m 21s) (800 80%) 0.0364 \n",
      "1m 30s (- 0m 16s) (850 85%) 0.0665 \n",
      "1m 35s (- 0m 10s) (900 90%) 0.0335 \n",
      "1m 39s (- 0m 5s) (950 95%) 0.0295 \n",
      "1m 43s (- 0m 0s) (1000 100%) 0.0375 81.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 82.00%\n",
      "0m 14s (- 4m 26s) (50 5%) 0.1268 \n",
      "0m 18s (- 2m 42s) (100 10%) 0.0295 \n",
      "0m 22s (- 2m 6s) (150 15%) 0.0238 \n",
      "0m 27s (- 1m 48s) (200 20%) 0.0579 \n",
      "0m 31s (- 1m 34s) (250 25%) 0.0842 \n",
      "0m 35s (- 1m 22s) (300 30%) 0.0282 \n",
      "0m 40s (- 1m 15s) (350 35%) 0.0415 \n",
      "0m 44s (- 1m 6s) (400 40%) 0.0238 \n",
      "0m 49s (- 0m 59s) (450 45%) 0.0363 \n",
      "0m 54s (- 0m 54s) (500 50%) 0.0586 82.50%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 3s (- 0m 52s) (550 55%) 0.0325 \n",
      "1m 8s (- 0m 45s) (600 60%) 0.0239 \n",
      "1m 12s (- 0m 39s) (650 65%) 0.0347 \n",
      "1m 18s (- 0m 33s) (700 70%) 0.0247 \n",
      "1m 22s (- 0m 27s) (750 75%) 0.0282 \n",
      "1m 26s (- 0m 21s) (800 80%) 0.0260 \n",
      "1m 33s (- 0m 16s) (850 85%) 0.0479 \n",
      "1m 40s (- 0m 11s) (900 90%) 0.0176 \n",
      "1m 46s (- 0m 5s) (950 95%) 0.0336 \n",
      "1m 53s (- 0m 0s) (1000 100%) 0.0591 81.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 82.00%\n",
      "0m 12s (- 4m 6s) (50 5%) 0.0808 \n",
      "0m 17s (- 2m 36s) (100 10%) 0.0683 \n",
      "0m 21s (- 2m 2s) (150 15%) 0.0500 \n",
      "0m 25s (- 1m 41s) (200 20%) 0.0450 \n",
      "0m 29s (- 1m 27s) (250 25%) 0.0365 \n",
      "0m 32s (- 1m 16s) (300 30%) 0.0239 \n",
      "0m 37s (- 1m 8s) (350 35%) 0.0289 \n",
      "0m 40s (- 1m 1s) (400 40%) 0.0237 \n",
      "0m 44s (- 0m 54s) (450 45%) 0.0283 \n",
      "0m 48s (- 0m 48s) (500 50%) 0.0312 82.00%\n",
      "0m 58s (- 0m 47s) (550 55%) 0.0311 \n",
      "1m 2s (- 0m 41s) (600 60%) 0.0495 \n",
      "1m 6s (- 0m 35s) (650 65%) 0.0378 \n",
      "1m 10s (- 0m 30s) (700 70%) 0.0448 \n",
      "1m 14s (- 0m 24s) (750 75%) 0.0292 \n",
      "1m 17s (- 0m 19s) (800 80%) 0.0365 \n",
      "1m 21s (- 0m 14s) (850 85%) 0.0262 \n",
      "1m 25s (- 0m 9s) (900 90%) 0.0560 \n",
      "1m 28s (- 0m 4s) (950 95%) 0.0205 \n",
      "1m 32s (- 0m 0s) (1000 100%) 0.0228 87.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 86.00%\n",
      "0m 16s (- 5m 5s) (50 5%) 0.0225 \n",
      "0m 20s (- 3m 0s) (100 10%) 0.0175 \n",
      "0m 24s (- 2m 17s) (150 15%) 0.0207 \n",
      "0m 28s (- 1m 52s) (200 20%) 0.0356 \n",
      "0m 31s (- 1m 35s) (250 25%) 0.0299 \n",
      "0m 36s (- 1m 24s) (300 30%) 0.0298 \n",
      "0m 39s (- 1m 13s) (350 35%) 0.0240 \n",
      "0m 43s (- 1m 5s) (400 40%) 0.0254 \n",
      "0m 47s (- 0m 58s) (450 45%) 0.0209 \n",
      "0m 51s (- 0m 51s) (500 50%) 0.0398 86.00%\n",
      "0m 58s (- 0m 48s) (550 55%) 0.0409 \n",
      "1m 3s (- 0m 42s) (600 60%) 0.0250 \n",
      "1m 6s (- 0m 35s) (650 65%) 0.0155 \n",
      "1m 11s (- 0m 30s) (700 70%) 0.0273 \n",
      "1m 14s (- 0m 24s) (750 75%) 0.0146 \n",
      "1m 18s (- 0m 19s) (800 80%) 0.0774 \n",
      "1m 22s (- 0m 14s) (850 85%) 0.0241 \n",
      "1m 26s (- 0m 9s) (900 90%) 0.0188 \n",
      "1m 30s (- 0m 4s) (950 95%) 0.0296 \n",
      "1m 34s (- 0m 0s) (1000 100%) 0.0267 89.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 88.20%\n",
      "0m 9s (- 6m 13s) (50 2%) 0.0208 \n",
      "0m 12s (- 4m 1s) (100 5%) 0.0207 \n",
      "0m 15s (- 3m 16s) (150 7%) 0.0418 \n",
      "0m 19s (- 2m 55s) (200 10%) 0.0258 \n",
      "0m 22s (- 2m 38s) (250 12%) 0.0207 \n",
      "0m 26s (- 2m 27s) (300 15%) 0.0243 \n",
      "0m 29s (- 2m 19s) (350 17%) 0.0228 \n",
      "0m 32s (- 2m 10s) (400 20%) 0.0336 \n",
      "0m 36s (- 2m 4s) (450 22%) 0.0235 \n",
      "0m 39s (- 1m 57s) (500 25%) 0.0178 90.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 45s (- 2m 0s) (550 27%) 0.0231 \n",
      "0m 49s (- 1m 54s) (600 30%) 0.0249 \n",
      "0m 52s (- 1m 49s) (650 32%) 0.0237 \n",
      "0m 55s (- 1m 43s) (700 35%) 0.0172 \n",
      "0m 59s (- 1m 38s) (750 37%) 0.0192 \n",
      "1m 2s (- 1m 33s) (800 40%) 0.0227 \n",
      "1m 5s (- 1m 29s) (850 42%) 0.0402 \n",
      "1m 8s (- 1m 24s) (900 45%) 0.0405 \n",
      "1m 12s (- 1m 19s) (950 47%) 0.0162 \n",
      "1m 15s (- 1m 15s) (1000 50%) 0.0371 90.50%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 21s (- 1m 13s) (1050 52%) 0.0127 \n",
      "1m 25s (- 1m 10s) (1100 55%) 0.0229 \n",
      "1m 30s (- 1m 6s) (1150 57%) 0.0210 \n",
      "1m 34s (- 1m 2s) (1200 60%) 0.0183 \n",
      "1m 37s (- 0m 58s) (1250 62%) 0.0450 \n",
      "1m 41s (- 0m 54s) (1300 65%) 0.0246 \n",
      "1m 45s (- 0m 50s) (1350 67%) 0.0190 \n",
      "1m 48s (- 0m 46s) (1400 70%) 0.0244 \n",
      "1m 51s (- 0m 42s) (1450 72%) 0.0157 \n",
      "1m 55s (- 0m 38s) (1500 75%) 0.0175 87.50%\n",
      "2m 1s (- 0m 35s) (1550 77%) 0.0168 \n",
      "2m 5s (- 0m 31s) (1600 80%) 0.0237 \n",
      "2m 9s (- 0m 27s) (1650 82%) 0.1364 \n",
      "2m 12s (- 0m 23s) (1700 85%) 0.0217 \n",
      "2m 16s (- 0m 19s) (1750 87%) 0.0231 \n",
      "2m 19s (- 0m 15s) (1800 90%) 0.0191 \n",
      "2m 23s (- 0m 11s) (1850 92%) 0.0152 \n",
      "2m 28s (- 0m 7s) (1900 95%) 0.0203 \n",
      "2m 31s (- 0m 3s) (1950 97%) 0.0191 \n",
      "2m 35s (- 0m 0s) (2000 100%) 0.0375 89.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 89.80%\n",
      "0m 11s (- 7m 41s) (50 2%) 0.0396 \n",
      "0m 15s (- 4m 50s) (100 5%) 0.0222 \n",
      "0m 19s (- 4m 5s) (150 7%) 0.0224 \n",
      "0m 25s (- 3m 45s) (200 10%) 0.0236 \n",
      "0m 28s (- 3m 22s) (250 12%) 0.0197 \n",
      "0m 32s (- 3m 6s) (300 15%) 0.0321 \n",
      "0m 37s (- 2m 56s) (350 17%) 0.0353 \n",
      "0m 40s (- 2m 41s) (400 20%) 0.0191 \n",
      "0m 43s (- 2m 29s) (450 22%) 0.0216 \n",
      "0m 46s (- 2m 20s) (500 25%) 0.0229 91.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 52s (- 2m 18s) (550 27%) 0.0504 \n",
      "0m 57s (- 2m 13s) (600 30%) 0.0213 \n",
      "1m 0s (- 2m 6s) (650 32%) 0.0179 \n",
      "1m 6s (- 2m 3s) (700 35%) 0.0228 \n",
      "1m 9s (- 1m 56s) (750 37%) 0.0154 \n",
      "1m 13s (- 1m 49s) (800 40%) 0.0214 \n",
      "1m 16s (- 1m 44s) (850 42%) 0.0194 \n",
      "1m 20s (- 1m 38s) (900 45%) 0.0263 \n",
      "1m 24s (- 1m 33s) (950 47%) 0.0269 \n",
      "1m 27s (- 1m 27s) (1000 50%) 0.0447 88.50%\n",
      "1m 35s (- 1m 26s) (1050 52%) 0.0626 \n",
      "1m 39s (- 1m 21s) (1100 55%) 0.0392 \n",
      "1m 44s (- 1m 17s) (1150 57%) 0.0257 \n",
      "1m 48s (- 1m 12s) (1200 60%) 0.0162 \n",
      "1m 52s (- 1m 7s) (1250 62%) 0.0170 \n",
      "1m 56s (- 1m 2s) (1300 65%) 0.0394 \n",
      "2m 0s (- 0m 57s) (1350 67%) 0.0162 \n",
      "2m 3s (- 0m 52s) (1400 70%) 0.0176 \n",
      "2m 8s (- 0m 48s) (1450 72%) 0.0890 \n",
      "2m 12s (- 0m 44s) (1500 75%) 0.0424 89.00%\n",
      "2m 19s (- 0m 40s) (1550 77%) 0.0200 \n",
      "2m 23s (- 0m 35s) (1600 80%) 0.0202 \n",
      "2m 27s (- 0m 31s) (1650 82%) 0.0594 \n",
      "2m 30s (- 0m 26s) (1700 85%) 0.0164 \n",
      "2m 35s (- 0m 22s) (1750 87%) 0.0394 \n",
      "2m 38s (- 0m 17s) (1800 90%) 0.0322 \n",
      "2m 42s (- 0m 13s) (1850 92%) 0.0207 \n",
      "2m 46s (- 0m 8s) (1900 95%) 0.0175 \n",
      "2m 50s (- 0m 4s) (1950 97%) 0.0255 \n",
      "2m 53s (- 0m 0s) (2000 100%) 0.0168 89.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 89.00%\n",
      "0m 15s (- 10m 20s) (50 2%) 0.0208 \n",
      "0m 19s (- 6m 6s) (100 5%) 0.0496 \n",
      "0m 23s (- 4m 45s) (150 7%) 0.0340 \n",
      "0m 27s (- 4m 6s) (200 10%) 0.0233 \n",
      "0m 30s (- 3m 34s) (250 12%) 0.0178 \n",
      "0m 34s (- 3m 16s) (300 15%) 0.0172 \n",
      "0m 38s (- 3m 1s) (350 17%) 0.0183 \n",
      "0m 43s (- 2m 54s) (400 20%) 0.0297 \n",
      "0m 48s (- 2m 46s) (450 22%) 0.0119 \n",
      "0m 52s (- 2m 37s) (500 25%) 0.0198 90.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 58s (- 2m 35s) (550 27%) 0.0351 \n",
      "1m 3s (- 2m 27s) (600 30%) 0.0362 \n",
      "1m 7s (- 2m 20s) (650 32%) 0.0188 \n",
      "1m 11s (- 2m 12s) (700 35%) 0.0201 \n",
      "1m 14s (- 2m 4s) (750 37%) 0.0421 \n",
      "1m 18s (- 1m 57s) (800 40%) 0.0201 \n",
      "1m 22s (- 1m 51s) (850 42%) 0.0149 \n",
      "1m 25s (- 1m 44s) (900 45%) 0.0253 \n",
      "1m 29s (- 1m 38s) (950 47%) 0.0337 \n",
      "1m 32s (- 1m 32s) (1000 50%) 0.0187 87.50%\n",
      "1m 39s (- 1m 30s) (1050 52%) 0.0379 \n",
      "1m 42s (- 1m 24s) (1100 55%) 0.0444 \n",
      "1m 46s (- 1m 18s) (1150 57%) 0.0196 \n",
      "1m 50s (- 1m 13s) (1200 60%) 0.0408 \n",
      "1m 55s (- 1m 9s) (1250 62%) 0.0203 \n",
      "2m 0s (- 1m 4s) (1300 65%) 0.0394 \n",
      "2m 5s (- 1m 0s) (1350 67%) 0.0327 \n",
      "2m 9s (- 0m 55s) (1400 70%) 0.0162 \n",
      "2m 16s (- 0m 51s) (1450 72%) 0.0399 \n",
      "2m 19s (- 0m 46s) (1500 75%) 0.0186 89.50%\n",
      "2m 27s (- 0m 42s) (1550 77%) 0.0138 \n",
      "2m 32s (- 0m 38s) (1600 80%) 0.0203 \n",
      "2m 35s (- 0m 33s) (1650 82%) 0.0223 \n",
      "2m 40s (- 0m 28s) (1700 85%) 0.0191 \n",
      "2m 44s (- 0m 23s) (1750 87%) 0.0154 \n",
      "2m 49s (- 0m 18s) (1800 90%) 0.0198 \n",
      "2m 53s (- 0m 14s) (1850 92%) 0.0203 \n",
      "2m 57s (- 0m 9s) (1900 95%) 0.0174 \n",
      "3m 2s (- 0m 4s) (1950 97%) 0.0331 \n",
      "3m 7s (- 0m 0s) (2000 100%) 0.0206 91.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 92.40%\n",
      "0m 11s (- 7m 19s) (50 2%) 0.0162 \n",
      "0m 14s (- 4m 41s) (100 5%) 0.0170 \n",
      "0m 18s (- 3m 43s) (150 7%) 0.0167 \n",
      "0m 21s (- 3m 12s) (200 10%) 0.0498 \n",
      "0m 25s (- 2m 57s) (250 12%) 0.0124 \n",
      "0m 28s (- 2m 43s) (300 15%) 0.0160 \n",
      "0m 31s (- 2m 30s) (350 17%) 0.0384 \n",
      "0m 35s (- 2m 22s) (400 20%) 0.0159 \n",
      "0m 38s (- 2m 13s) (450 22%) 0.0199 \n",
      "0m 42s (- 2m 6s) (500 25%) 0.0156 94.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 48s (- 2m 7s) (550 27%) 0.0168 \n",
      "0m 51s (- 2m 0s) (600 30%) 0.0208 \n",
      "0m 55s (- 1m 54s) (650 32%) 0.0228 \n",
      "0m 58s (- 1m 49s) (700 35%) 0.0168 \n",
      "1m 2s (- 1m 43s) (750 37%) 0.0236 \n",
      "1m 5s (- 1m 38s) (800 40%) 0.0199 \n",
      "1m 8s (- 1m 32s) (850 42%) 0.0167 \n",
      "1m 12s (- 1m 28s) (900 45%) 0.0217 \n",
      "1m 15s (- 1m 23s) (950 47%) 0.0180 \n",
      "1m 18s (- 1m 18s) (1000 50%) 0.0146 91.00%\n",
      "1m 24s (- 1m 16s) (1050 52%) 0.0165 \n",
      "1m 29s (- 1m 13s) (1100 55%) 0.0216 \n",
      "1m 33s (- 1m 9s) (1150 57%) 0.0280 \n",
      "1m 37s (- 1m 4s) (1200 60%) 0.0634 \n",
      "1m 40s (- 1m 0s) (1250 62%) 0.0572 \n",
      "1m 43s (- 0m 55s) (1300 65%) 0.0453 \n",
      "1m 46s (- 0m 51s) (1350 67%) 0.0257 \n",
      "1m 49s (- 0m 46s) (1400 70%) 0.0136 \n",
      "1m 53s (- 0m 42s) (1450 72%) 0.0176 \n",
      "1m 56s (- 0m 38s) (1500 75%) 0.0165 90.00%\n",
      "2m 3s (- 0m 35s) (1550 77%) 0.0207 \n",
      "2m 8s (- 0m 32s) (1600 80%) 0.0225 \n",
      "2m 11s (- 0m 27s) (1650 82%) 0.0258 \n",
      "2m 16s (- 0m 24s) (1700 85%) 0.0359 \n",
      "2m 21s (- 0m 20s) (1750 87%) 0.0217 \n",
      "2m 24s (- 0m 16s) (1800 90%) 0.0134 \n",
      "2m 28s (- 0m 12s) (1850 92%) 0.0940 \n",
      "2m 31s (- 0m 7s) (1900 95%) 0.0215 \n",
      "2m 35s (- 0m 3s) (1950 97%) 0.0486 \n",
      "2m 39s (- 0m 0s) (2000 100%) 0.0424 92.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90400000000000003"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "    encoder2 = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "    decoder2 = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "evaluateAccuracy(encoder2, decoder2, n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> walk around right after jump opposite left thrice\n",
      "= I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK\n",
      "< I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK <EOS>\n",
      "\n",
      "> jump opposite left twice and look left\n",
      "= I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_LOOK\n",
      "< I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_LOOK <EOS>\n",
      "\n",
      "> turn around left twice after jump\n",
      "= I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT\n",
      "< I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT <EOS>\n",
      "\n",
      "> look around left thrice and jump opposite left twice\n",
      "= I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP\n",
      "< I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> jump around right after run left\n",
      "= I_TURN_LEFT I_RUN I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_LEFT I_RUN I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> jump around left twice and turn left\n",
      "= I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT\n",
      "< I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> walk thrice and jump left twice\n",
      "= I_WALK I_WALK I_WALK I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP\n",
      "< I_WALK I_WALK I_WALK I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> walk twice after jump around right thrice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_WALK I_WALK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_WALK I_WALK <EOS>\n",
      "\n",
      "> run around right after jump right\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN <EOS>\n",
      "\n",
      "> jump left thrice and jump right thrice\n",
      "= I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

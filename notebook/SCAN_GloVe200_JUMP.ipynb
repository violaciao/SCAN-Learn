{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCAN Add-Prim JUMP Experiment\n",
    "*************************************************************\n",
    "\n",
    "Reference: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Python 3.6\n",
    "* PyTorch 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Device is using\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "TASK_NAME = \"addprim-jump\"\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False, trainOrtest='train'):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines        \n",
    "    lines = open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/processed/{}-{}_{}-{}.txt'.\\\n",
    "                 format(trainOrtest, TASK_NAME, lang1, lang2), encoding='utf-8').\\\n",
    "                 read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "# PRED_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 37046 sentence pairs\n",
      "Trimmed to 37046 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['turn around right twice after jump left', 'I_TURN_LEFT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataFrom='train'):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=False, trainOrtest=dataFrom)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('in', 'out', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "=================\n",
    "\n",
    "The model we are using is a GRU encoder-decoder seq2seq model with attention mechanism. In order to solve the zero-shot generalization task, we embed the encoder networks with pre-trained embeddings, from GloVe and Google Word2Vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_SOURCE = 'glove'\n",
    "hidden_size = 200\n",
    "\n",
    "if EMBEDDEING_SOURCE == 'google':\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_GoogleNews300Negative.pkl', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "else:\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_raw{}d.pkl'.format(hidden_size), 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "\n",
    "pretrained_emb = np.zeros((input_lang.n_words, hidden_size))\n",
    "for k, v in input_lang.index2word.items():\n",
    "    if v == 'SOS':\n",
    "        pretrained_emb[k] = np.zeros(hidden_size)\n",
    "    elif (v == 'EOS') and (EMBEDDEING_SOURCE != 'google'):\n",
    "        pretrained_emb[k] = b['.']\n",
    "    elif (v == 'and') and (EMBEDDEING_SOURCE == 'google'):\n",
    "        pretrained_emb[k] = b['AND']\n",
    "    else:\n",
    "        pretrained_emb[k] = b[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of this seq2seq network is a GRU netword. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_PRETRAINED = True\n",
    "WEIGHT_UPDATE = False\n",
    "\n",
    "MODEL_VERSION = 'T0.4_glv200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        if EMBEDDEING_PRETRAINED:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "            self.embedding.weight.requires_grad = WEIGHT_UPDATE\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is a GRU network with attention mechanism that takes the last output of the encoder and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "First we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "We use teacher forcing to help converge faster with a delay fashion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, \n",
    "          max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for timing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, eval_every=1000, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "        encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "        decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "        \n",
    "    best_test_acc = evaluateAccuracy(encoder, decoder, 500)\n",
    "    print(\"Best evaluation accuracy: {0:.2f}%\".format(best_test_acc * 100))\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        \n",
    "    encoder_optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg), end=' ')\n",
    "            \n",
    "            if iter % eval_every == 0:\n",
    "                test_acc = evaluateAccuracy(encoder, decoder, 200)\n",
    "                print('{0:.2f}%'.format(test_acc * 100))\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    with open(\"saved_models/encoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(encoder, f)\n",
    "                    with open(\"saved_models/decoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(decoder, f)\n",
    "                    print(\"New best test accuracy! Model Updated!\")\n",
    "                    best_test_acc = test_acc\n",
    "#                 elif test_acc < best_test_acc - 0.001:\n",
    "#                     encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "#                     decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "                    \n",
    "            else:\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 15412 sentence pairs\n",
      "Trimmed to 15412 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['turn opposite left thrice after jump opposite left', 'I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs_eval = prepareData('in', 'out', True, dataFrom='test')\n",
    "print(random.choice(pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateAccuracy(encoder, decoder, n=10):\n",
    "    ACCs = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        \n",
    "        if output_words[-1] == '<EOS>':\n",
    "            output_words = output_words[:-1]\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        if output_sentence == pair[1]:\n",
    "            ACCs.append(1)\n",
    "        else:\n",
    "            ACCs.append(0)\n",
    "    return np.array(ACCs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initially trained with a higher teacher aid, and relatively large learning rate. Both teacher forcing effect and the learning rate decay over iterations when the model approaches the optimum.  \n",
    "\n",
    "#### The model achieves 97% accuracy rate for the best test sample evaluation, and is 94% correct on average for the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 0.00%\n",
      "0m 23s (- 39m 25s) (50 1%) 1.8528 \n",
      "0m 30s (- 24m 33s) (100 2%) 1.4852 \n",
      "0m 36s (- 19m 38s) (150 3%) 1.2120 \n",
      "0m 39s (- 15m 44s) (200 4%) 1.3494 \n",
      "0m 41s (- 13m 17s) (250 5%) 1.2040 \n",
      "0m 44s (- 11m 37s) (300 6%) 1.2234 \n",
      "0m 47s (- 10m 32s) (350 7%) 0.9504 \n",
      "0m 50s (- 9m 40s) (400 8%) 1.1201 \n",
      "0m 53s (- 9m 1s) (450 9%) 1.0016 \n",
      "0m 56s (- 8m 28s) (500 10%) 1.0039 0.50%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 6s (- 8m 58s) (550 11%) 0.9534 \n",
      "1m 9s (- 8m 30s) (600 12%) 0.8782 \n",
      "1m 12s (- 8m 3s) (650 13%) 1.1839 \n",
      "1m 15s (- 7m 42s) (700 14%) 0.9678 \n",
      "1m 18s (- 7m 22s) (750 15%) 0.8661 \n",
      "1m 20s (- 7m 2s) (800 16%) 0.9400 \n",
      "1m 23s (- 6m 45s) (850 17%) 0.7798 \n",
      "1m 25s (- 6m 30s) (900 18%) 0.7225 \n",
      "1m 28s (- 6m 18s) (950 19%) 0.6772 \n",
      "1m 31s (- 6m 7s) (1000 20%) 0.6705 0.00%\n",
      "1m 38s (- 6m 10s) (1050 21%) 0.7176 \n",
      "1m 40s (- 5m 57s) (1100 22%) 0.6323 \n",
      "1m 44s (- 5m 49s) (1150 23%) 0.7206 \n",
      "1m 47s (- 5m 38s) (1200 24%) 0.6546 \n",
      "1m 50s (- 5m 30s) (1250 25%) 0.7597 \n",
      "1m 52s (- 5m 21s) (1300 26%) 0.7223 \n",
      "1m 55s (- 5m 12s) (1350 27%) 0.6204 \n",
      "1m 58s (- 5m 4s) (1400 28%) 0.5764 \n",
      "2m 1s (- 4m 56s) (1450 28%) 0.7019 \n",
      "2m 4s (- 4m 50s) (1500 30%) 0.7662 0.00%\n",
      "2m 13s (- 4m 56s) (1550 31%) 0.5954 \n",
      "2m 16s (- 4m 49s) (1600 32%) 0.5704 \n",
      "2m 18s (- 4m 41s) (1650 33%) 0.5613 \n",
      "2m 21s (- 4m 34s) (1700 34%) 0.5474 \n",
      "2m 24s (- 4m 27s) (1750 35%) 0.5333 \n",
      "2m 27s (- 4m 21s) (1800 36%) 0.5711 \n",
      "2m 30s (- 4m 15s) (1850 37%) 0.5764 \n",
      "2m 32s (- 4m 8s) (1900 38%) 0.5098 \n",
      "2m 35s (- 4m 2s) (1950 39%) 0.4853 \n",
      "2m 38s (- 3m 57s) (2000 40%) 0.5754 1.00%\n",
      "New best test accuracy! Model Updated!\n",
      "2m 45s (- 3m 57s) (2050 41%) 0.5436 \n",
      "2m 48s (- 3m 52s) (2100 42%) 0.6718 \n",
      "2m 50s (- 3m 46s) (2150 43%) 0.5716 \n",
      "2m 53s (- 3m 41s) (2200 44%) 0.4272 \n",
      "2m 56s (- 3m 36s) (2250 45%) 0.5411 \n",
      "2m 59s (- 3m 31s) (2300 46%) 0.4727 \n",
      "3m 2s (- 3m 25s) (2350 47%) 0.5490 \n",
      "3m 5s (- 3m 21s) (2400 48%) 0.4829 \n",
      "3m 8s (- 3m 16s) (2450 49%) 0.4884 \n",
      "3m 11s (- 3m 11s) (2500 50%) 0.4461 3.50%\n",
      "New best test accuracy! Model Updated!\n",
      "3m 17s (- 3m 9s) (2550 51%) 0.5017 \n",
      "3m 20s (- 3m 4s) (2600 52%) 0.4531 \n",
      "3m 22s (- 2m 59s) (2650 53%) 0.4637 \n",
      "3m 25s (- 2m 54s) (2700 54%) 0.5874 \n",
      "3m 28s (- 2m 50s) (2750 55%) 0.4730 \n",
      "3m 31s (- 2m 45s) (2800 56%) 0.4586 \n",
      "3m 34s (- 2m 41s) (2850 56%) 0.4723 \n",
      "3m 36s (- 2m 36s) (2900 57%) 0.5497 \n",
      "3m 38s (- 2m 31s) (2950 59%) 0.4372 \n",
      "3m 41s (- 2m 27s) (3000 60%) 0.4585 3.50%\n",
      "3m 48s (- 2m 26s) (3050 61%) 0.4411 \n",
      "3m 51s (- 2m 21s) (3100 62%) 0.4560 \n",
      "3m 53s (- 2m 17s) (3150 63%) 0.4104 \n",
      "3m 55s (- 2m 12s) (3200 64%) 0.4853 \n",
      "3m 58s (- 2m 8s) (3250 65%) 0.3778 \n",
      "4m 0s (- 2m 3s) (3300 66%) 0.3523 \n",
      "4m 2s (- 1m 59s) (3350 67%) 0.5026 \n",
      "4m 5s (- 1m 55s) (3400 68%) 0.4671 \n",
      "4m 8s (- 1m 51s) (3450 69%) 0.4604 \n",
      "4m 10s (- 1m 47s) (3500 70%) 0.4630 4.00%\n",
      "New best test accuracy! Model Updated!\n",
      "4m 17s (- 1m 45s) (3550 71%) 0.4909 \n",
      "4m 19s (- 1m 41s) (3600 72%) 0.4440 \n",
      "4m 22s (- 1m 37s) (3650 73%) 0.4417 \n",
      "4m 24s (- 1m 33s) (3700 74%) 0.4111 \n",
      "4m 27s (- 1m 29s) (3750 75%) 0.4073 \n",
      "4m 29s (- 1m 25s) (3800 76%) 0.5081 \n",
      "4m 32s (- 1m 21s) (3850 77%) 0.3741 \n",
      "4m 34s (- 1m 17s) (3900 78%) 0.4269 \n",
      "4m 37s (- 1m 13s) (3950 79%) 0.3064 \n",
      "4m 40s (- 1m 10s) (4000 80%) 0.3846 9.50%\n",
      "New best test accuracy! Model Updated!\n",
      "4m 46s (- 1m 7s) (4050 81%) 0.3022 \n",
      "4m 49s (- 1m 3s) (4100 82%) 0.2972 \n",
      "4m 51s (- 0m 59s) (4150 83%) 0.3614 \n",
      "4m 54s (- 0m 56s) (4200 84%) 0.4067 \n",
      "4m 56s (- 0m 52s) (4250 85%) 0.3358 \n",
      "4m 59s (- 0m 48s) (4300 86%) 0.2998 \n",
      "5m 1s (- 0m 45s) (4350 87%) 0.2677 \n",
      "5m 4s (- 0m 41s) (4400 88%) 0.4300 \n",
      "5m 6s (- 0m 37s) (4450 89%) 0.3347 \n",
      "5m 8s (- 0m 34s) (4500 90%) 0.2933 13.00%\n",
      "New best test accuracy! Model Updated!\n",
      "5m 16s (- 0m 31s) (4550 91%) 0.2996 \n",
      "5m 20s (- 0m 27s) (4600 92%) 0.3222 \n",
      "5m 23s (- 0m 24s) (4650 93%) 0.3314 \n",
      "5m 26s (- 0m 20s) (4700 94%) 0.4076 \n",
      "5m 29s (- 0m 17s) (4750 95%) 0.2817 \n",
      "5m 31s (- 0m 13s) (4800 96%) 0.3148 \n",
      "5m 34s (- 0m 10s) (4850 97%) 0.4467 \n",
      "5m 37s (- 0m 6s) (4900 98%) 0.3227 \n",
      "5m 39s (- 0m 3s) (4950 99%) 0.3355 \n",
      "5m 42s (- 0m 0s) (5000 100%) 0.4268 13.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 13.00%\n",
      "0m 10s (- 3m 27s) (50 5%) 0.4327 \n",
      "0m 13s (- 2m 1s) (100 10%) 0.4090 \n",
      "0m 15s (- 1m 29s) (150 15%) 0.3706 \n",
      "0m 18s (- 1m 13s) (200 20%) 0.3954 \n",
      "0m 20s (- 1m 2s) (250 25%) 0.3077 \n",
      "0m 23s (- 0m 53s) (300 30%) 0.3458 \n",
      "0m 25s (- 0m 47s) (350 35%) 0.4096 \n",
      "0m 27s (- 0m 41s) (400 40%) 0.2793 \n",
      "0m 29s (- 0m 36s) (450 45%) 0.3734 \n",
      "0m 31s (- 0m 31s) (500 50%) 0.4641 13.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 36s (- 0m 29s) (550 55%) 0.3641 \n",
      "0m 38s (- 0m 25s) (600 60%) 0.4805 \n",
      "0m 40s (- 0m 21s) (650 65%) 0.4370 \n",
      "0m 42s (- 0m 18s) (700 70%) 0.3724 \n",
      "0m 44s (- 0m 14s) (750 75%) 0.3727 \n",
      "0m 46s (- 0m 11s) (800 80%) 0.4332 \n",
      "0m 49s (- 0m 8s) (850 85%) 0.3828 \n",
      "0m 51s (- 0m 5s) (900 90%) 0.4553 \n",
      "0m 53s (- 0m 2s) (950 95%) 0.3721 \n",
      "0m 55s (- 0m 0s) (1000 100%) 0.3132 18.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 22.80%\n",
      "0m 10s (- 3m 26s) (50 5%) 0.3685 \n",
      "0m 13s (- 2m 3s) (100 10%) 0.3895 \n",
      "0m 16s (- 1m 31s) (150 15%) 0.3307 \n",
      "0m 18s (- 1m 14s) (200 20%) 0.4044 \n",
      "0m 21s (- 1m 3s) (250 25%) 0.4664 \n",
      "0m 24s (- 0m 56s) (300 30%) 0.3737 \n",
      "0m 26s (- 0m 48s) (350 35%) 0.3540 \n",
      "0m 29s (- 0m 44s) (400 40%) 0.4336 \n",
      "0m 31s (- 0m 38s) (450 45%) 0.2859 \n",
      "0m 34s (- 0m 34s) (500 50%) 0.3185 23.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 39s (- 0m 32s) (550 55%) 0.2757 \n",
      "0m 42s (- 0m 28s) (600 60%) 0.2597 \n",
      "0m 45s (- 0m 24s) (650 65%) 0.3287 \n",
      "0m 48s (- 0m 20s) (700 70%) 0.2763 \n",
      "0m 51s (- 0m 17s) (750 75%) 0.3816 \n",
      "0m 54s (- 0m 13s) (800 80%) 0.4050 \n",
      "0m 56s (- 0m 10s) (850 85%) 0.4415 \n",
      "0m 59s (- 0m 6s) (900 90%) 0.4049 \n",
      "1m 2s (- 0m 3s) (950 95%) 0.3234 \n",
      "1m 4s (- 0m 0s) (1000 100%) 0.2589 26.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 26.00%\n",
      "0m 9s (- 3m 8s) (50 5%) 0.2597 \n",
      "0m 12s (- 1m 53s) (100 10%) 0.3489 \n",
      "0m 15s (- 1m 25s) (150 15%) 0.3552 \n",
      "0m 17s (- 1m 9s) (200 20%) 0.3550 \n",
      "0m 19s (- 0m 58s) (250 25%) 0.3171 \n",
      "0m 22s (- 0m 51s) (300 30%) 0.3217 \n",
      "0m 24s (- 0m 45s) (350 35%) 0.3326 \n",
      "0m 26s (- 0m 40s) (400 40%) 0.3178 \n",
      "0m 29s (- 0m 35s) (450 45%) 0.3383 \n",
      "0m 31s (- 0m 31s) (500 50%) 0.3638 19.00%\n",
      "0m 38s (- 0m 31s) (550 55%) 0.2803 \n",
      "0m 42s (- 0m 28s) (600 60%) 0.2171 \n",
      "0m 45s (- 0m 24s) (650 65%) 0.2560 \n",
      "0m 47s (- 0m 20s) (700 70%) 0.3973 \n",
      "0m 50s (- 0m 16s) (750 75%) 0.3027 \n",
      "0m 52s (- 0m 13s) (800 80%) 0.2712 \n",
      "0m 54s (- 0m 9s) (850 85%) 0.2445 \n",
      "0m 56s (- 0m 6s) (900 90%) 0.2572 \n",
      "0m 58s (- 0m 3s) (950 95%) 0.2502 \n",
      "1m 1s (- 0m 0s) (1000 100%) 0.3007 28.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 24.20%\n",
      "0m 8s (- 2m 33s) (50 5%) 0.2736 \n",
      "0m 10s (- 1m 31s) (100 10%) 0.2354 \n",
      "0m 12s (- 1m 8s) (150 15%) 0.2280 \n",
      "0m 14s (- 0m 56s) (200 20%) 0.2210 \n",
      "0m 15s (- 0m 47s) (250 25%) 0.1877 \n",
      "0m 18s (- 0m 42s) (300 30%) 0.2043 \n",
      "0m 20s (- 0m 37s) (350 35%) 0.2662 \n",
      "0m 22s (- 0m 33s) (400 40%) 0.1662 \n",
      "0m 24s (- 0m 29s) (450 45%) 0.2147 \n",
      "0m 26s (- 0m 26s) (500 50%) 0.2646 37.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 30s (- 0m 25s) (550 55%) 0.1655 \n",
      "0m 33s (- 0m 22s) (600 60%) 0.2257 \n",
      "0m 36s (- 0m 19s) (650 65%) 0.1719 \n",
      "0m 39s (- 0m 16s) (700 70%) 0.1569 \n",
      "0m 42s (- 0m 14s) (750 75%) 0.1472 \n",
      "0m 44s (- 0m 11s) (800 80%) 0.1904 \n",
      "0m 47s (- 0m 8s) (850 85%) 0.1280 \n",
      "0m 49s (- 0m 5s) (900 90%) 0.1214 \n",
      "0m 52s (- 0m 2s) (950 95%) 0.1518 \n",
      "0m 54s (- 0m 0s) (1000 100%) 0.2459 50.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 50.60%\n",
      "0m 9s (- 3m 1s) (50 5%) 0.2201 \n",
      "0m 12s (- 1m 53s) (100 10%) 0.1564 \n",
      "0m 15s (- 1m 27s) (150 15%) 0.1260 \n",
      "0m 18s (- 1m 13s) (200 20%) 0.2432 \n",
      "0m 21s (- 1m 3s) (250 25%) 0.1582 \n",
      "0m 23s (- 0m 55s) (300 30%) 0.2006 \n",
      "0m 25s (- 0m 47s) (350 35%) 0.1389 \n",
      "0m 28s (- 0m 42s) (400 40%) 0.1476 \n",
      "0m 31s (- 0m 37s) (450 45%) 0.2308 \n",
      "0m 33s (- 0m 33s) (500 50%) 0.1481 54.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 38s (- 0m 31s) (550 55%) 0.1340 \n",
      "0m 41s (- 0m 27s) (600 60%) 0.1744 \n",
      "0m 43s (- 0m 23s) (650 65%) 0.1972 \n",
      "0m 45s (- 0m 19s) (700 70%) 0.1305 \n",
      "0m 48s (- 0m 16s) (750 75%) 0.1683 \n",
      "0m 50s (- 0m 12s) (800 80%) 0.1459 \n",
      "0m 53s (- 0m 9s) (850 85%) 0.0958 \n",
      "0m 55s (- 0m 6s) (900 90%) 0.0811 \n",
      "0m 58s (- 0m 3s) (950 95%) 0.1281 \n",
      "1m 0s (- 0m 0s) (1000 100%) 0.1597 58.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 55.20%\n",
      "0m 10s (- 3m 27s) (50 5%) 0.1399 \n",
      "0m 13s (- 2m 4s) (100 10%) 0.1119 \n",
      "0m 16s (- 1m 31s) (150 15%) 0.1526 \n",
      "0m 18s (- 1m 13s) (200 20%) 0.1238 \n",
      "0m 20s (- 1m 2s) (250 25%) 0.1109 \n",
      "0m 22s (- 0m 52s) (300 30%) 0.1096 \n",
      "0m 24s (- 0m 45s) (350 35%) 0.1343 \n",
      "0m 26s (- 0m 39s) (400 40%) 0.1570 \n",
      "0m 28s (- 0m 35s) (450 45%) 0.1851 \n",
      "0m 30s (- 0m 30s) (500 50%) 0.1958 53.00%\n",
      "0m 36s (- 0m 29s) (550 55%) 0.0906 \n",
      "0m 39s (- 0m 26s) (600 60%) 0.1460 \n",
      "0m 41s (- 0m 22s) (650 65%) 0.1232 \n",
      "0m 44s (- 0m 18s) (700 70%) 0.1376 \n",
      "0m 46s (- 0m 15s) (750 75%) 0.1311 \n",
      "0m 49s (- 0m 12s) (800 80%) 0.1192 \n",
      "0m 52s (- 0m 9s) (850 85%) 0.1359 \n",
      "0m 54s (- 0m 6s) (900 90%) 0.1289 \n",
      "0m 56s (- 0m 2s) (950 95%) 0.1102 \n",
      "0m 59s (- 0m 0s) (1000 100%) 0.1914 56.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 55.20%\n",
      "0m 11s (- 3m 39s) (50 5%) 0.0987 \n",
      "0m 13s (- 2m 5s) (100 10%) 0.0995 \n",
      "0m 16s (- 1m 32s) (150 15%) 0.0885 \n",
      "0m 18s (- 1m 15s) (200 20%) 0.1138 \n",
      "0m 21s (- 1m 4s) (250 25%) 0.1024 \n",
      "0m 24s (- 0m 56s) (300 30%) 0.1225 \n",
      "0m 26s (- 0m 49s) (350 35%) 0.1334 \n",
      "0m 28s (- 0m 42s) (400 40%) 0.2292 \n",
      "0m 31s (- 0m 38s) (450 45%) 0.1512 \n",
      "0m 33s (- 0m 33s) (500 50%) 0.1226 58.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 38s (- 0m 31s) (550 55%) 0.1112 \n",
      "0m 41s (- 0m 27s) (600 60%) 0.1707 \n",
      "0m 45s (- 0m 24s) (650 65%) 0.0874 \n",
      "0m 48s (- 0m 20s) (700 70%) 0.0974 \n",
      "0m 51s (- 0m 17s) (750 75%) 0.0835 \n",
      "0m 53s (- 0m 13s) (800 80%) 0.1868 \n",
      "0m 56s (- 0m 9s) (850 85%) 0.1516 \n",
      "0m 58s (- 0m 6s) (900 90%) 0.1065 \n",
      "1m 1s (- 0m 3s) (950 95%) 0.1120 \n",
      "1m 3s (- 0m 0s) (1000 100%) 0.1012 62.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 59.20%\n",
      "0m 9s (- 2m 56s) (50 5%) 0.0926 \n",
      "0m 11s (- 1m 47s) (100 10%) 0.1560 \n",
      "0m 14s (- 1m 20s) (150 15%) 0.1176 \n",
      "0m 17s (- 1m 9s) (200 20%) 0.1497 \n",
      "0m 20s (- 1m 0s) (250 25%) 0.0919 \n",
      "0m 22s (- 0m 53s) (300 30%) 0.1208 \n",
      "0m 25s (- 0m 47s) (350 35%) 0.0871 \n",
      "0m 27s (- 0m 41s) (400 40%) 0.0994 \n",
      "0m 30s (- 0m 37s) (450 45%) 0.0788 \n",
      "0m 32s (- 0m 32s) (500 50%) 0.0960 70.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 38s (- 0m 31s) (550 55%) 0.1235 \n",
      "0m 40s (- 0m 27s) (600 60%) 0.0853 \n",
      "0m 43s (- 0m 23s) (650 65%) 0.0938 \n",
      "0m 46s (- 0m 19s) (700 70%) 0.2044 \n",
      "0m 48s (- 0m 16s) (750 75%) 0.1214 \n",
      "0m 50s (- 0m 12s) (800 80%) 0.1076 \n",
      "0m 52s (- 0m 9s) (850 85%) 0.1148 \n",
      "0m 54s (- 0m 6s) (900 90%) 0.0723 \n",
      "0m 57s (- 0m 3s) (950 95%) 0.0979 \n",
      "0m 59s (- 0m 0s) (1000 100%) 0.1076 71.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 64.80%\n",
      "0m 7s (- 2m 29s) (50 5%) 0.0884 \n",
      "0m 10s (- 1m 33s) (100 10%) 0.0809 \n",
      "0m 13s (- 1m 13s) (150 15%) 0.0707 \n",
      "0m 15s (- 1m 2s) (200 20%) 0.1138 \n",
      "0m 18s (- 0m 54s) (250 25%) 0.0956 \n",
      "0m 20s (- 0m 47s) (300 30%) 0.1063 \n",
      "0m 22s (- 0m 41s) (350 35%) 0.0559 \n",
      "0m 24s (- 0m 37s) (400 40%) 0.0657 \n",
      "0m 26s (- 0m 32s) (450 45%) 0.1779 \n",
      "0m 29s (- 0m 29s) (500 50%) 0.1235 64.00%\n",
      "0m 33s (- 0m 27s) (550 55%) 0.0665 \n",
      "0m 35s (- 0m 23s) (600 60%) 0.0625 \n",
      "0m 38s (- 0m 20s) (650 65%) 0.0727 \n",
      "0m 40s (- 0m 17s) (700 70%) 0.0523 \n",
      "0m 42s (- 0m 14s) (750 75%) 0.0692 \n",
      "0m 44s (- 0m 11s) (800 80%) 0.0613 \n",
      "0m 46s (- 0m 8s) (850 85%) 0.0567 \n",
      "0m 48s (- 0m 5s) (900 90%) 0.0630 \n",
      "0m 50s (- 0m 2s) (950 95%) 0.0723 \n",
      "0m 52s (- 0m 0s) (1000 100%) 0.0678 73.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 67.40%\n",
      "0m 9s (- 3m 5s) (50 5%) 0.0534 \n",
      "0m 12s (- 1m 48s) (100 10%) 0.0483 \n",
      "0m 14s (- 1m 21s) (150 15%) 0.0469 \n",
      "0m 16s (- 1m 6s) (200 20%) 0.0351 \n",
      "0m 19s (- 0m 57s) (250 25%) 0.0483 \n",
      "0m 21s (- 0m 50s) (300 30%) 0.0464 \n",
      "0m 24s (- 0m 45s) (350 35%) 0.0702 \n",
      "0m 26s (- 0m 40s) (400 40%) 0.0747 \n",
      "0m 29s (- 0m 35s) (450 45%) 0.1533 \n",
      "0m 32s (- 0m 32s) (500 50%) 0.0419 82.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 38s (- 0m 31s) (550 55%) 0.0500 \n",
      "0m 41s (- 0m 27s) (600 60%) 0.0427 \n",
      "0m 43s (- 0m 23s) (650 65%) 0.0763 \n",
      "0m 46s (- 0m 19s) (700 70%) 0.0511 \n",
      "0m 49s (- 0m 16s) (750 75%) 0.0573 \n",
      "0m 51s (- 0m 12s) (800 80%) 0.0485 \n",
      "0m 53s (- 0m 9s) (850 85%) 0.0375 \n",
      "0m 55s (- 0m 6s) (900 90%) 0.0679 \n",
      "0m 57s (- 0m 3s) (950 95%) 0.0572 \n",
      "1m 0s (- 0m 0s) (1000 100%) 0.0566 81.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 79.00%\n",
      "0m 8s (- 2m 50s) (50 5%) 0.0559 \n",
      "0m 11s (- 1m 41s) (100 10%) 0.0360 \n",
      "0m 13s (- 1m 16s) (150 15%) 0.0537 \n",
      "0m 16s (- 1m 4s) (200 20%) 0.0376 \n",
      "0m 18s (- 0m 56s) (250 25%) 0.0370 \n",
      "0m 20s (- 0m 48s) (300 30%) 0.0255 \n",
      "0m 23s (- 0m 42s) (350 35%) 0.0454 \n",
      "0m 25s (- 0m 37s) (400 40%) 0.0286 \n",
      "0m 28s (- 0m 34s) (450 45%) 0.0535 \n",
      "0m 30s (- 0m 30s) (500 50%) 0.0490 78.00%\n",
      "0m 35s (- 0m 28s) (550 55%) 0.0605 \n",
      "0m 37s (- 0m 25s) (600 60%) 0.0356 \n",
      "0m 40s (- 0m 21s) (650 65%) 0.0420 \n",
      "0m 42s (- 0m 18s) (700 70%) 0.0317 \n",
      "0m 45s (- 0m 15s) (750 75%) 0.0408 \n",
      "0m 47s (- 0m 11s) (800 80%) 0.1052 \n",
      "0m 50s (- 0m 8s) (850 85%) 0.0390 \n",
      "0m 53s (- 0m 5s) (900 90%) 0.0371 \n",
      "0m 55s (- 0m 2s) (950 95%) 0.0381 \n",
      "0m 58s (- 0m 0s) (1000 100%) 0.0383 85.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 83.40%\n",
      "0m 10s (- 3m 27s) (50 5%) 0.0358 \n",
      "0m 13s (- 2m 1s) (100 10%) 0.0549 \n",
      "0m 16s (- 1m 33s) (150 15%) 0.0409 \n",
      "0m 18s (- 1m 15s) (200 20%) 0.0284 \n",
      "0m 21s (- 1m 5s) (250 25%) 0.0608 \n",
      "0m 24s (- 0m 57s) (300 30%) 0.0460 \n",
      "0m 27s (- 0m 50s) (350 35%) 0.0408 \n",
      "0m 29s (- 0m 44s) (400 40%) 0.0392 \n",
      "0m 31s (- 0m 38s) (450 45%) 0.0369 \n",
      "0m 34s (- 0m 34s) (500 50%) 0.0242 85.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 38s (- 0m 31s) (550 55%) 0.0257 \n",
      "0m 40s (- 0m 26s) (600 60%) 0.0380 \n",
      "0m 42s (- 0m 22s) (650 65%) 0.0482 \n",
      "0m 44s (- 0m 19s) (700 70%) 0.1251 \n",
      "0m 47s (- 0m 15s) (750 75%) 0.0346 \n",
      "0m 49s (- 0m 12s) (800 80%) 0.0241 \n",
      "0m 51s (- 0m 9s) (850 85%) 0.0430 \n",
      "0m 53s (- 0m 5s) (900 90%) 0.0317 \n",
      "0m 55s (- 0m 2s) (950 95%) 0.0807 \n",
      "0m 57s (- 0m 0s) (1000 100%) 0.0241 88.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 85.60%\n",
      "0m 9s (- 2m 58s) (50 5%) 0.0235 \n",
      "0m 11s (- 1m 44s) (100 10%) 0.0331 \n",
      "0m 13s (- 1m 18s) (150 15%) 0.0393 \n",
      "0m 16s (- 1m 5s) (200 20%) 0.0192 \n",
      "0m 18s (- 0m 56s) (250 25%) 0.0220 \n",
      "0m 21s (- 0m 49s) (300 30%) 0.0298 \n",
      "0m 23s (- 0m 44s) (350 35%) 0.0320 \n",
      "0m 26s (- 0m 39s) (400 40%) 0.0253 \n",
      "0m 28s (- 0m 35s) (450 45%) 0.0495 \n",
      "0m 31s (- 0m 31s) (500 50%) 0.0495 85.50%\n",
      "0m 37s (- 0m 30s) (550 55%) 0.0305 \n",
      "0m 39s (- 0m 26s) (600 60%) 0.0196 \n",
      "0m 42s (- 0m 22s) (650 65%) 0.0208 \n",
      "0m 45s (- 0m 19s) (700 70%) 0.0160 \n",
      "0m 47s (- 0m 15s) (750 75%) 0.0243 \n",
      "0m 49s (- 0m 12s) (800 80%) 0.0239 \n",
      "0m 52s (- 0m 9s) (850 85%) 0.0199 \n",
      "0m 55s (- 0m 6s) (900 90%) 0.0230 \n",
      "0m 57s (- 0m 3s) (950 95%) 0.0191 \n",
      "0m 59s (- 0m 0s) (1000 100%) 0.0476 91.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 88.80%\n",
      "0m 10s (- 3m 15s) (50 5%) 0.0397 \n",
      "0m 13s (- 1m 58s) (100 10%) 0.0334 \n",
      "0m 15s (- 1m 29s) (150 15%) 0.0328 \n",
      "0m 18s (- 1m 13s) (200 20%) 0.0334 \n",
      "0m 20s (- 1m 2s) (250 25%) 0.0386 \n",
      "0m 22s (- 0m 53s) (300 30%) 0.0276 \n",
      "0m 24s (- 0m 46s) (350 35%) 0.0593 \n",
      "0m 26s (- 0m 40s) (400 40%) 0.0178 \n",
      "0m 28s (- 0m 35s) (450 45%) 0.0197 \n",
      "0m 31s (- 0m 31s) (500 50%) 0.0327 90.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 37s (- 0m 30s) (550 55%) 0.0364 \n",
      "0m 40s (- 0m 26s) (600 60%) 0.0392 \n",
      "0m 42s (- 0m 22s) (650 65%) 0.0172 \n",
      "0m 45s (- 0m 19s) (700 70%) 0.0313 \n",
      "0m 47s (- 0m 15s) (750 75%) 0.0150 \n",
      "0m 50s (- 0m 12s) (800 80%) 0.0286 \n",
      "0m 52s (- 0m 9s) (850 85%) 0.0199 \n",
      "0m 55s (- 0m 6s) (900 90%) 0.0354 \n",
      "0m 59s (- 0m 3s) (950 95%) 0.0378 \n",
      "1m 1s (- 0m 0s) (1000 100%) 0.0298 89.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 89.40%\n",
      "0m 10s (- 3m 13s) (50 5%) 0.0194 \n",
      "0m 12s (- 1m 52s) (100 10%) 0.0284 \n",
      "0m 14s (- 1m 23s) (150 15%) 0.0219 \n",
      "0m 16s (- 1m 7s) (200 20%) 0.0568 \n",
      "0m 19s (- 0m 57s) (250 25%) 0.0324 \n",
      "0m 21s (- 0m 50s) (300 30%) 0.0176 \n",
      "0m 24s (- 0m 44s) (350 35%) 0.0290 \n",
      "0m 26s (- 0m 39s) (400 40%) 0.0186 \n",
      "0m 29s (- 0m 35s) (450 45%) 0.0202 \n",
      "0m 31s (- 0m 31s) (500 50%) 0.0247 87.00%\n",
      "0m 36s (- 0m 30s) (550 55%) 0.0392 \n",
      "0m 39s (- 0m 26s) (600 60%) 0.0209 \n",
      "0m 41s (- 0m 22s) (650 65%) 0.0214 \n",
      "0m 44s (- 0m 19s) (700 70%) 0.0190 \n",
      "0m 47s (- 0m 15s) (750 75%) 0.0319 \n",
      "0m 49s (- 0m 12s) (800 80%) 0.0319 \n",
      "0m 51s (- 0m 9s) (850 85%) 0.0219 \n",
      "0m 54s (- 0m 6s) (900 90%) 0.0463 \n",
      "0m 57s (- 0m 3s) (950 95%) 0.0174 \n",
      "0m 59s (- 0m 0s) (1000 100%) 0.0133 90.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 91.20%\n",
      "0m 9s (- 2m 59s) (50 5%) 0.0243 \n",
      "0m 12s (- 1m 51s) (100 10%) 0.0248 \n",
      "0m 14s (- 1m 24s) (150 15%) 0.0177 \n",
      "0m 17s (- 1m 10s) (200 20%) 0.0416 \n",
      "0m 20s (- 1m 0s) (250 25%) 0.0593 \n",
      "0m 22s (- 0m 52s) (300 30%) 0.0275 \n",
      "0m 25s (- 0m 46s) (350 35%) 0.0144 \n",
      "0m 27s (- 0m 41s) (400 40%) 0.0148 \n",
      "0m 30s (- 0m 36s) (450 45%) 0.0429 \n",
      "0m 32s (- 0m 32s) (500 50%) 0.0185 95.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 37s (- 0m 30s) (550 55%) 0.0421 \n",
      "0m 40s (- 0m 26s) (600 60%) 0.0195 \n",
      "0m 42s (- 0m 22s) (650 65%) 0.0132 \n",
      "0m 45s (- 0m 19s) (700 70%) 0.0134 \n",
      "0m 47s (- 0m 15s) (750 75%) 0.0324 \n",
      "0m 50s (- 0m 12s) (800 80%) 0.0224 \n",
      "0m 52s (- 0m 9s) (850 85%) 0.0158 \n",
      "0m 55s (- 0m 6s) (900 90%) 0.0210 \n",
      "0m 57s (- 0m 3s) (950 95%) 0.0174 \n",
      "1m 0s (- 0m 0s) (1000 100%) 0.0249 93.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 92.40%\n",
      "0m 8s (- 2m 47s) (50 5%) 0.0242 \n",
      "0m 10s (- 1m 38s) (100 10%) 0.0401 \n",
      "0m 12s (- 1m 13s) (150 15%) 0.0147 \n",
      "0m 14s (- 0m 59s) (200 20%) 0.0188 \n",
      "0m 17s (- 0m 51s) (250 25%) 0.0419 \n",
      "0m 18s (- 0m 44s) (300 30%) 0.0489 \n",
      "0m 20s (- 0m 38s) (350 35%) 0.0143 \n",
      "0m 22s (- 0m 34s) (400 40%) 0.0147 \n",
      "0m 24s (- 0m 30s) (450 45%) 0.0111 \n",
      "0m 26s (- 0m 26s) (500 50%) 0.0294 91.00%\n",
      "0m 30s (- 0m 24s) (550 55%) 0.0146 \n",
      "0m 32s (- 0m 21s) (600 60%) 0.0185 \n",
      "0m 34s (- 0m 18s) (650 65%) 0.0262 \n",
      "0m 36s (- 0m 15s) (700 70%) 0.0133 \n",
      "0m 38s (- 0m 12s) (750 75%) 0.0144 \n",
      "0m 40s (- 0m 10s) (800 80%) 0.0152 \n",
      "0m 42s (- 0m 7s) (850 85%) 0.0105 \n",
      "0m 45s (- 0m 5s) (900 90%) 0.0194 \n",
      "0m 47s (- 0m 2s) (950 95%) 0.0250 \n",
      "0m 49s (- 0m 0s) (1000 100%) 0.0153 93.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 91.40%\n",
      "0m 7s (- 2m 27s) (50 5%) 0.0208 \n",
      "0m 10s (- 1m 31s) (100 10%) 0.0221 \n",
      "0m 13s (- 1m 15s) (150 15%) 0.0321 \n",
      "0m 15s (- 1m 2s) (200 20%) 0.0169 \n",
      "0m 17s (- 0m 53s) (250 25%) 0.0162 \n",
      "0m 20s (- 0m 47s) (300 30%) 0.0146 \n",
      "0m 22s (- 0m 42s) (350 35%) 0.0202 \n",
      "0m 25s (- 0m 38s) (400 40%) 0.0212 \n",
      "0m 27s (- 0m 34s) (450 45%) 0.0130 \n",
      "0m 30s (- 0m 30s) (500 50%) 0.0176 91.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 34s (- 0m 28s) (550 55%) 0.0129 \n",
      "0m 37s (- 0m 25s) (600 60%) 0.0141 \n",
      "0m 40s (- 0m 21s) (650 65%) 0.0678 \n",
      "0m 43s (- 0m 18s) (700 70%) 0.0161 \n",
      "0m 45s (- 0m 15s) (750 75%) 0.0125 \n",
      "0m 48s (- 0m 12s) (800 80%) 0.0134 \n",
      "0m 50s (- 0m 8s) (850 85%) 0.0282 \n",
      "0m 53s (- 0m 5s) (900 90%) 0.0157 \n",
      "0m 55s (- 0m 2s) (950 95%) 0.0178 \n",
      "0m 58s (- 0m 0s) (1000 100%) 0.0153 91.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 91.60%\n",
      "0m 7s (- 2m 27s) (50 5%) 0.0161 \n",
      "0m 10s (- 1m 38s) (100 10%) 0.0314 \n",
      "0m 13s (- 1m 17s) (150 15%) 0.0167 \n",
      "0m 16s (- 1m 5s) (200 20%) 0.0124 \n",
      "0m 18s (- 0m 55s) (250 25%) 0.0175 \n",
      "0m 21s (- 0m 49s) (300 30%) 0.0143 \n",
      "0m 23s (- 0m 44s) (350 35%) 0.0215 \n",
      "0m 26s (- 0m 40s) (400 40%) 0.0113 \n",
      "0m 29s (- 0m 36s) (450 45%) 0.0171 \n",
      "0m 31s (- 0m 31s) (500 50%) 0.0187 93.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 36s (- 0m 29s) (550 55%) 0.0128 \n",
      "0m 38s (- 0m 25s) (600 60%) 0.0096 \n",
      "0m 40s (- 0m 21s) (650 65%) 0.0132 \n",
      "0m 42s (- 0m 18s) (700 70%) 0.0283 \n",
      "0m 44s (- 0m 14s) (750 75%) 0.0140 \n",
      "0m 46s (- 0m 11s) (800 80%) 0.0504 \n",
      "0m 48s (- 0m 8s) (850 85%) 0.0152 \n",
      "0m 50s (- 0m 5s) (900 90%) 0.0116 \n",
      "0m 52s (- 0m 2s) (950 95%) 0.0206 \n",
      "0m 55s (- 0m 0s) (1000 100%) 0.0131 93.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 90.60%\n",
      "0m 7s (- 2m 31s) (50 5%) 0.0249 \n",
      "0m 9s (- 1m 29s) (100 10%) 0.0148 \n",
      "0m 12s (- 1m 9s) (150 15%) 0.0116 \n",
      "0m 14s (- 0m 57s) (200 20%) 0.0127 \n",
      "0m 16s (- 0m 50s) (250 25%) 0.0144 \n",
      "0m 19s (- 0m 44s) (300 30%) 0.0174 \n",
      "0m 21s (- 0m 39s) (350 35%) 0.0347 \n",
      "0m 22s (- 0m 34s) (400 40%) 0.0130 \n",
      "0m 24s (- 0m 30s) (450 45%) 0.0113 \n",
      "0m 27s (- 0m 27s) (500 50%) 0.0139 95.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 31s (- 0m 26s) (550 55%) 0.0107 \n",
      "0m 34s (- 0m 22s) (600 60%) 0.0099 \n",
      "0m 36s (- 0m 19s) (650 65%) 0.0097 \n",
      "0m 38s (- 0m 16s) (700 70%) 0.0107 \n",
      "0m 41s (- 0m 13s) (750 75%) 0.0067 \n",
      "0m 43s (- 0m 10s) (800 80%) 0.0328 \n",
      "0m 46s (- 0m 8s) (850 85%) 0.0084 \n",
      "0m 49s (- 0m 5s) (900 90%) 0.0256 \n",
      "0m 51s (- 0m 2s) (950 95%) 0.0117 \n",
      "0m 54s (- 0m 0s) (1000 100%) 0.0162 92.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 94.40%\n",
      "0m 9s (- 2m 52s) (50 5%) 0.0118 \n",
      "0m 11s (- 1m 45s) (100 10%) 0.0074 \n",
      "0m 14s (- 1m 20s) (150 15%) 0.0108 \n",
      "0m 16s (- 1m 4s) (200 20%) 0.0101 \n",
      "0m 18s (- 0m 54s) (250 25%) 0.0078 \n",
      "0m 21s (- 0m 49s) (300 30%) 0.0143 \n",
      "0m 24s (- 0m 44s) (350 35%) 0.0102 \n",
      "0m 27s (- 0m 41s) (400 40%) 0.0236 \n",
      "0m 29s (- 0m 36s) (450 45%) 0.0157 \n",
      "0m 32s (- 0m 32s) (500 50%) 0.0068 91.50%\n",
      "0m 37s (- 0m 30s) (550 55%) 0.0145 \n",
      "0m 39s (- 0m 26s) (600 60%) 0.0080 \n",
      "0m 42s (- 0m 22s) (650 65%) 0.0066 \n",
      "0m 44s (- 0m 19s) (700 70%) 0.0126 \n",
      "0m 47s (- 0m 15s) (750 75%) 0.0080 \n",
      "0m 49s (- 0m 12s) (800 80%) 0.0131 \n",
      "0m 51s (- 0m 9s) (850 85%) 0.0190 \n",
      "0m 53s (- 0m 5s) (900 90%) 0.0084 \n",
      "0m 55s (- 0m 2s) (950 95%) 0.0106 \n",
      "0m 57s (- 0m 0s) (1000 100%) 0.0099 95.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 95.60%\n",
      "0m 8s (- 5m 20s) (50 2%) 0.0073 \n",
      "0m 11s (- 3m 34s) (100 5%) 0.0161 \n",
      "0m 13s (- 2m 51s) (150 7%) 0.0071 \n",
      "0m 16s (- 2m 26s) (200 10%) 0.0127 \n",
      "0m 18s (- 2m 12s) (250 12%) 0.0076 \n",
      "0m 21s (- 2m 3s) (300 15%) 0.0246 \n",
      "0m 24s (- 1m 55s) (350 17%) 0.0115 \n",
      "0m 27s (- 1m 49s) (400 20%) 0.0067 \n",
      "0m 30s (- 1m 43s) (450 22%) 0.0080 \n",
      "0m 32s (- 1m 37s) (500 25%) 0.0075 97.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 38s (- 1m 40s) (550 27%) 0.0315 \n",
      "0m 40s (- 1m 34s) (600 30%) 0.0077 \n",
      "0m 42s (- 1m 28s) (650 32%) 0.0108 \n",
      "0m 44s (- 1m 22s) (700 35%) 0.0109 \n",
      "0m 46s (- 1m 18s) (750 37%) 0.0088 \n",
      "0m 49s (- 1m 13s) (800 40%) 0.0051 \n",
      "0m 51s (- 1m 9s) (850 42%) 0.0093 \n",
      "0m 53s (- 1m 5s) (900 45%) 0.0068 \n",
      "0m 55s (- 1m 1s) (950 47%) 0.0092 \n",
      "0m 57s (- 0m 57s) (1000 50%) 0.0063 92.00%\n",
      "1m 1s (- 0m 56s) (1050 52%) 0.0146 \n",
      "1m 4s (- 0m 52s) (1100 55%) 0.0251 \n",
      "1m 6s (- 0m 49s) (1150 57%) 0.0085 \n",
      "1m 8s (- 0m 45s) (1200 60%) 0.0092 \n",
      "1m 10s (- 0m 42s) (1250 62%) 0.0223 \n",
      "1m 13s (- 0m 39s) (1300 65%) 0.0113 \n",
      "1m 15s (- 0m 36s) (1350 67%) 0.0053 \n",
      "1m 17s (- 0m 33s) (1400 70%) 0.0074 \n",
      "1m 20s (- 0m 30s) (1450 72%) 0.0174 \n",
      "1m 23s (- 0m 27s) (1500 75%) 0.0448 94.50%\n",
      "1m 29s (- 0m 25s) (1550 77%) 0.0088 \n",
      "1m 33s (- 0m 23s) (1600 80%) 0.0084 \n",
      "1m 36s (- 0m 20s) (1650 82%) 0.0071 \n",
      "1m 38s (- 0m 17s) (1700 85%) 0.0060 \n",
      "1m 41s (- 0m 14s) (1750 87%) 0.0069 \n",
      "1m 43s (- 0m 11s) (1800 90%) 0.0159 \n",
      "1m 46s (- 0m 8s) (1850 92%) 0.0231 \n",
      "1m 49s (- 0m 5s) (1900 95%) 0.0061 \n",
      "1m 51s (- 0m 2s) (1950 97%) 0.0117 \n",
      "1m 53s (- 0m 0s) (2000 100%) 0.0095 96.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 96.40%\n",
      "0m 7s (- 5m 5s) (50 2%) 0.0103 \n",
      "0m 11s (- 3m 38s) (100 5%) 0.0182 \n",
      "0m 14s (- 2m 58s) (150 7%) 0.0097 \n",
      "0m 17s (- 2m 34s) (200 10%) 0.0110 \n",
      "0m 19s (- 2m 18s) (250 12%) 0.0063 \n",
      "0m 22s (- 2m 6s) (300 15%) 0.0095 \n",
      "0m 24s (- 1m 56s) (350 17%) 0.0411 \n",
      "0m 27s (- 1m 50s) (400 20%) 0.0094 \n",
      "0m 31s (- 1m 47s) (450 22%) 0.0100 \n",
      "0m 33s (- 1m 41s) (500 25%) 0.0119 97.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 38s (- 1m 41s) (550 27%) 0.0318 \n",
      "0m 41s (- 1m 36s) (600 30%) 0.0082 \n",
      "0m 44s (- 1m 31s) (650 32%) 0.0112 \n",
      "0m 47s (- 1m 27s) (700 35%) 0.0076 \n",
      "0m 49s (- 1m 22s) (750 37%) 0.0187 \n",
      "0m 52s (- 1m 18s) (800 40%) 0.0098 \n",
      "0m 55s (- 1m 14s) (850 42%) 0.0075 \n",
      "0m 57s (- 1m 10s) (900 45%) 0.0069 \n",
      "1m 0s (- 1m 6s) (950 47%) 0.0203 \n",
      "1m 2s (- 1m 2s) (1000 50%) 0.0063 98.00%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 8s (- 1m 1s) (1050 52%) 0.0100 \n",
      "1m 10s (- 0m 57s) (1100 55%) 0.0070 \n",
      "1m 13s (- 0m 53s) (1150 57%) 0.0094 \n",
      "1m 15s (- 0m 50s) (1200 60%) 0.0120 \n",
      "1m 18s (- 0m 47s) (1250 62%) 0.0213 \n",
      "1m 20s (- 0m 43s) (1300 65%) 0.0059 \n",
      "1m 23s (- 0m 40s) (1350 67%) 0.0082 \n",
      "1m 26s (- 0m 37s) (1400 70%) 0.0109 \n",
      "1m 29s (- 0m 33s) (1450 72%) 0.0090 \n",
      "1m 31s (- 0m 30s) (1500 75%) 0.0154 99.00%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 37s (- 0m 28s) (1550 77%) 0.0259 \n",
      "1m 40s (- 0m 25s) (1600 80%) 0.0102 \n",
      "1m 42s (- 0m 21s) (1650 82%) 0.0105 \n",
      "1m 45s (- 0m 18s) (1700 85%) 0.0081 \n",
      "1m 47s (- 0m 15s) (1750 87%) 0.0117 \n",
      "1m 50s (- 0m 12s) (1800 90%) 0.0092 \n",
      "1m 52s (- 0m 9s) (1850 92%) 0.0084 \n",
      "1m 55s (- 0m 6s) (1900 95%) 0.0150 \n",
      "1m 57s (- 0m 3s) (1950 97%) 0.0059 \n",
      "2m 0s (- 0m 0s) (2000 100%) 0.0155 97.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 96.80%\n",
      "0m 9s (- 2m 56s) (50 5%) 0.0053 \n",
      "0m 11s (- 1m 42s) (100 10%) 0.0083 \n",
      "0m 13s (- 1m 17s) (150 15%) 0.0063 \n",
      "0m 16s (- 1m 4s) (200 20%) 0.0076 \n",
      "0m 18s (- 0m 55s) (250 25%) 0.0145 \n",
      "0m 20s (- 0m 48s) (300 30%) 0.0112 \n",
      "0m 23s (- 0m 43s) (350 35%) 0.0203 \n",
      "0m 26s (- 0m 39s) (400 40%) 0.0100 \n",
      "0m 28s (- 0m 34s) (450 45%) 0.0070 \n",
      "0m 31s (- 0m 31s) (500 50%) 0.0214 96.00%\n",
      "0m 37s (- 0m 30s) (550 55%) 0.0313 \n",
      "0m 39s (- 0m 26s) (600 60%) 0.0215 \n",
      "0m 42s (- 0m 22s) (650 65%) 0.0090 \n",
      "0m 45s (- 0m 19s) (700 70%) 0.0078 \n",
      "0m 47s (- 0m 15s) (750 75%) 0.0093 \n",
      "0m 50s (- 0m 12s) (800 80%) 0.0079 \n",
      "0m 52s (- 0m 9s) (850 85%) 0.0096 \n",
      "0m 55s (- 0m 6s) (900 90%) 0.0066 \n",
      "0m 57s (- 0m 3s) (950 95%) 0.0092 \n",
      "0m 59s (- 0m 0s) (1000 100%) 0.0072 97.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 97.80%\n",
      "0m 9s (- 2m 54s) (50 5%) 0.0246 \n",
      "0m 11s (- 1m 42s) (100 10%) 0.0068 \n",
      "0m 13s (- 1m 18s) (150 15%) 0.0063 \n",
      "0m 15s (- 1m 3s) (200 20%) 0.0108 \n",
      "0m 17s (- 0m 53s) (250 25%) 0.0346 \n",
      "0m 19s (- 0m 46s) (300 30%) 0.0070 \n",
      "0m 21s (- 0m 40s) (350 35%) 0.0072 \n",
      "0m 23s (- 0m 35s) (400 40%) 0.0072 \n",
      "0m 25s (- 0m 31s) (450 45%) 0.0402 \n",
      "0m 28s (- 0m 28s) (500 50%) 0.0103 97.00%\n",
      "0m 32s (- 0m 26s) (550 55%) 0.0067 \n",
      "0m 34s (- 0m 23s) (600 60%) 0.0100 \n",
      "0m 36s (- 0m 19s) (650 65%) 0.0078 \n",
      "0m 38s (- 0m 16s) (700 70%) 0.0085 \n",
      "0m 40s (- 0m 13s) (750 75%) 0.0072 \n",
      "0m 42s (- 0m 10s) (800 80%) 0.0151 \n",
      "0m 44s (- 0m 7s) (850 85%) 0.0101 \n",
      "0m 46s (- 0m 5s) (900 90%) 0.0067 \n",
      "0m 48s (- 0m 2s) (950 95%) 0.0104 \n",
      "0m 50s (- 0m 0s) (1000 100%) 0.0107 97.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96999999999999997"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "    encoder2 = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "    decoder2 = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "evaluateAccuracy(encoder2, decoder2, n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> jump around right twice and look around right twice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK <EOS>\n",
      "\n",
      "> jump thrice and walk left thrice\n",
      "= I_JUMP I_JUMP I_JUMP I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK\n",
      "< I_JUMP I_JUMP I_JUMP I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK <EOS>\n",
      "\n",
      "> jump opposite left thrice after look right thrice\n",
      "= I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP\n",
      "< I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> look left and jump right\n",
      "= I_TURN_LEFT I_LOOK I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_LEFT I_LOOK I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> run right thrice and jump around right thrice\n",
      "= I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> jump right thrice and jump left thrice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> run right thrice and jump around left thrice\n",
      "= I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP\n",
      "< I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> jump around right twice after walk around right thrice\n",
      "= I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> jump around left twice and jump opposite left twice\n",
      "= I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP\n",
      "< I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> jump opposite right twice after run opposite right twice\n",
      "= I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

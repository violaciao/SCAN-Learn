{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCAN Add-Prim JUMP Experiment\n",
    "*************************************************************\n",
    "\n",
    "Reference: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Python 3.6\n",
    "* PyTorch 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Device is using\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "TASK_NAME = \"addprim-jump\"\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False, trainOrtest='train'):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines        \n",
    "    lines = open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/processed/{}-{}_{}-{}.txt'.\\\n",
    "                 format(trainOrtest, TASK_NAME, lang1, lang2), encoding='utf-8').\\\n",
    "                 read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "# PRED_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 37046 sentence pairs\n",
      "Trimmed to 37046 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['jump thrice after jump right thrice', 'I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_JUMP I_JUMP I_JUMP']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataFrom='train'):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=False, trainOrtest=dataFrom)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('in', 'out', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "=================\n",
    "\n",
    "The model we are using is a GRU encoder-decoder seq2seq model with attention mechanism. In order to solve the zero-shot generalization task, we embed the encoder networks with pre-trained embeddings, from GloVe and Google Word2Vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_SOURCE = 'glove'\n",
    "hidden_size = 50\n",
    "\n",
    "if EMBEDDEING_SOURCE == 'google':\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_GoogleNews300Negative.pkl', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "else:\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_raw{}d.pkl'.format(hidden_size), 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "\n",
    "pretrained_emb = np.zeros((input_lang.n_words, hidden_size))\n",
    "for k, v in input_lang.index2word.items():\n",
    "    if v == 'SOS':\n",
    "        pretrained_emb[k] = np.zeros(hidden_size)\n",
    "    elif (v == 'EOS') and (EMBEDDEING_SOURCE != 'google'):\n",
    "        pretrained_emb[k] = b['.']\n",
    "    elif (v == 'and') and (EMBEDDEING_SOURCE == 'google'):\n",
    "        pretrained_emb[k] = b['AND']\n",
    "    else:\n",
    "        pretrained_emb[k] = b[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of this seq2seq network is a GRU netword. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_PRETRAINED = True\n",
    "WEIGHT_UPDATE = False\n",
    "\n",
    "MODEL_VERSION = 'T0.4_glv50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        if EMBEDDEING_PRETRAINED:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "            self.embedding.weight.requires_grad = WEIGHT_UPDATE\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is a GRU network with attention mechanism that takes the last output of the encoder and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "First we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "We use teacher forcing to help converge faster with a delay fashion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, \n",
    "          max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for timing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, eval_every=1000, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "        encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "        decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "        \n",
    "    best_test_acc = evaluateAccuracy(encoder, decoder, 500)\n",
    "    print(\"Best evaluation accuracy: {0:.2f}%\".format(best_test_acc * 100))\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        \n",
    "    encoder_optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg), end=' ')\n",
    "            \n",
    "            if iter % eval_every == 0:\n",
    "                test_acc = evaluateAccuracy(encoder, decoder, 200)\n",
    "                print('{0:.2f}%'.format(test_acc * 100))\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    with open(\"saved_models/encoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(encoder, f)\n",
    "                    with open(\"saved_models/decoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(decoder, f)\n",
    "                    print(\"New best test accuracy! Model Updated!\")\n",
    "                    best_test_acc = test_acc\n",
    "#                 elif test_acc < best_test_acc - 0.001:\n",
    "#                     encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "#                     decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "                    \n",
    "            else:\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 15412 sentence pairs\n",
      "Trimmed to 15412 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['turn opposite left and jump around left twice', 'I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs_eval = prepareData('in', 'out', True, dataFrom='test')\n",
    "print(random.choice(pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateAccuracy(encoder, decoder, n=10):\n",
    "    ACCs = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        \n",
    "        if output_words[-1] == '<EOS>':\n",
    "            output_words = output_words[:-1]\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        if output_sentence == pair[1]:\n",
    "            ACCs.append(1)\n",
    "        else:\n",
    "            ACCs.append(0)\n",
    "    return np.array(ACCs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initially trained with a higher teacher aid, and relatively large learning rate. Both teacher forcing effect and the learning rate decay over iterations when the model approaches the optimum.  \n",
    "\n",
    "#### The model achieves 97% accuracy rate for the best test sample evaluation, and is 94% correct on average for the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 0.00%\n",
      "0m 15s (- 26m 1s) (50 1%) 1.9284 \n",
      "0m 17s (- 14m 10s) (100 2%) 1.7474 \n",
      "0m 19s (- 10m 22s) (150 3%) 1.6616 \n",
      "0m 20s (- 8m 22s) (200 4%) 1.5682 \n",
      "0m 22s (- 7m 6s) (250 5%) 1.4619 \n",
      "0m 23s (- 6m 15s) (300 6%) 1.3767 \n",
      "0m 25s (- 5m 38s) (350 7%) 1.3701 \n",
      "0m 26s (- 5m 10s) (400 8%) 1.2315 \n",
      "0m 28s (- 4m 47s) (450 9%) 1.2413 \n",
      "0m 30s (- 4m 34s) (500 10%) 1.0623 0.00%\n",
      "0m 39s (- 5m 16s) (550 11%) 1.3117 \n",
      "0m 40s (- 4m 59s) (600 12%) 1.2106 \n",
      "0m 42s (- 4m 43s) (650 13%) 1.1701 \n",
      "0m 44s (- 4m 30s) (700 14%) 1.1185 \n",
      "0m 45s (- 4m 17s) (750 15%) 1.1089 \n",
      "0m 47s (- 4m 8s) (800 16%) 1.0465 \n",
      "0m 49s (- 4m 0s) (850 17%) 1.1710 \n",
      "0m 51s (- 3m 53s) (900 18%) 1.1640 \n",
      "0m 52s (- 3m 45s) (950 19%) 1.0087 \n",
      "0m 54s (- 3m 37s) (1000 20%) 1.0207 0.00%\n",
      "1m 1s (- 3m 52s) (1050 21%) 1.0772 \n",
      "1m 3s (- 3m 44s) (1100 22%) 0.8944 \n",
      "1m 4s (- 3m 37s) (1150 23%) 0.9568 \n",
      "1m 6s (- 3m 30s) (1200 24%) 1.1029 \n",
      "1m 8s (- 3m 24s) (1250 25%) 0.9491 \n",
      "1m 10s (- 3m 21s) (1300 26%) 1.0088 \n",
      "1m 13s (- 3m 17s) (1350 27%) 1.0875 \n",
      "1m 14s (- 3m 12s) (1400 28%) 1.0120 \n",
      "1m 16s (- 3m 7s) (1450 28%) 0.9658 \n",
      "1m 18s (- 3m 3s) (1500 30%) 0.9667 0.00%\n",
      "1m 25s (- 3m 9s) (1550 31%) 0.9240 \n",
      "1m 26s (- 3m 3s) (1600 32%) 0.9620 \n",
      "1m 28s (- 2m 59s) (1650 33%) 1.0356 \n",
      "1m 29s (- 2m 54s) (1700 34%) 0.9634 \n",
      "1m 31s (- 2m 49s) (1750 35%) 0.8387 \n",
      "1m 33s (- 2m 45s) (1800 36%) 0.9014 \n",
      "1m 34s (- 2m 40s) (1850 37%) 0.9882 \n",
      "1m 35s (- 2m 36s) (1900 38%) 1.0103 \n",
      "1m 37s (- 2m 32s) (1950 39%) 0.9024 \n",
      "1m 38s (- 2m 28s) (2000 40%) 0.9797 0.00%\n",
      "1m 45s (- 2m 31s) (2050 41%) 0.9682 \n",
      "1m 46s (- 2m 27s) (2100 42%) 0.8239 \n",
      "1m 48s (- 2m 23s) (2150 43%) 0.8503 \n",
      "1m 49s (- 2m 19s) (2200 44%) 0.8352 \n",
      "1m 51s (- 2m 15s) (2250 45%) 1.1108 \n",
      "1m 52s (- 2m 12s) (2300 46%) 0.9414 \n",
      "1m 54s (- 2m 8s) (2350 47%) 0.9567 \n",
      "1m 55s (- 2m 5s) (2400 48%) 0.8559 \n",
      "1m 57s (- 2m 2s) (2450 49%) 0.8127 \n",
      "1m 59s (- 1m 59s) (2500 50%) 0.7827 0.00%\n",
      "2m 5s (- 2m 0s) (2550 51%) 0.7956 \n",
      "2m 7s (- 1m 57s) (2600 52%) 0.8519 \n",
      "2m 8s (- 1m 54s) (2650 53%) 0.8044 \n",
      "2m 10s (- 1m 50s) (2700 54%) 0.7057 \n",
      "2m 11s (- 1m 47s) (2750 55%) 0.6922 \n",
      "2m 13s (- 1m 44s) (2800 56%) 0.7836 \n",
      "2m 14s (- 1m 41s) (2850 56%) 0.7457 \n",
      "2m 16s (- 1m 38s) (2900 57%) 0.7608 \n",
      "2m 17s (- 1m 35s) (2950 59%) 0.7620 \n",
      "2m 19s (- 1m 33s) (3000 60%) 0.7157 0.00%\n",
      "2m 25s (- 1m 32s) (3050 61%) 0.8530 \n",
      "2m 26s (- 1m 29s) (3100 62%) 0.7576 \n",
      "2m 28s (- 1m 27s) (3150 63%) 0.8106 \n",
      "2m 29s (- 1m 24s) (3200 64%) 0.6865 \n",
      "2m 31s (- 1m 21s) (3250 65%) 0.6951 \n",
      "2m 32s (- 1m 18s) (3300 66%) 0.7075 \n",
      "2m 34s (- 1m 16s) (3350 67%) 0.7273 \n",
      "2m 35s (- 1m 13s) (3400 68%) 0.6670 \n",
      "2m 37s (- 1m 10s) (3450 69%) 0.6719 \n",
      "2m 39s (- 1m 8s) (3500 70%) 0.7415 0.00%\n",
      "2m 45s (- 1m 7s) (3550 71%) 0.7559 \n",
      "2m 47s (- 1m 4s) (3600 72%) 0.5846 \n",
      "2m 48s (- 1m 2s) (3650 73%) 0.6436 \n",
      "2m 50s (- 0m 59s) (3700 74%) 0.6050 \n",
      "2m 51s (- 0m 57s) (3750 75%) 0.6328 \n",
      "2m 52s (- 0m 54s) (3800 76%) 0.6667 \n",
      "2m 54s (- 0m 52s) (3850 77%) 0.7113 \n",
      "2m 56s (- 0m 49s) (3900 78%) 0.6292 \n",
      "2m 57s (- 0m 47s) (3950 79%) 0.5941 \n",
      "2m 59s (- 0m 44s) (4000 80%) 0.6894 0.00%\n",
      "3m 5s (- 0m 43s) (4050 81%) 0.7640 \n",
      "3m 6s (- 0m 40s) (4100 82%) 0.6658 \n",
      "3m 8s (- 0m 38s) (4150 83%) 0.6278 \n",
      "3m 9s (- 0m 36s) (4200 84%) 0.6036 \n",
      "3m 10s (- 0m 33s) (4250 85%) 0.7215 \n",
      "3m 12s (- 0m 31s) (4300 86%) 0.6843 \n",
      "3m 13s (- 0m 28s) (4350 87%) 0.6562 \n",
      "3m 15s (- 0m 26s) (4400 88%) 0.7279 \n",
      "3m 16s (- 0m 24s) (4450 89%) 0.6109 \n",
      "3m 18s (- 0m 22s) (4500 90%) 0.6103 0.50%\n",
      "New best test accuracy! Model Updated!\n",
      "3m 24s (- 0m 20s) (4550 91%) 0.6007 \n",
      "3m 25s (- 0m 17s) (4600 92%) 0.5911 \n",
      "3m 27s (- 0m 15s) (4650 93%) 0.4672 \n",
      "3m 28s (- 0m 13s) (4700 94%) 0.6017 \n",
      "3m 30s (- 0m 11s) (4750 95%) 0.6030 \n",
      "3m 31s (- 0m 8s) (4800 96%) 0.5344 \n",
      "3m 33s (- 0m 6s) (4850 97%) 0.5815 \n",
      "3m 34s (- 0m 4s) (4900 98%) 0.6309 \n",
      "3m 36s (- 0m 2s) (4950 99%) 0.5653 \n",
      "3m 37s (- 0m 0s) (5000 100%) 0.4803 1.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 0.60%\n",
      "0m 10s (- 3m 19s) (50 5%) 0.6257 \n",
      "0m 11s (- 1m 47s) (100 10%) 0.7002 \n",
      "0m 13s (- 1m 16s) (150 15%) 0.6019 \n",
      "0m 15s (- 1m 0s) (200 20%) 0.7238 \n",
      "0m 16s (- 0m 49s) (250 25%) 0.6301 \n",
      "0m 18s (- 0m 42s) (300 30%) 0.6415 \n",
      "0m 19s (- 0m 36s) (350 35%) 0.7643 \n",
      "0m 21s (- 0m 31s) (400 40%) 0.6502 \n",
      "0m 22s (- 0m 27s) (450 45%) 0.6610 \n",
      "0m 23s (- 0m 23s) (500 50%) 0.5823 1.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 28s (- 0m 23s) (550 55%) 0.6141 \n",
      "0m 30s (- 0m 20s) (600 60%) 0.5827 \n",
      "0m 31s (- 0m 17s) (650 65%) 0.6119 \n",
      "0m 33s (- 0m 14s) (700 70%) 0.7216 \n",
      "0m 34s (- 0m 11s) (750 75%) 0.5672 \n",
      "0m 36s (- 0m 9s) (800 80%) 0.6139 \n",
      "0m 37s (- 0m 6s) (850 85%) 0.6459 \n",
      "0m 38s (- 0m 4s) (900 90%) 0.5812 \n",
      "0m 40s (- 0m 2s) (950 95%) 0.5373 \n",
      "0m 41s (- 0m 0s) (1000 100%) 0.6255 2.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 2.40%\n",
      "0m 5s (- 1m 42s) (50 5%) 0.5556 \n",
      "0m 6s (- 0m 56s) (100 10%) 0.5471 \n",
      "0m 7s (- 0m 41s) (150 15%) 0.5851 \n",
      "0m 8s (- 0m 33s) (200 20%) 0.7022 \n",
      "0m 9s (- 0m 28s) (250 25%) 0.6876 \n",
      "0m 10s (- 0m 24s) (300 30%) 0.5488 \n",
      "0m 11s (- 0m 20s) (350 35%) 0.5184 \n",
      "0m 12s (- 0m 18s) (400 40%) 0.5934 \n",
      "0m 13s (- 0m 15s) (450 45%) 0.6456 \n",
      "0m 14s (- 0m 14s) (500 50%) 0.5957 1.50%\n",
      "0m 17s (- 0m 14s) (550 55%) 0.5451 \n",
      "0m 18s (- 0m 12s) (600 60%) 0.6204 \n",
      "0m 20s (- 0m 10s) (650 65%) 0.5905 \n",
      "0m 21s (- 0m 9s) (700 70%) 0.4906 \n",
      "0m 23s (- 0m 7s) (750 75%) 0.5889 \n",
      "0m 24s (- 0m 6s) (800 80%) 0.6090 \n",
      "0m 26s (- 0m 4s) (850 85%) 0.5920 \n",
      "0m 28s (- 0m 3s) (900 90%) 0.6193 \n",
      "0m 29s (- 0m 1s) (950 95%) 0.6804 \n",
      "0m 30s (- 0m 0s) (1000 100%) 0.5919 3.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 4.20%\n",
      "0m 8s (- 2m 44s) (50 5%) 0.5123 \n",
      "0m 10s (- 1m 30s) (100 10%) 0.5919 \n",
      "0m 11s (- 1m 6s) (150 15%) 0.5420 \n",
      "0m 13s (- 0m 53s) (200 20%) 0.5056 \n",
      "0m 14s (- 0m 43s) (250 25%) 0.6662 \n",
      "0m 16s (- 0m 37s) (300 30%) 0.6343 \n",
      "0m 17s (- 0m 32s) (350 35%) 0.6375 \n",
      "0m 19s (- 0m 28s) (400 40%) 0.5405 \n",
      "0m 20s (- 0m 25s) (450 45%) 0.5012 \n",
      "0m 22s (- 0m 22s) (500 50%) 0.5384 8.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 26s (- 0m 21s) (550 55%) 0.4594 \n",
      "0m 28s (- 0m 18s) (600 60%) 0.4834 \n",
      "0m 29s (- 0m 15s) (650 65%) 0.4953 \n",
      "0m 31s (- 0m 13s) (700 70%) 0.5596 \n",
      "0m 32s (- 0m 10s) (750 75%) 0.5847 \n",
      "0m 34s (- 0m 8s) (800 80%) 0.5266 \n",
      "0m 35s (- 0m 6s) (850 85%) 0.5564 \n",
      "0m 36s (- 0m 4s) (900 90%) 0.4839 \n",
      "0m 38s (- 0m 2s) (950 95%) 0.5087 \n",
      "0m 40s (- 0m 0s) (1000 100%) 0.5528 7.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 5.80%\n",
      "0m 6s (- 1m 54s) (50 5%) 0.4246 \n",
      "0m 6s (- 1m 2s) (100 10%) 0.4152 \n",
      "0m 7s (- 0m 45s) (150 15%) 0.4982 \n",
      "0m 9s (- 0m 36s) (200 20%) 0.5420 \n",
      "0m 9s (- 0m 29s) (250 25%) 0.4766 \n",
      "0m 10s (- 0m 25s) (300 30%) 0.5508 \n",
      "0m 11s (- 0m 21s) (350 35%) 0.4823 \n",
      "0m 12s (- 0m 18s) (400 40%) 0.4037 \n",
      "0m 13s (- 0m 16s) (450 45%) 0.4792 \n",
      "0m 14s (- 0m 14s) (500 50%) 0.5292 4.00%\n",
      "0m 17s (- 0m 14s) (550 55%) 0.5050 \n",
      "0m 18s (- 0m 12s) (600 60%) 0.3935 \n",
      "0m 19s (- 0m 10s) (650 65%) 0.4860 \n",
      "0m 20s (- 0m 8s) (700 70%) 0.4790 \n",
      "0m 21s (- 0m 7s) (750 75%) 0.4174 \n",
      "0m 22s (- 0m 5s) (800 80%) 0.4228 \n",
      "0m 23s (- 0m 4s) (850 85%) 0.5064 \n",
      "0m 24s (- 0m 2s) (900 90%) 0.4953 \n",
      "0m 25s (- 0m 1s) (950 95%) 0.4291 \n",
      "0m 26s (- 0m 0s) (1000 100%) 0.3717 8.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 7.20%\n",
      "0m 6s (- 1m 54s) (50 5%) 0.4579 \n",
      "0m 6s (- 1m 1s) (100 10%) 0.4575 \n",
      "0m 7s (- 0m 44s) (150 15%) 0.4038 \n",
      "0m 8s (- 0m 35s) (200 20%) 0.4190 \n",
      "0m 9s (- 0m 28s) (250 25%) 0.4342 \n",
      "0m 10s (- 0m 24s) (300 30%) 0.3723 \n",
      "0m 12s (- 0m 22s) (350 35%) 0.4845 \n",
      "0m 13s (- 0m 19s) (400 40%) 0.4840 \n",
      "0m 14s (- 0m 17s) (450 45%) 0.4308 \n",
      "0m 15s (- 0m 15s) (500 50%) 0.3744 5.50%\n",
      "0m 18s (- 0m 14s) (550 55%) 0.5235 \n",
      "0m 19s (- 0m 12s) (600 60%) 0.4299 \n",
      "0m 20s (- 0m 10s) (650 65%) 0.4329 \n",
      "0m 21s (- 0m 9s) (700 70%) 0.4555 \n",
      "0m 22s (- 0m 7s) (750 75%) 0.4128 \n",
      "0m 23s (- 0m 5s) (800 80%) 0.3845 \n",
      "0m 25s (- 0m 4s) (850 85%) 0.4594 \n",
      "0m 26s (- 0m 2s) (900 90%) 0.4676 \n",
      "0m 27s (- 0m 1s) (950 95%) 0.3726 \n",
      "0m 28s (- 0m 0s) (1000 100%) 0.3975 10.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 9.20%\n",
      "0m 6s (- 2m 11s) (50 5%) 0.4176 \n",
      "0m 8s (- 1m 12s) (100 10%) 0.3750 \n",
      "0m 8s (- 0m 50s) (150 15%) 0.5005 \n",
      "0m 9s (- 0m 39s) (200 20%) 0.4292 \n",
      "0m 10s (- 0m 32s) (250 25%) 0.4185 \n",
      "0m 12s (- 0m 28s) (300 30%) 0.4218 \n",
      "0m 13s (- 0m 24s) (350 35%) 0.3495 \n",
      "0m 14s (- 0m 21s) (400 40%) 0.3991 \n",
      "0m 15s (- 0m 19s) (450 45%) 0.3999 \n",
      "0m 16s (- 0m 16s) (500 50%) 0.4338 10.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 19s (- 0m 16s) (550 55%) 0.3504 \n",
      "0m 21s (- 0m 14s) (600 60%) 0.4174 \n",
      "0m 22s (- 0m 11s) (650 65%) 0.4201 \n",
      "0m 23s (- 0m 10s) (700 70%) 0.3734 \n",
      "0m 24s (- 0m 8s) (750 75%) 0.4381 \n",
      "0m 25s (- 0m 6s) (800 80%) 0.3593 \n",
      "0m 26s (- 0m 4s) (850 85%) 0.5190 \n",
      "0m 28s (- 0m 3s) (900 90%) 0.4696 \n",
      "0m 29s (- 0m 1s) (950 95%) 0.4379 \n",
      "0m 30s (- 0m 0s) (1000 100%) 0.3647 11.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 9.00%\n",
      "0m 5s (- 1m 46s) (50 5%) 0.3681 \n",
      "0m 6s (- 1m 0s) (100 10%) 0.3404 \n",
      "0m 7s (- 0m 43s) (150 15%) 0.3969 \n",
      "0m 8s (- 0m 35s) (200 20%) 0.4362 \n",
      "0m 9s (- 0m 29s) (250 25%) 0.3245 \n",
      "0m 11s (- 0m 25s) (300 30%) 0.3340 \n",
      "0m 12s (- 0m 22s) (350 35%) 0.4027 \n",
      "0m 13s (- 0m 19s) (400 40%) 0.3720 \n",
      "0m 14s (- 0m 17s) (450 45%) 0.2879 \n",
      "0m 15s (- 0m 15s) (500 50%) 0.3876 22.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 18s (- 0m 14s) (550 55%) 0.4775 \n",
      "0m 19s (- 0m 12s) (600 60%) 0.3579 \n",
      "0m 20s (- 0m 10s) (650 65%) 0.4076 \n",
      "0m 21s (- 0m 9s) (700 70%) 0.3535 \n",
      "0m 22s (- 0m 7s) (750 75%) 0.3926 \n",
      "0m 23s (- 0m 5s) (800 80%) 0.2908 \n",
      "0m 24s (- 0m 4s) (850 85%) 0.3936 \n",
      "0m 25s (- 0m 2s) (900 90%) 0.3876 \n",
      "0m 26s (- 0m 1s) (950 95%) 0.3626 \n",
      "0m 27s (- 0m 0s) (1000 100%) 0.3500 19.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 13.20%\n",
      "0m 6s (- 1m 56s) (50 5%) 0.3677 \n",
      "0m 6s (- 1m 2s) (100 10%) 0.3660 \n",
      "0m 7s (- 0m 44s) (150 15%) 0.4475 \n",
      "0m 8s (- 0m 34s) (200 20%) 0.3972 \n",
      "0m 9s (- 0m 29s) (250 25%) 0.3881 \n",
      "0m 10s (- 0m 25s) (300 30%) 0.3622 \n",
      "0m 12s (- 0m 22s) (350 35%) 0.3845 \n",
      "0m 13s (- 0m 19s) (400 40%) 0.3785 \n",
      "0m 13s (- 0m 17s) (450 45%) 0.3637 \n",
      "0m 15s (- 0m 15s) (500 50%) 0.3296 19.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 18s (- 0m 14s) (550 55%) 0.2972 \n",
      "0m 19s (- 0m 12s) (600 60%) 0.2873 \n",
      "0m 20s (- 0m 11s) (650 65%) 0.2903 \n",
      "0m 21s (- 0m 9s) (700 70%) 0.3328 \n",
      "0m 22s (- 0m 7s) (750 75%) 0.3653 \n",
      "0m 23s (- 0m 5s) (800 80%) 0.3441 \n",
      "0m 24s (- 0m 4s) (850 85%) 0.3209 \n",
      "0m 25s (- 0m 2s) (900 90%) 0.3516 \n",
      "0m 26s (- 0m 1s) (950 95%) 0.3505 \n",
      "0m 27s (- 0m 0s) (1000 100%) 0.3819 15.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 14.80%\n",
      "0m 5s (- 1m 50s) (50 5%) 0.3785 \n",
      "0m 6s (- 1m 1s) (100 10%) 0.4159 \n",
      "0m 8s (- 0m 46s) (150 15%) 0.3920 \n",
      "0m 9s (- 0m 37s) (200 20%) 0.3423 \n",
      "0m 10s (- 0m 32s) (250 25%) 0.3268 \n",
      "0m 11s (- 0m 27s) (300 30%) 0.3301 \n",
      "0m 13s (- 0m 24s) (350 35%) 0.3614 \n",
      "0m 14s (- 0m 21s) (400 40%) 0.5037 \n",
      "0m 15s (- 0m 19s) (450 45%) 0.3353 \n",
      "0m 17s (- 0m 17s) (500 50%) 0.3746 13.00%\n",
      "0m 22s (- 0m 18s) (550 55%) 0.3070 \n",
      "0m 23s (- 0m 15s) (600 60%) 0.3506 \n",
      "0m 25s (- 0m 13s) (650 65%) 0.3548 \n",
      "0m 26s (- 0m 11s) (700 70%) 0.3524 \n",
      "0m 27s (- 0m 9s) (750 75%) 0.3401 \n",
      "0m 29s (- 0m 7s) (800 80%) 0.3377 \n",
      "0m 31s (- 0m 5s) (850 85%) 0.3205 \n",
      "0m 32s (- 0m 3s) (900 90%) 0.4070 \n",
      "0m 33s (- 0m 1s) (950 95%) 0.4408 \n",
      "0m 35s (- 0m 0s) (1000 100%) 0.3131 17.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 18.80%\n",
      "0m 7s (- 2m 31s) (50 5%) 0.2603 \n",
      "0m 9s (- 1m 26s) (100 10%) 0.3881 \n",
      "0m 10s (- 1m 2s) (150 15%) 0.3755 \n",
      "0m 12s (- 0m 48s) (200 20%) 0.3415 \n",
      "0m 13s (- 0m 40s) (250 25%) 0.3404 \n",
      "0m 14s (- 0m 34s) (300 30%) 0.3882 \n",
      "0m 16s (- 0m 30s) (350 35%) 0.3955 \n",
      "0m 17s (- 0m 26s) (400 40%) 0.3331 \n",
      "0m 19s (- 0m 24s) (450 45%) 0.3724 \n",
      "0m 21s (- 0m 21s) (500 50%) 0.2954 19.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 24s (- 0m 20s) (550 55%) 0.3652 \n",
      "0m 26s (- 0m 17s) (600 60%) 0.4008 \n",
      "0m 28s (- 0m 15s) (650 65%) 0.3297 \n",
      "0m 29s (- 0m 12s) (700 70%) 0.3229 \n",
      "0m 31s (- 0m 10s) (750 75%) 0.3197 \n",
      "0m 32s (- 0m 8s) (800 80%) 0.3918 \n",
      "0m 34s (- 0m 6s) (850 85%) 0.3950 \n",
      "0m 35s (- 0m 3s) (900 90%) 0.3361 \n",
      "0m 36s (- 0m 1s) (950 95%) 0.3172 \n",
      "0m 38s (- 0m 0s) (1000 100%) 0.3350 17.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 15.60%\n",
      "0m 5s (- 1m 43s) (50 5%) 0.3388 \n",
      "0m 6s (- 0m 58s) (100 10%) 0.3973 \n",
      "0m 7s (- 0m 42s) (150 15%) 0.3323 \n",
      "0m 8s (- 0m 35s) (200 20%) 0.3246 \n",
      "0m 10s (- 0m 30s) (250 25%) 0.4174 \n",
      "0m 11s (- 0m 27s) (300 30%) 0.3779 \n",
      "0m 13s (- 0m 24s) (350 35%) 0.3093 \n",
      "0m 14s (- 0m 21s) (400 40%) 0.3645 \n",
      "0m 15s (- 0m 19s) (450 45%) 0.3071 \n",
      "0m 16s (- 0m 16s) (500 50%) 0.3279 17.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 20s (- 0m 16s) (550 55%) 0.3580 \n",
      "0m 21s (- 0m 14s) (600 60%) 0.3908 \n",
      "0m 22s (- 0m 12s) (650 65%) 0.3340 \n",
      "0m 23s (- 0m 10s) (700 70%) 0.3336 \n",
      "0m 24s (- 0m 8s) (750 75%) 0.3402 \n",
      "0m 26s (- 0m 6s) (800 80%) 0.2863 \n",
      "0m 27s (- 0m 4s) (850 85%) 0.3023 \n",
      "0m 28s (- 0m 3s) (900 90%) 0.4773 \n",
      "0m 30s (- 0m 1s) (950 95%) 0.3348 \n",
      "0m 31s (- 0m 0s) (1000 100%) 0.3915 21.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 19.00%\n",
      "0m 5s (- 1m 50s) (50 5%) 0.3824 \n",
      "0m 7s (- 1m 3s) (100 10%) 0.3377 \n",
      "0m 8s (- 0m 47s) (150 15%) 0.3240 \n",
      "0m 9s (- 0m 38s) (200 20%) 0.3259 \n",
      "0m 11s (- 0m 33s) (250 25%) 0.3816 \n",
      "0m 12s (- 0m 28s) (300 30%) 0.3572 \n",
      "0m 13s (- 0m 25s) (350 35%) 0.4397 \n",
      "0m 14s (- 0m 22s) (400 40%) 0.3530 \n",
      "0m 16s (- 0m 19s) (450 45%) 0.3900 \n",
      "0m 17s (- 0m 17s) (500 50%) 0.3139 19.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 20s (- 0m 16s) (550 55%) 0.3662 \n",
      "0m 21s (- 0m 14s) (600 60%) 0.3256 \n",
      "0m 22s (- 0m 12s) (650 65%) 0.2986 \n",
      "0m 23s (- 0m 10s) (700 70%) 0.3300 \n",
      "0m 24s (- 0m 8s) (750 75%) 0.3259 \n",
      "0m 26s (- 0m 6s) (800 80%) 0.3206 \n",
      "0m 26s (- 0m 4s) (850 85%) 0.2795 \n",
      "0m 28s (- 0m 3s) (900 90%) 0.4466 \n",
      "0m 28s (- 0m 1s) (950 95%) 0.3099 \n",
      "0m 29s (- 0m 0s) (1000 100%) 0.3596 16.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 17.20%\n",
      "0m 5s (- 1m 49s) (50 5%) 0.2635 \n",
      "0m 6s (- 1m 1s) (100 10%) 0.4356 \n",
      "0m 8s (- 0m 46s) (150 15%) 0.3473 \n",
      "0m 9s (- 0m 38s) (200 20%) 0.3136 \n",
      "0m 10s (- 0m 31s) (250 25%) 0.3146 \n",
      "0m 11s (- 0m 27s) (300 30%) 0.3910 \n",
      "0m 12s (- 0m 23s) (350 35%) 0.3204 \n",
      "0m 13s (- 0m 20s) (400 40%) 0.3329 \n",
      "0m 14s (- 0m 18s) (450 45%) 0.3204 \n",
      "0m 15s (- 0m 15s) (500 50%) 0.3076 21.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 18s (- 0m 15s) (550 55%) 0.2833 \n",
      "0m 20s (- 0m 13s) (600 60%) 0.3141 \n",
      "0m 21s (- 0m 11s) (650 65%) 0.3148 \n",
      "0m 22s (- 0m 9s) (700 70%) 0.2673 \n",
      "0m 23s (- 0m 7s) (750 75%) 0.3046 \n",
      "0m 24s (- 0m 6s) (800 80%) 0.3172 \n",
      "0m 25s (- 0m 4s) (850 85%) 0.4159 \n",
      "0m 26s (- 0m 2s) (900 90%) 0.2896 \n",
      "0m 28s (- 0m 1s) (950 95%) 0.3489 \n",
      "0m 29s (- 0m 0s) (1000 100%) 0.3206 15.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 17.80%\n",
      "0m 6s (- 1m 59s) (50 5%) 0.3584 \n",
      "0m 7s (- 1m 7s) (100 10%) 0.3784 \n",
      "0m 8s (- 0m 48s) (150 15%) 0.4138 \n",
      "0m 9s (- 0m 38s) (200 20%) 0.3838 \n",
      "0m 10s (- 0m 32s) (250 25%) 0.3354 \n",
      "0m 11s (- 0m 27s) (300 30%) 0.3232 \n",
      "0m 13s (- 0m 24s) (350 35%) 0.3491 \n",
      "0m 14s (- 0m 21s) (400 40%) 0.4598 \n",
      "0m 15s (- 0m 19s) (450 45%) 0.3980 \n",
      "0m 16s (- 0m 16s) (500 50%) 0.3701 18.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 20s (- 0m 16s) (550 55%) 0.3356 \n",
      "0m 21s (- 0m 14s) (600 60%) 0.3537 \n",
      "0m 22s (- 0m 11s) (650 65%) 0.2898 \n",
      "0m 23s (- 0m 10s) (700 70%) 0.3349 \n",
      "0m 24s (- 0m 8s) (750 75%) 0.3355 \n",
      "0m 25s (- 0m 6s) (800 80%) 0.3462 \n",
      "0m 26s (- 0m 4s) (850 85%) 0.3630 \n",
      "0m 27s (- 0m 3s) (900 90%) 0.3862 \n",
      "0m 28s (- 0m 1s) (950 95%) 0.3097 \n",
      "0m 29s (- 0m 0s) (1000 100%) 0.4064 18.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 16.00%\n",
      "0m 6s (- 1m 56s) (50 5%) 0.3110 \n",
      "0m 7s (- 1m 4s) (100 10%) 0.3399 \n",
      "0m 8s (- 0m 46s) (150 15%) 0.3054 \n",
      "0m 9s (- 0m 37s) (200 20%) 0.3144 \n",
      "0m 10s (- 0m 31s) (250 25%) 0.4007 \n",
      "0m 11s (- 0m 26s) (300 30%) 0.3191 \n",
      "0m 11s (- 0m 22s) (350 35%) 0.3071 \n",
      "0m 12s (- 0m 19s) (400 40%) 0.3812 \n",
      "0m 13s (- 0m 16s) (450 45%) 0.3225 \n",
      "0m 14s (- 0m 14s) (500 50%) 0.3219 20.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 16s (- 0m 13s) (550 55%) 0.3160 \n",
      "0m 18s (- 0m 12s) (600 60%) 0.3214 \n",
      "0m 19s (- 0m 10s) (650 65%) 0.4255 \n",
      "0m 20s (- 0m 8s) (700 70%) 0.3353 \n",
      "0m 21s (- 0m 7s) (750 75%) 0.3410 \n",
      "0m 21s (- 0m 5s) (800 80%) 0.3159 \n",
      "0m 22s (- 0m 4s) (850 85%) 0.3694 \n",
      "0m 23s (- 0m 2s) (900 90%) 0.3775 \n",
      "0m 24s (- 0m 1s) (950 95%) 0.2476 \n",
      "0m 25s (- 0m 0s) (1000 100%) 0.3279 16.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 17.40%\n",
      "0m 5s (- 1m 38s) (50 5%) 0.3380 \n",
      "0m 6s (- 0m 54s) (100 10%) 0.3351 \n",
      "0m 6s (- 0m 38s) (150 15%) 0.2962 \n",
      "0m 7s (- 0m 31s) (200 20%) 0.3398 \n",
      "0m 8s (- 0m 26s) (250 25%) 0.3038 \n",
      "0m 9s (- 0m 22s) (300 30%) 0.4005 \n",
      "0m 10s (- 0m 19s) (350 35%) 0.3062 \n",
      "0m 11s (- 0m 16s) (400 40%) 0.3372 \n",
      "0m 12s (- 0m 14s) (450 45%) 0.3962 \n",
      "0m 13s (- 0m 13s) (500 50%) 0.3373 18.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 15s (- 0m 12s) (550 55%) 0.3230 \n",
      "0m 16s (- 0m 10s) (600 60%) 0.3596 \n",
      "0m 17s (- 0m 9s) (650 65%) 0.3462 \n",
      "0m 18s (- 0m 7s) (700 70%) 0.2856 \n",
      "0m 18s (- 0m 6s) (750 75%) 0.3312 \n",
      "0m 19s (- 0m 4s) (800 80%) 0.3491 \n",
      "0m 20s (- 0m 3s) (850 85%) 0.3733 \n",
      "0m 21s (- 0m 2s) (900 90%) 0.3207 \n",
      "0m 22s (- 0m 1s) (950 95%) 0.4276 \n",
      "0m 23s (- 0m 0s) (1000 100%) 0.3381 18.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 19.00%\n",
      "0m 5s (- 1m 45s) (50 5%) 0.3035 \n",
      "0m 6s (- 0m 57s) (100 10%) 0.3249 \n",
      "0m 7s (- 0m 42s) (150 15%) 0.3026 \n",
      "0m 8s (- 0m 33s) (200 20%) 0.3037 \n",
      "0m 9s (- 0m 27s) (250 25%) 0.3133 \n",
      "0m 10s (- 0m 23s) (300 30%) 0.2588 \n",
      "0m 11s (- 0m 20s) (350 35%) 0.2949 \n",
      "0m 12s (- 0m 18s) (400 40%) 0.3268 \n",
      "0m 13s (- 0m 16s) (450 45%) 0.3512 \n",
      "0m 14s (- 0m 14s) (500 50%) 0.3235 16.50%\n",
      "0m 17s (- 0m 13s) (550 55%) 0.4115 \n",
      "0m 18s (- 0m 12s) (600 60%) 0.3200 \n",
      "0m 20s (- 0m 11s) (650 65%) 0.3084 \n",
      "0m 23s (- 0m 9s) (700 70%) 0.3150 \n",
      "0m 26s (- 0m 8s) (750 75%) 0.3503 \n",
      "0m 28s (- 0m 7s) (800 80%) 0.3480 \n",
      "0m 30s (- 0m 5s) (850 85%) 0.3196 \n",
      "0m 31s (- 0m 3s) (900 90%) 0.2566 \n",
      "0m 32s (- 0m 1s) (950 95%) 0.3322 \n",
      "0m 33s (- 0m 0s) (1000 100%) 0.2847 19.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 17.20%\n",
      "0m 5s (- 1m 49s) (50 5%) 0.3437 \n",
      "0m 7s (- 1m 3s) (100 10%) 0.3441 \n",
      "0m 8s (- 0m 46s) (150 15%) 0.2922 \n",
      "0m 9s (- 0m 36s) (200 20%) 0.3489 \n",
      "0m 9s (- 0m 29s) (250 25%) 0.2643 \n",
      "0m 10s (- 0m 25s) (300 30%) 0.3513 \n",
      "0m 11s (- 0m 22s) (350 35%) 0.3026 \n",
      "0m 13s (- 0m 19s) (400 40%) 0.3472 \n",
      "0m 14s (- 0m 17s) (450 45%) 0.3261 \n",
      "0m 15s (- 0m 15s) (500 50%) 0.3724 20.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 19s (- 0m 15s) (550 55%) 0.3424 \n",
      "0m 20s (- 0m 13s) (600 60%) 0.3827 \n",
      "0m 21s (- 0m 11s) (650 65%) 0.3623 \n",
      "0m 22s (- 0m 9s) (700 70%) 0.3152 \n",
      "0m 24s (- 0m 8s) (750 75%) 0.3183 \n",
      "0m 25s (- 0m 6s) (800 80%) 0.3099 \n",
      "0m 26s (- 0m 4s) (850 85%) 0.3274 \n",
      "0m 27s (- 0m 3s) (900 90%) 0.3796 \n",
      "0m 28s (- 0m 1s) (950 95%) 0.3272 \n",
      "0m 30s (- 0m 0s) (1000 100%) 0.2948 23.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 19.40%\n",
      "0m 5s (- 1m 44s) (50 5%) 0.3427 \n",
      "0m 6s (- 0m 56s) (100 10%) 0.3746 \n",
      "0m 7s (- 0m 40s) (150 15%) 0.3153 \n",
      "0m 8s (- 0m 33s) (200 20%) 0.4292 \n",
      "0m 9s (- 0m 27s) (250 25%) 0.2907 \n",
      "0m 9s (- 0m 23s) (300 30%) 0.2807 \n",
      "0m 10s (- 0m 20s) (350 35%) 0.3428 \n",
      "0m 11s (- 0m 17s) (400 40%) 0.3189 \n",
      "0m 12s (- 0m 15s) (450 45%) 0.3285 \n",
      "0m 13s (- 0m 13s) (500 50%) 0.3642 17.50%\n",
      "0m 16s (- 0m 13s) (550 55%) 0.3492 \n",
      "0m 16s (- 0m 11s) (600 60%) 0.3662 \n",
      "0m 17s (- 0m 9s) (650 65%) 0.4229 \n",
      "0m 18s (- 0m 7s) (700 70%) 0.2988 \n",
      "0m 19s (- 0m 6s) (750 75%) 0.3677 \n",
      "0m 20s (- 0m 5s) (800 80%) 0.3462 \n",
      "0m 21s (- 0m 3s) (850 85%) 0.3012 \n",
      "0m 22s (- 0m 2s) (900 90%) 0.3678 \n",
      "0m 22s (- 0m 1s) (950 95%) 0.2642 \n",
      "0m 23s (- 0m 0s) (1000 100%) 0.3176 23.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 20.60%\n",
      "0m 5s (- 3m 22s) (50 2%) 0.3225 \n",
      "0m 6s (- 1m 55s) (100 5%) 0.3088 \n",
      "0m 6s (- 1m 26s) (150 7%) 0.3385 \n",
      "0m 8s (- 1m 12s) (200 10%) 0.3139 \n",
      "0m 9s (- 1m 3s) (250 12%) 0.3115 \n",
      "0m 9s (- 0m 56s) (300 15%) 0.2409 \n",
      "0m 10s (- 0m 51s) (350 17%) 0.2689 \n",
      "0m 11s (- 0m 47s) (400 20%) 0.3391 \n",
      "0m 12s (- 0m 43s) (450 22%) 0.3198 \n",
      "0m 13s (- 0m 40s) (500 25%) 0.2449 25.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 16s (- 0m 42s) (550 27%) 0.3855 \n",
      "0m 16s (- 0m 39s) (600 30%) 0.3554 \n",
      "0m 17s (- 0m 36s) (650 32%) 0.2855 \n",
      "0m 18s (- 0m 34s) (700 35%) 0.3224 \n",
      "0m 19s (- 0m 32s) (750 37%) 0.3880 \n",
      "0m 20s (- 0m 30s) (800 40%) 0.2774 \n",
      "0m 21s (- 0m 28s) (850 42%) 0.3161 \n",
      "0m 22s (- 0m 27s) (900 45%) 0.3271 \n",
      "0m 23s (- 0m 25s) (950 47%) 0.3311 \n",
      "0m 23s (- 0m 23s) (1000 50%) 0.2949 22.00%\n",
      "0m 26s (- 0m 23s) (1050 52%) 0.3431 \n",
      "0m 27s (- 0m 22s) (1100 55%) 0.3931 \n",
      "0m 28s (- 0m 20s) (1150 57%) 0.3510 \n",
      "0m 29s (- 0m 19s) (1200 60%) 0.2847 \n",
      "0m 30s (- 0m 18s) (1250 62%) 0.2829 \n",
      "0m 31s (- 0m 16s) (1300 65%) 0.3015 \n",
      "0m 32s (- 0m 15s) (1350 67%) 0.4177 \n",
      "0m 33s (- 0m 14s) (1400 70%) 0.3113 \n",
      "0m 33s (- 0m 12s) (1450 72%) 0.3856 \n",
      "0m 34s (- 0m 11s) (1500 75%) 0.3959 19.00%\n",
      "0m 37s (- 0m 10s) (1550 77%) 0.3768 \n",
      "0m 39s (- 0m 9s) (1600 80%) 0.3672 \n",
      "0m 40s (- 0m 8s) (1650 82%) 0.3095 \n",
      "0m 41s (- 0m 7s) (1700 85%) 0.3664 \n",
      "0m 43s (- 0m 6s) (1750 87%) 0.2869 \n",
      "0m 44s (- 0m 4s) (1800 90%) 0.4687 \n",
      "0m 45s (- 0m 3s) (1850 92%) 0.3465 \n",
      "0m 46s (- 0m 2s) (1900 95%) 0.3100 \n",
      "0m 47s (- 0m 1s) (1950 97%) 0.3513 \n",
      "0m 49s (- 0m 0s) (2000 100%) 0.4138 20.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 25.20%\n",
      "0m 4s (- 3m 14s) (50 2%) 0.3846 \n",
      "0m 6s (- 1m 54s) (100 5%) 0.3449 \n",
      "0m 7s (- 1m 26s) (150 7%) 0.3311 \n",
      "0m 7s (- 1m 11s) (200 10%) 0.3377 \n",
      "0m 8s (- 1m 0s) (250 12%) 0.2966 \n",
      "0m 9s (- 0m 53s) (300 15%) 0.2885 \n",
      "0m 10s (- 0m 49s) (350 17%) 0.3440 \n",
      "0m 11s (- 0m 46s) (400 20%) 0.3012 \n",
      "0m 12s (- 0m 42s) (450 22%) 0.3777 \n",
      "0m 13s (- 0m 40s) (500 25%) 0.2823 25.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 16s (- 0m 42s) (550 27%) 0.2853 \n",
      "0m 17s (- 0m 40s) (600 30%) 0.2901 \n",
      "0m 18s (- 0m 37s) (650 32%) 0.2694 \n",
      "0m 19s (- 0m 35s) (700 35%) 0.2646 \n",
      "0m 20s (- 0m 33s) (750 37%) 0.3171 \n",
      "0m 20s (- 0m 31s) (800 40%) 0.3614 \n",
      "0m 21s (- 0m 29s) (850 42%) 0.3456 \n",
      "0m 22s (- 0m 27s) (900 45%) 0.3109 \n",
      "0m 23s (- 0m 25s) (950 47%) 0.3272 \n",
      "0m 24s (- 0m 24s) (1000 50%) 0.2835 19.50%\n",
      "0m 26s (- 0m 23s) (1050 52%) 0.3743 \n",
      "0m 27s (- 0m 22s) (1100 55%) 0.3170 \n",
      "0m 28s (- 0m 20s) (1150 57%) 0.3413 \n",
      "0m 29s (- 0m 19s) (1200 60%) 0.2654 \n",
      "0m 30s (- 0m 18s) (1250 62%) 0.2930 \n",
      "0m 30s (- 0m 16s) (1300 65%) 0.3441 \n",
      "0m 31s (- 0m 15s) (1350 67%) 0.3474 \n",
      "0m 32s (- 0m 13s) (1400 70%) 0.2871 \n",
      "0m 33s (- 0m 12s) (1450 72%) 0.2983 \n",
      "0m 34s (- 0m 11s) (1500 75%) 0.3451 17.00%\n",
      "0m 36s (- 0m 10s) (1550 77%) 0.2953 \n",
      "0m 37s (- 0m 9s) (1600 80%) 0.2837 \n",
      "0m 38s (- 0m 8s) (1650 82%) 0.3943 \n",
      "0m 39s (- 0m 6s) (1700 85%) 0.3208 \n",
      "0m 40s (- 0m 5s) (1750 87%) 0.3473 \n",
      "0m 40s (- 0m 4s) (1800 90%) 0.2911 \n",
      "0m 41s (- 0m 3s) (1850 92%) 0.3536 \n",
      "0m 42s (- 0m 2s) (1900 95%) 0.3443 \n",
      "0m 43s (- 0m 1s) (1950 97%) 0.2961 \n",
      "0m 44s (- 0m 0s) (2000 100%) 0.2788 18.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 19.40%\n",
      "0m 6s (- 2m 11s) (50 5%) 0.3655 \n",
      "0m 7s (- 1m 10s) (100 10%) 0.3454 \n",
      "0m 8s (- 0m 49s) (150 15%) 0.2668 \n",
      "0m 9s (- 0m 37s) (200 20%) 0.2534 \n",
      "0m 10s (- 0m 31s) (250 25%) 0.2702 \n",
      "0m 11s (- 0m 26s) (300 30%) 0.3126 \n",
      "0m 12s (- 0m 23s) (350 35%) 0.2670 \n",
      "0m 13s (- 0m 20s) (400 40%) 0.3051 \n",
      "0m 14s (- 0m 17s) (450 45%) 0.2991 \n",
      "0m 15s (- 0m 15s) (500 50%) 0.3651 21.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 18s (- 0m 14s) (550 55%) 0.3474 \n",
      "0m 19s (- 0m 12s) (600 60%) 0.3454 \n",
      "0m 20s (- 0m 11s) (650 65%) 0.2965 \n",
      "0m 21s (- 0m 9s) (700 70%) 0.3419 \n",
      "0m 22s (- 0m 7s) (750 75%) 0.3141 \n",
      "0m 23s (- 0m 5s) (800 80%) 0.3758 \n",
      "0m 24s (- 0m 4s) (850 85%) 0.3529 \n",
      "0m 25s (- 0m 2s) (900 90%) 0.3056 \n",
      "0m 26s (- 0m 1s) (950 95%) 0.3208 \n",
      "0m 26s (- 0m 0s) (1000 100%) 0.3307 16.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 20.80%\n",
      "0m 4s (- 1m 32s) (50 5%) 0.3146 \n",
      "0m 5s (- 0m 50s) (100 10%) 0.2733 \n",
      "0m 6s (- 0m 36s) (150 15%) 0.3876 \n",
      "0m 7s (- 0m 29s) (200 20%) 0.3646 \n",
      "0m 8s (- 0m 24s) (250 25%) 0.3722 \n",
      "0m 9s (- 0m 21s) (300 30%) 0.3511 \n",
      "0m 10s (- 0m 18s) (350 35%) 0.2987 \n",
      "0m 11s (- 0m 16s) (400 40%) 0.3165 \n",
      "0m 12s (- 0m 14s) (450 45%) 0.2660 \n",
      "0m 13s (- 0m 13s) (500 50%) 0.3819 25.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 15s (- 0m 12s) (550 55%) 0.3084 \n",
      "0m 16s (- 0m 10s) (600 60%) 0.2547 \n",
      "0m 17s (- 0m 9s) (650 65%) 0.3697 \n",
      "0m 17s (- 0m 7s) (700 70%) 0.3418 \n",
      "0m 18s (- 0m 6s) (750 75%) 0.2785 \n",
      "0m 19s (- 0m 4s) (800 80%) 0.3691 \n",
      "0m 20s (- 0m 3s) (850 85%) 0.3179 \n",
      "0m 21s (- 0m 2s) (900 90%) 0.3320 \n",
      "0m 22s (- 0m 1s) (950 95%) 0.2796 \n",
      "0m 23s (- 0m 0s) (1000 100%) 0.3399 16.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 21.20%\n",
      "0m 4s (- 1m 22s) (50 5%) 0.3552 \n",
      "0m 5s (- 0m 45s) (100 10%) 0.3284 \n",
      "0m 5s (- 0m 33s) (150 15%) 0.3918 \n",
      "0m 6s (- 0m 26s) (200 20%) 0.3271 \n",
      "0m 7s (- 0m 22s) (250 25%) 0.3019 \n",
      "0m 8s (- 0m 19s) (300 30%) 0.3032 \n",
      "0m 9s (- 0m 17s) (350 35%) 0.3509 \n",
      "0m 10s (- 0m 15s) (400 40%) 0.3662 \n",
      "0m 11s (- 0m 13s) (450 45%) 0.2358 \n",
      "0m 12s (- 0m 12s) (500 50%) 0.2846 24.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 14s (- 0m 11s) (550 55%) 0.2712 \n",
      "0m 15s (- 0m 10s) (600 60%) 0.3173 \n",
      "0m 16s (- 0m 8s) (650 65%) 0.2524 \n",
      "0m 17s (- 0m 7s) (700 70%) 0.3117 \n",
      "0m 17s (- 0m 5s) (750 75%) 0.3238 \n",
      "0m 18s (- 0m 4s) (800 80%) 0.3463 \n",
      "0m 19s (- 0m 3s) (850 85%) 0.3174 \n",
      "0m 20s (- 0m 2s) (900 90%) 0.2893 \n",
      "0m 21s (- 0m 1s) (950 95%) 0.2807 \n",
      "0m 22s (- 0m 0s) (1000 100%) 0.4004 21.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20799999999999999"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "    encoder2 = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "    decoder2 = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "evaluateAccuracy(encoder2, decoder2, n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> run right thrice and jump opposite left\n",
      "= I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_LEFT I_TURN_LEFT I_JUMP\n",
      "< I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_LEFT I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> jump around right and look opposite right\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_LOOK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_LOOK <EOS>\n",
      "\n",
      "> turn around left twice after jump opposite left twice\n",
      "= I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT\n",
      "< I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT <EOS>\n",
      "\n",
      "> walk opposite right twice after jump opposite right twice\n",
      "= I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_WALK\n",
      "< I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_WALK <EOS>\n",
      "\n",
      "> jump around right thrice and jump opposite right\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> walk opposite right thrice and jump left\n",
      "= I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_LEFT I_JUMP\n",
      "< I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_JUMP I_JUMP <EOS>\n",
      "\n",
      "> jump right and run left thrice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN <EOS>\n",
      "\n",
      "> jump right thrice after walk around right twice\n",
      "= I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> jump thrice and turn around right twice\n",
      "= I_JUMP I_JUMP I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT\n",
      "< I_JUMP I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT <EOS>\n",
      "\n",
      "> look thrice and jump around left twice\n",
      "= I_LOOK I_LOOK I_LOOK I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP\n",
      "< I_LOOK I_LOOK I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

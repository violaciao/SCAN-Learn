{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCAN Add-Prim JUMP Experiment\n",
    "*************************************************************\n",
    "\n",
    "Reference: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Python 3.6\n",
    "* PyTorch 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Device is using\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "TASK_NAME = \"addprim-jump\"\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False, trainOrtest='train'):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines        \n",
    "    lines = open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/processed/{}-{}_{}-{}.txt'.\\\n",
    "                 format(trainOrtest, TASK_NAME, lang1, lang2), encoding='utf-8').\\\n",
    "                 read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "# PRED_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 37046 sentence pairs\n",
      "Trimmed to 37046 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['turn around right and turn opposite left', 'I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_LEFT I_TURN_LEFT']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataFrom='train'):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=False, trainOrtest=dataFrom)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('in', 'out', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "=================\n",
    "\n",
    "The model we are using is a GRU encoder-decoder seq2seq model with attention mechanism. In order to solve the zero-shot generalization task, we embed the encoder networks with pre-trained embeddings, from GloVe and Google Word2Vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_SOURCE = 'glove'\n",
    "hidden_size = 100\n",
    "\n",
    "if EMBEDDEING_SOURCE == 'google':\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_GoogleNews300Negative.pkl', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "else:\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_raw{}d.pkl'.format(hidden_size), 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "\n",
    "pretrained_emb = np.zeros((input_lang.n_words, hidden_size))\n",
    "for k, v in input_lang.index2word.items():\n",
    "    if v == 'SOS':\n",
    "        pretrained_emb[k] = np.zeros(hidden_size)\n",
    "    elif (v == 'EOS') and (EMBEDDEING_SOURCE != 'google'):\n",
    "        pretrained_emb[k] = b['.']\n",
    "    elif (v == 'and') and (EMBEDDEING_SOURCE == 'google'):\n",
    "        pretrained_emb[k] = b['AND']\n",
    "    else:\n",
    "        pretrained_emb[k] = b[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of this seq2seq network is a GRU netword. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_PRETRAINED = True\n",
    "WEIGHT_UPDATE = False\n",
    "\n",
    "MODEL_VERSION = 'T0.4_glv100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        if EMBEDDEING_PRETRAINED:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "            self.embedding.weight.requires_grad = WEIGHT_UPDATE\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is a GRU network with attention mechanism that takes the last output of the encoder and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "First we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "We use teacher forcing to help converge faster with a delay fashion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, \n",
    "          max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for timing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, eval_every=1000, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "        encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "        decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "        \n",
    "    best_test_acc = evaluateAccuracy(encoder, decoder, 500)\n",
    "    print(\"Best evaluation accuracy: {0:.2f}%\".format(best_test_acc * 100))\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        \n",
    "    encoder_optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg), end=' ')\n",
    "            \n",
    "            if iter % eval_every == 0:\n",
    "                test_acc = evaluateAccuracy(encoder, decoder, 200)\n",
    "                print('{0:.2f}%'.format(test_acc * 100))\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    with open(\"saved_models/encoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(encoder, f)\n",
    "                    with open(\"saved_models/decoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(decoder, f)\n",
    "                    print(\"New best test accuracy! Model Updated!\")\n",
    "                    best_test_acc = test_acc\n",
    "#                 elif test_acc < best_test_acc - 0.001:\n",
    "#                     encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "#                     decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "                    \n",
    "            else:\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 15412 sentence pairs\n",
      "Trimmed to 15412 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['look left twice and jump around left', 'I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs_eval = prepareData('in', 'out', True, dataFrom='test')\n",
    "print(random.choice(pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateAccuracy(encoder, decoder, n=10):\n",
    "    ACCs = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        \n",
    "        if output_words[-1] == '<EOS>':\n",
    "            output_words = output_words[:-1]\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        if output_sentence == pair[1]:\n",
    "            ACCs.append(1)\n",
    "        else:\n",
    "            ACCs.append(0)\n",
    "    return np.array(ACCs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initially trained with a higher teacher aid, and relatively large learning rate. Both teacher forcing effect and the learning rate decay over iterations when the model approaches the optimum.  \n",
    "\n",
    "#### The model achieves 97% accuracy rate for the best test sample evaluation, and is 94% correct on average for the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 0.00%\n",
      "0m 4s (- 7m 16s) (50 1%) 1.8580 \n",
      "0m 6s (- 4m 55s) (100 2%) 1.6051 \n",
      "0m 7s (- 4m 5s) (150 3%) 1.4842 \n",
      "0m 9s (- 3m 39s) (200 4%) 1.3071 \n",
      "0m 10s (- 3m 26s) (250 5%) 1.3116 \n",
      "0m 12s (- 3m 18s) (300 6%) 1.2965 \n",
      "0m 14s (- 3m 13s) (350 7%) 1.0414 \n",
      "0m 16s (- 3m 4s) (400 8%) 1.1737 \n",
      "0m 18s (- 3m 5s) (450 9%) 1.1512 \n",
      "0m 20s (- 3m 0s) (500 10%) 1.2825 0.00%\n",
      "0m 28s (- 3m 49s) (550 11%) 1.2795 \n",
      "0m 30s (- 3m 42s) (600 12%) 1.0927 \n",
      "0m 31s (- 3m 34s) (650 13%) 1.1269 \n",
      "0m 33s (- 3m 24s) (700 14%) 0.9627 \n",
      "0m 35s (- 3m 18s) (750 15%) 1.0182 \n",
      "0m 36s (- 3m 12s) (800 16%) 0.9904 \n",
      "0m 38s (- 3m 7s) (850 17%) 0.9553 \n",
      "0m 40s (- 3m 2s) (900 18%) 0.9635 \n",
      "0m 41s (- 2m 59s) (950 19%) 0.9128 \n",
      "0m 43s (- 2m 53s) (1000 20%) 0.9097 0.00%\n",
      "0m 50s (- 3m 11s) (1050 21%) 0.8488 \n",
      "0m 52s (- 3m 6s) (1100 22%) 0.8443 \n",
      "0m 54s (- 3m 1s) (1150 23%) 0.8983 \n",
      "0m 56s (- 2m 58s) (1200 24%) 0.8404 \n",
      "0m 58s (- 2m 55s) (1250 25%) 0.7303 \n",
      "1m 0s (- 2m 52s) (1300 26%) 0.8454 \n",
      "1m 2s (- 2m 48s) (1350 27%) 0.8392 \n",
      "1m 4s (- 2m 46s) (1400 28%) 0.8741 \n",
      "1m 7s (- 2m 45s) (1450 28%) 0.8917 \n",
      "1m 10s (- 2m 44s) (1500 30%) 0.7737 0.00%\n",
      "1m 20s (- 2m 58s) (1550 31%) 0.7922 \n",
      "1m 22s (- 2m 54s) (1600 32%) 0.7702 \n",
      "1m 23s (- 2m 50s) (1650 33%) 0.7686 \n",
      "1m 25s (- 2m 45s) (1700 34%) 0.7861 \n",
      "1m 27s (- 2m 41s) (1750 35%) 0.6800 \n",
      "1m 28s (- 2m 37s) (1800 36%) 0.7922 \n",
      "1m 30s (- 2m 33s) (1850 37%) 0.7411 \n",
      "1m 31s (- 2m 29s) (1900 38%) 0.6977 \n",
      "1m 33s (- 2m 26s) (1950 39%) 0.6939 \n",
      "1m 35s (- 2m 22s) (2000 40%) 0.5901 0.00%\n",
      "1m 41s (- 2m 25s) (2050 41%) 0.7143 \n",
      "1m 42s (- 2m 21s) (2100 42%) 0.6542 \n",
      "1m 44s (- 2m 18s) (2150 43%) 0.5919 \n",
      "1m 45s (- 2m 14s) (2200 44%) 0.6969 \n",
      "1m 47s (- 2m 11s) (2250 45%) 0.5829 \n",
      "1m 48s (- 2m 7s) (2300 46%) 0.5787 \n",
      "1m 50s (- 2m 4s) (2350 47%) 0.6760 \n",
      "1m 51s (- 2m 1s) (2400 48%) 0.6590 \n",
      "1m 53s (- 1m 58s) (2450 49%) 0.5195 \n",
      "1m 55s (- 1m 55s) (2500 50%) 0.6089 0.50%\n",
      "New best test accuracy! Model Updated!\n",
      "2m 1s (- 1m 56s) (2550 51%) 0.6595 \n",
      "2m 2s (- 1m 53s) (2600 52%) 0.6337 \n",
      "2m 4s (- 1m 50s) (2650 53%) 0.6338 \n",
      "2m 5s (- 1m 46s) (2700 54%) 0.6306 \n",
      "2m 7s (- 1m 43s) (2750 55%) 0.5966 \n",
      "2m 8s (- 1m 40s) (2800 56%) 0.5169 \n",
      "2m 9s (- 1m 37s) (2850 56%) 0.6088 \n",
      "2m 11s (- 1m 35s) (2900 57%) 0.5151 \n",
      "2m 12s (- 1m 32s) (2950 59%) 0.5911 \n",
      "2m 14s (- 1m 29s) (3000 60%) 0.6009 0.00%\n",
      "2m 19s (- 1m 29s) (3050 61%) 0.5980 \n",
      "2m 21s (- 1m 26s) (3100 62%) 0.5062 \n",
      "2m 22s (- 1m 23s) (3150 63%) 0.6884 \n",
      "2m 24s (- 1m 21s) (3200 64%) 0.5669 \n",
      "2m 25s (- 1m 18s) (3250 65%) 0.5318 \n",
      "2m 27s (- 1m 15s) (3300 66%) 0.5013 \n",
      "2m 28s (- 1m 13s) (3350 67%) 0.5129 \n",
      "2m 29s (- 1m 10s) (3400 68%) 0.4980 \n",
      "2m 31s (- 1m 7s) (3450 69%) 0.5545 \n",
      "2m 32s (- 1m 5s) (3500 70%) 0.5246 0.00%\n",
      "2m 37s (- 1m 4s) (3550 71%) 0.5623 \n",
      "2m 39s (- 1m 1s) (3600 72%) 0.4705 \n",
      "2m 40s (- 0m 59s) (3650 73%) 0.5124 \n",
      "2m 42s (- 0m 56s) (3700 74%) 0.4778 \n",
      "2m 43s (- 0m 54s) (3750 75%) 0.4866 \n",
      "2m 45s (- 0m 52s) (3800 76%) 0.4399 \n",
      "2m 46s (- 0m 49s) (3850 77%) 0.5335 \n",
      "2m 48s (- 0m 47s) (3900 78%) 0.4312 \n",
      "2m 50s (- 0m 45s) (3950 79%) 0.4762 \n",
      "2m 51s (- 0m 42s) (4000 80%) 0.4642 1.50%\n",
      "New best test accuracy! Model Updated!\n",
      "2m 56s (- 0m 41s) (4050 81%) 0.5038 \n",
      "2m 57s (- 0m 39s) (4100 82%) 0.5635 \n",
      "2m 59s (- 0m 36s) (4150 83%) 0.5317 \n",
      "3m 0s (- 0m 34s) (4200 84%) 0.4739 \n",
      "3m 2s (- 0m 32s) (4250 85%) 0.4383 \n",
      "3m 4s (- 0m 29s) (4300 86%) 0.4806 \n",
      "3m 5s (- 0m 27s) (4350 87%) 0.4632 \n",
      "3m 6s (- 0m 25s) (4400 88%) 0.4212 \n",
      "3m 8s (- 0m 23s) (4450 89%) 0.4247 \n",
      "3m 9s (- 0m 21s) (4500 90%) 0.4507 6.00%\n",
      "New best test accuracy! Model Updated!\n",
      "3m 14s (- 0m 19s) (4550 91%) 0.3397 \n",
      "3m 15s (- 0m 16s) (4600 92%) 0.4236 \n",
      "3m 16s (- 0m 14s) (4650 93%) 0.4846 \n",
      "3m 18s (- 0m 12s) (4700 94%) 0.4561 \n",
      "3m 19s (- 0m 10s) (4750 95%) 0.4430 \n",
      "3m 21s (- 0m 8s) (4800 96%) 0.4591 \n",
      "3m 22s (- 0m 6s) (4850 97%) 0.3725 \n",
      "3m 24s (- 0m 4s) (4900 98%) 0.4009 \n",
      "3m 25s (- 0m 2s) (4950 99%) 0.4529 \n",
      "3m 26s (- 0m 0s) (5000 100%) 0.4625 1.50%\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 5.20%\n",
      "0m 7s (- 2m 24s) (50 5%) 0.5383 \n",
      "0m 9s (- 1m 23s) (100 10%) 0.6090 \n",
      "0m 10s (- 1m 2s) (150 15%) 0.6029 \n",
      "0m 12s (- 0m 49s) (200 20%) 0.5605 \n",
      "0m 13s (- 0m 41s) (250 25%) 0.6172 \n",
      "0m 15s (- 0m 35s) (300 30%) 0.5836 \n",
      "0m 17s (- 0m 32s) (350 35%) 0.5007 \n",
      "0m 19s (- 0m 28s) (400 40%) 0.5395 \n",
      "0m 20s (- 0m 25s) (450 45%) 0.4296 \n",
      "0m 22s (- 0m 22s) (500 50%) 0.4641 7.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 26s (- 0m 21s) (550 55%) 0.5286 \n",
      "0m 28s (- 0m 18s) (600 60%) 0.4023 \n",
      "0m 29s (- 0m 15s) (650 65%) 0.4764 \n",
      "0m 30s (- 0m 13s) (700 70%) 0.5611 \n",
      "0m 31s (- 0m 10s) (750 75%) 0.6120 \n",
      "0m 33s (- 0m 8s) (800 80%) 0.5463 \n",
      "0m 34s (- 0m 6s) (850 85%) 0.4433 \n",
      "0m 35s (- 0m 3s) (900 90%) 0.4698 \n",
      "0m 36s (- 0m 1s) (950 95%) 0.5156 \n",
      "0m 38s (- 0m 0s) (1000 100%) 0.3970 9.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 7.60%\n",
      "0m 6s (- 2m 9s) (50 5%) 0.4757 \n",
      "0m 8s (- 1m 13s) (100 10%) 0.4615 \n",
      "0m 9s (- 0m 55s) (150 15%) 0.5772 \n",
      "0m 11s (- 0m 44s) (200 20%) 0.5126 \n",
      "0m 12s (- 0m 36s) (250 25%) 0.3797 \n",
      "0m 13s (- 0m 31s) (300 30%) 0.5019 \n",
      "0m 14s (- 0m 27s) (350 35%) 0.4571 \n",
      "0m 16s (- 0m 24s) (400 40%) 0.4054 \n",
      "0m 17s (- 0m 20s) (450 45%) 0.4459 \n",
      "0m 18s (- 0m 18s) (500 50%) 0.3970 8.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 21s (- 0m 17s) (550 55%) 0.4339 \n",
      "0m 23s (- 0m 15s) (600 60%) 0.4655 \n",
      "0m 24s (- 0m 13s) (650 65%) 0.3987 \n",
      "0m 25s (- 0m 10s) (700 70%) 0.4732 \n",
      "0m 26s (- 0m 8s) (750 75%) 0.5640 \n",
      "0m 27s (- 0m 6s) (800 80%) 0.4725 \n",
      "0m 29s (- 0m 5s) (850 85%) 0.4714 \n",
      "0m 30s (- 0m 3s) (900 90%) 0.4647 \n",
      "0m 31s (- 0m 1s) (950 95%) 0.4640 \n",
      "0m 33s (- 0m 0s) (1000 100%) 0.4241 7.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 10.40%\n",
      "0m 7s (- 2m 20s) (50 5%) 0.4427 \n",
      "0m 8s (- 1m 17s) (100 10%) 0.5037 \n",
      "0m 9s (- 0m 55s) (150 15%) 0.4901 \n",
      "0m 10s (- 0m 43s) (200 20%) 0.4545 \n",
      "0m 12s (- 0m 37s) (250 25%) 0.4620 \n",
      "0m 13s (- 0m 32s) (300 30%) 0.4927 \n",
      "0m 15s (- 0m 28s) (350 35%) 0.3937 \n",
      "0m 16s (- 0m 24s) (400 40%) 0.4439 \n",
      "0m 17s (- 0m 21s) (450 45%) 0.5226 \n",
      "0m 18s (- 0m 18s) (500 50%) 0.3739 5.50%\n",
      "0m 22s (- 0m 18s) (550 55%) 0.4772 \n",
      "0m 24s (- 0m 16s) (600 60%) 0.5234 \n",
      "0m 25s (- 0m 13s) (650 65%) 0.4979 \n",
      "0m 26s (- 0m 11s) (700 70%) 0.3922 \n",
      "0m 28s (- 0m 9s) (750 75%) 0.4187 \n",
      "0m 29s (- 0m 7s) (800 80%) 0.5085 \n",
      "0m 30s (- 0m 5s) (850 85%) 0.4024 \n",
      "0m 31s (- 0m 3s) (900 90%) 0.4515 \n",
      "0m 33s (- 0m 1s) (950 95%) 0.4398 \n",
      "0m 34s (- 0m 0s) (1000 100%) 0.3864 12.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 14.20%\n",
      "0m 6s (- 2m 4s) (50 5%) 0.3915 \n",
      "0m 7s (- 1m 10s) (100 10%) 0.3490 \n",
      "0m 9s (- 0m 51s) (150 15%) 0.4112 \n",
      "0m 10s (- 0m 41s) (200 20%) 0.3682 \n",
      "0m 11s (- 0m 35s) (250 25%) 0.3475 \n",
      "0m 12s (- 0m 30s) (300 30%) 0.3576 \n",
      "0m 14s (- 0m 26s) (350 35%) 0.3858 \n",
      "0m 15s (- 0m 23s) (400 40%) 0.3455 \n",
      "0m 17s (- 0m 21s) (450 45%) 0.3492 \n",
      "0m 18s (- 0m 18s) (500 50%) 0.3704 13.50%\n",
      "0m 21s (- 0m 17s) (550 55%) 0.2993 \n",
      "0m 22s (- 0m 15s) (600 60%) 0.3477 \n",
      "0m 24s (- 0m 13s) (650 65%) 0.4200 \n",
      "0m 25s (- 0m 11s) (700 70%) 0.4100 \n",
      "0m 26s (- 0m 8s) (750 75%) 0.3797 \n",
      "0m 28s (- 0m 7s) (800 80%) 0.4026 \n",
      "0m 29s (- 0m 5s) (850 85%) 0.4233 \n",
      "0m 30s (- 0m 3s) (900 90%) 0.3627 \n",
      "0m 31s (- 0m 1s) (950 95%) 0.3189 \n",
      "0m 33s (- 0m 0s) (1000 100%) 0.3310 18.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 18.00%\n",
      "0m 6s (- 2m 5s) (50 5%) 0.3199 \n",
      "0m 8s (- 1m 13s) (100 10%) 0.3287 \n",
      "0m 9s (- 0m 54s) (150 15%) 0.3888 \n",
      "0m 10s (- 0m 43s) (200 20%) 0.4593 \n",
      "0m 11s (- 0m 35s) (250 25%) 0.4251 \n",
      "0m 13s (- 0m 30s) (300 30%) 0.3593 \n",
      "0m 14s (- 0m 27s) (350 35%) 0.2973 \n",
      "0m 16s (- 0m 24s) (400 40%) 0.3846 \n",
      "0m 18s (- 0m 22s) (450 45%) 0.3521 \n",
      "0m 19s (- 0m 19s) (500 50%) 0.2842 20.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 23s (- 0m 18s) (550 55%) 0.4198 \n",
      "0m 24s (- 0m 16s) (600 60%) 0.3272 \n",
      "0m 25s (- 0m 13s) (650 65%) 0.3464 \n",
      "0m 27s (- 0m 11s) (700 70%) 0.3055 \n",
      "0m 28s (- 0m 9s) (750 75%) 0.3557 \n",
      "0m 29s (- 0m 7s) (800 80%) 0.3601 \n",
      "0m 30s (- 0m 5s) (850 85%) 0.3569 \n",
      "0m 32s (- 0m 3s) (900 90%) 0.3738 \n",
      "0m 33s (- 0m 1s) (950 95%) 0.3838 \n",
      "0m 34s (- 0m 0s) (1000 100%) 0.3139 21.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 17.60%\n",
      "0m 6s (- 1m 57s) (50 5%) 0.3104 \n",
      "0m 7s (- 1m 8s) (100 10%) 0.2667 \n",
      "0m 8s (- 0m 50s) (150 15%) 0.2784 \n",
      "0m 10s (- 0m 40s) (200 20%) 0.3867 \n",
      "0m 11s (- 0m 34s) (250 25%) 0.2541 \n",
      "0m 12s (- 0m 29s) (300 30%) 0.2868 \n",
      "0m 14s (- 0m 26s) (350 35%) 0.3123 \n",
      "0m 15s (- 0m 22s) (400 40%) 0.3285 \n",
      "0m 16s (- 0m 20s) (450 45%) 0.2926 \n",
      "0m 17s (- 0m 17s) (500 50%) 0.3668 15.50%\n",
      "0m 21s (- 0m 17s) (550 55%) 0.3510 \n",
      "0m 22s (- 0m 14s) (600 60%) 0.2983 \n",
      "0m 23s (- 0m 12s) (650 65%) 0.3660 \n",
      "0m 24s (- 0m 10s) (700 70%) 0.3188 \n",
      "0m 26s (- 0m 8s) (750 75%) 0.3105 \n",
      "0m 27s (- 0m 6s) (800 80%) 0.2514 \n",
      "0m 29s (- 0m 5s) (850 85%) 0.3052 \n",
      "0m 30s (- 0m 3s) (900 90%) 0.3243 \n",
      "0m 32s (- 0m 1s) (950 95%) 0.2222 \n",
      "0m 34s (- 0m 0s) (1000 100%) 0.2810 21.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 23.60%\n",
      "0m 6s (- 2m 3s) (50 5%) 0.3969 \n",
      "0m 8s (- 1m 12s) (100 10%) 0.3800 \n",
      "0m 9s (- 0m 55s) (150 15%) 0.2687 \n",
      "0m 11s (- 0m 44s) (200 20%) 0.2581 \n",
      "0m 13s (- 0m 39s) (250 25%) 0.2519 \n",
      "0m 15s (- 0m 35s) (300 30%) 0.2826 \n",
      "0m 17s (- 0m 31s) (350 35%) 0.3072 \n",
      "0m 18s (- 0m 28s) (400 40%) 0.2636 \n",
      "0m 20s (- 0m 24s) (450 45%) 0.4020 \n",
      "0m 21s (- 0m 21s) (500 50%) 0.2641 27.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 25s (- 0m 20s) (550 55%) 0.2587 \n",
      "0m 26s (- 0m 17s) (600 60%) 0.3166 \n",
      "0m 27s (- 0m 14s) (650 65%) 0.2867 \n",
      "0m 28s (- 0m 12s) (700 70%) 0.2363 \n",
      "0m 29s (- 0m 9s) (750 75%) 0.2565 \n",
      "0m 31s (- 0m 7s) (800 80%) 0.2618 \n",
      "0m 32s (- 0m 5s) (850 85%) 0.3527 \n",
      "0m 33s (- 0m 3s) (900 90%) 0.2961 \n",
      "0m 34s (- 0m 1s) (950 95%) 0.4262 \n",
      "0m 36s (- 0m 0s) (1000 100%) 0.3297 30.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 23.40%\n",
      "0m 6s (- 2m 12s) (50 5%) 0.2943 \n",
      "0m 8s (- 1m 15s) (100 10%) 0.2845 \n",
      "0m 9s (- 0m 54s) (150 15%) 0.2690 \n",
      "0m 10s (- 0m 43s) (200 20%) 0.3612 \n",
      "0m 12s (- 0m 36s) (250 25%) 0.3019 \n",
      "0m 13s (- 0m 31s) (300 30%) 0.2435 \n",
      "0m 14s (- 0m 27s) (350 35%) 0.2649 \n",
      "0m 16s (- 0m 24s) (400 40%) 0.3982 \n",
      "0m 17s (- 0m 21s) (450 45%) 0.3596 \n",
      "0m 18s (- 0m 18s) (500 50%) 0.2912 27.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 22s (- 0m 18s) (550 55%) 0.3309 \n",
      "0m 23s (- 0m 15s) (600 60%) 0.2302 \n",
      "0m 24s (- 0m 13s) (650 65%) 0.2767 \n",
      "0m 26s (- 0m 11s) (700 70%) 0.2093 \n",
      "0m 27s (- 0m 9s) (750 75%) 0.2868 \n",
      "0m 28s (- 0m 7s) (800 80%) 0.3753 \n",
      "0m 29s (- 0m 5s) (850 85%) 0.2151 \n",
      "0m 30s (- 0m 3s) (900 90%) 0.2079 \n",
      "0m 32s (- 0m 1s) (950 95%) 0.2660 \n",
      "0m 34s (- 0m 0s) (1000 100%) 0.3611 34.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 32.20%\n",
      "0m 8s (- 2m 40s) (50 5%) 0.2810 \n",
      "0m 10s (- 1m 30s) (100 10%) 0.2241 \n",
      "0m 11s (- 1m 6s) (150 15%) 0.2429 \n",
      "0m 13s (- 0m 52s) (200 20%) 0.2949 \n",
      "0m 14s (- 0m 43s) (250 25%) 0.2248 \n",
      "0m 15s (- 0m 37s) (300 30%) 0.3535 \n",
      "0m 17s (- 0m 32s) (350 35%) 0.2236 \n",
      "0m 19s (- 0m 29s) (400 40%) 0.2602 \n",
      "0m 21s (- 0m 26s) (450 45%) 0.3283 \n",
      "0m 22s (- 0m 22s) (500 50%) 0.2657 30.50%\n",
      "0m 27s (- 0m 22s) (550 55%) 0.2086 \n",
      "0m 28s (- 0m 19s) (600 60%) 0.3000 \n",
      "0m 30s (- 0m 16s) (650 65%) 0.3118 \n",
      "0m 32s (- 0m 14s) (700 70%) 0.3167 \n",
      "0m 34s (- 0m 11s) (750 75%) 0.2624 \n",
      "0m 36s (- 0m 9s) (800 80%) 0.2352 \n",
      "0m 37s (- 0m 6s) (850 85%) 0.2575 \n",
      "0m 39s (- 0m 4s) (900 90%) 0.2056 \n",
      "0m 41s (- 0m 2s) (950 95%) 0.2061 \n",
      "0m 42s (- 0m 0s) (1000 100%) 0.2117 35.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 36.60%\n",
      "0m 5s (- 1m 52s) (50 5%) 0.2936 \n",
      "0m 7s (- 1m 4s) (100 10%) 0.2525 \n",
      "0m 8s (- 0m 48s) (150 15%) 0.2295 \n",
      "0m 9s (- 0m 39s) (200 20%) 0.2599 \n",
      "0m 11s (- 0m 34s) (250 25%) 0.2990 \n",
      "0m 12s (- 0m 29s) (300 30%) 0.2469 \n",
      "0m 14s (- 0m 26s) (350 35%) 0.2587 \n",
      "0m 15s (- 0m 23s) (400 40%) 0.2105 \n",
      "0m 16s (- 0m 20s) (450 45%) 0.3169 \n",
      "0m 18s (- 0m 18s) (500 50%) 0.1918 35.00%\n",
      "0m 21s (- 0m 17s) (550 55%) 0.1580 \n",
      "0m 22s (- 0m 14s) (600 60%) 0.2294 \n",
      "0m 23s (- 0m 12s) (650 65%) 0.1708 \n",
      "0m 24s (- 0m 10s) (700 70%) 0.2242 \n",
      "0m 26s (- 0m 8s) (750 75%) 0.1808 \n",
      "0m 27s (- 0m 6s) (800 80%) 0.2270 \n",
      "0m 28s (- 0m 5s) (850 85%) 0.2014 \n",
      "0m 29s (- 0m 3s) (900 90%) 0.2031 \n",
      "0m 31s (- 0m 1s) (950 95%) 0.2194 \n",
      "0m 32s (- 0m 0s) (1000 100%) 0.2527 42.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 39.00%\n",
      "0m 8s (- 2m 34s) (50 5%) 0.2175 \n",
      "0m 9s (- 1m 27s) (100 10%) 0.2281 \n",
      "0m 11s (- 1m 7s) (150 15%) 0.2322 \n",
      "0m 13s (- 0m 53s) (200 20%) 0.2345 \n",
      "0m 15s (- 0m 45s) (250 25%) 0.2491 \n",
      "0m 17s (- 0m 39s) (300 30%) 0.1614 \n",
      "0m 18s (- 0m 34s) (350 35%) 0.1876 \n",
      "0m 20s (- 0m 30s) (400 40%) 0.2191 \n",
      "0m 22s (- 0m 26s) (450 45%) 0.1696 \n",
      "0m 23s (- 0m 23s) (500 50%) 0.2750 42.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 27s (- 0m 22s) (550 55%) 0.1913 \n",
      "0m 29s (- 0m 19s) (600 60%) 0.1751 \n",
      "0m 30s (- 0m 16s) (650 65%) 0.2070 \n",
      "0m 32s (- 0m 13s) (700 70%) 0.2574 \n",
      "0m 34s (- 0m 11s) (750 75%) 0.2228 \n",
      "0m 35s (- 0m 8s) (800 80%) 0.2495 \n",
      "0m 37s (- 0m 6s) (850 85%) 0.2237 \n",
      "0m 39s (- 0m 4s) (900 90%) 0.1570 \n",
      "0m 40s (- 0m 2s) (950 95%) 0.2232 \n",
      "0m 42s (- 0m 0s) (1000 100%) 0.1791 39.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 38.80%\n",
      "0m 7s (- 2m 25s) (50 5%) 0.2002 \n",
      "0m 9s (- 1m 22s) (100 10%) 0.2120 \n",
      "0m 10s (- 1m 1s) (150 15%) 0.2259 \n",
      "0m 12s (- 0m 49s) (200 20%) 0.2215 \n",
      "0m 14s (- 0m 42s) (250 25%) 0.1665 \n",
      "0m 15s (- 0m 36s) (300 30%) 0.1648 \n",
      "0m 17s (- 0m 31s) (350 35%) 0.2719 \n",
      "0m 18s (- 0m 28s) (400 40%) 0.1703 \n",
      "0m 20s (- 0m 25s) (450 45%) 0.1945 \n",
      "0m 22s (- 0m 22s) (500 50%) 0.2626 41.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 25s (- 0m 21s) (550 55%) 0.2261 \n",
      "0m 27s (- 0m 18s) (600 60%) 0.2863 \n",
      "0m 28s (- 0m 15s) (650 65%) 0.1968 \n",
      "0m 30s (- 0m 13s) (700 70%) 0.2235 \n",
      "0m 32s (- 0m 10s) (750 75%) 0.2144 \n",
      "0m 34s (- 0m 8s) (800 80%) 0.1473 \n",
      "0m 35s (- 0m 6s) (850 85%) 0.1795 \n",
      "0m 37s (- 0m 4s) (900 90%) 0.1899 \n",
      "0m 39s (- 0m 2s) (950 95%) 0.2427 \n",
      "0m 41s (- 0m 0s) (1000 100%) 0.2064 44.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 46.40%\n",
      "0m 7s (- 2m 25s) (50 5%) 0.2124 \n",
      "0m 9s (- 1m 24s) (100 10%) 0.2538 \n",
      "0m 11s (- 1m 2s) (150 15%) 0.1588 \n",
      "0m 12s (- 0m 50s) (200 20%) 0.2729 \n",
      "0m 14s (- 0m 43s) (250 25%) 0.1815 \n",
      "0m 15s (- 0m 36s) (300 30%) 0.1614 \n",
      "0m 16s (- 0m 31s) (350 35%) 0.1456 \n",
      "0m 18s (- 0m 27s) (400 40%) 0.1608 \n",
      "0m 19s (- 0m 24s) (450 45%) 0.2019 \n",
      "0m 21s (- 0m 21s) (500 50%) 0.2446 55.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 25s (- 0m 20s) (550 55%) 0.1920 \n",
      "0m 27s (- 0m 18s) (600 60%) 0.3419 \n",
      "0m 28s (- 0m 15s) (650 65%) 0.2084 \n",
      "0m 30s (- 0m 13s) (700 70%) 0.2218 \n",
      "0m 32s (- 0m 10s) (750 75%) 0.1660 \n",
      "0m 33s (- 0m 8s) (800 80%) 0.2287 \n",
      "0m 35s (- 0m 6s) (850 85%) 0.1556 \n",
      "0m 36s (- 0m 4s) (900 90%) 0.2366 \n",
      "0m 38s (- 0m 2s) (950 95%) 0.2299 \n",
      "0m 39s (- 0m 0s) (1000 100%) 0.1696 51.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 46.60%\n",
      "0m 7s (- 2m 18s) (50 5%) 0.2130 \n",
      "0m 8s (- 1m 20s) (100 10%) 0.1342 \n",
      "0m 10s (- 0m 58s) (150 15%) 0.3052 \n",
      "0m 11s (- 0m 46s) (200 20%) 0.1433 \n",
      "0m 12s (- 0m 38s) (250 25%) 0.1490 \n",
      "0m 14s (- 0m 34s) (300 30%) 0.1744 \n",
      "0m 16s (- 0m 29s) (350 35%) 0.2392 \n",
      "0m 17s (- 0m 26s) (400 40%) 0.1450 \n",
      "0m 18s (- 0m 22s) (450 45%) 0.1357 \n",
      "0m 20s (- 0m 20s) (500 50%) 0.2777 53.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 24s (- 0m 19s) (550 55%) 0.2106 \n",
      "0m 25s (- 0m 17s) (600 60%) 0.1826 \n",
      "0m 27s (- 0m 14s) (650 65%) 0.1605 \n",
      "0m 29s (- 0m 12s) (700 70%) 0.1257 \n",
      "0m 31s (- 0m 10s) (750 75%) 0.1791 \n",
      "0m 32s (- 0m 8s) (800 80%) 0.1490 \n",
      "0m 34s (- 0m 6s) (850 85%) 0.1264 \n",
      "0m 36s (- 0m 4s) (900 90%) 0.1492 \n",
      "0m 37s (- 0m 1s) (950 95%) 0.1262 \n",
      "0m 39s (- 0m 0s) (1000 100%) 0.2390 46.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 49.40%\n",
      "0m 6s (- 2m 10s) (50 5%) 0.1686 \n",
      "0m 8s (- 1m 15s) (100 10%) 0.1646 \n",
      "0m 9s (- 0m 56s) (150 15%) 0.2086 \n",
      "0m 11s (- 0m 45s) (200 20%) 0.2002 \n",
      "0m 13s (- 0m 39s) (250 25%) 0.2141 \n",
      "0m 14s (- 0m 33s) (300 30%) 0.1446 \n",
      "0m 15s (- 0m 29s) (350 35%) 0.2058 \n",
      "0m 17s (- 0m 26s) (400 40%) 0.1376 \n",
      "0m 19s (- 0m 23s) (450 45%) 0.1801 \n",
      "0m 20s (- 0m 20s) (500 50%) 0.1411 46.50%\n",
      "0m 24s (- 0m 19s) (550 55%) 0.2378 \n",
      "0m 26s (- 0m 17s) (600 60%) 0.1953 \n",
      "0m 27s (- 0m 14s) (650 65%) 0.1446 \n",
      "0m 29s (- 0m 12s) (700 70%) 0.1643 \n",
      "0m 30s (- 0m 10s) (750 75%) 0.1611 \n",
      "0m 32s (- 0m 8s) (800 80%) 0.1360 \n",
      "0m 33s (- 0m 5s) (850 85%) 0.2002 \n",
      "0m 35s (- 0m 3s) (900 90%) 0.1191 \n",
      "0m 36s (- 0m 1s) (950 95%) 0.1565 \n",
      "0m 38s (- 0m 0s) (1000 100%) 0.2138 49.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 46.80%\n",
      "0m 6s (- 2m 5s) (50 5%) 0.1598 \n",
      "0m 7s (- 1m 10s) (100 10%) 0.1711 \n",
      "0m 9s (- 0m 52s) (150 15%) 0.1739 \n",
      "0m 10s (- 0m 41s) (200 20%) 0.2292 \n",
      "0m 11s (- 0m 35s) (250 25%) 0.1600 \n",
      "0m 13s (- 0m 31s) (300 30%) 0.1859 \n",
      "0m 14s (- 0m 27s) (350 35%) 0.1444 \n",
      "0m 15s (- 0m 23s) (400 40%) 0.1297 \n",
      "0m 17s (- 0m 20s) (450 45%) 0.2255 \n",
      "0m 18s (- 0m 18s) (500 50%) 0.1686 53.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 22s (- 0m 18s) (550 55%) 0.3106 \n",
      "0m 24s (- 0m 16s) (600 60%) 0.1345 \n",
      "0m 26s (- 0m 14s) (650 65%) 0.1987 \n",
      "0m 27s (- 0m 11s) (700 70%) 0.1157 \n",
      "0m 29s (- 0m 9s) (750 75%) 0.2089 \n",
      "0m 30s (- 0m 7s) (800 80%) 0.1541 \n",
      "0m 32s (- 0m 5s) (850 85%) 0.2308 \n",
      "0m 34s (- 0m 3s) (900 90%) 0.1932 \n",
      "0m 35s (- 0m 1s) (950 95%) 0.1561 \n",
      "0m 37s (- 0m 0s) (1000 100%) 0.1167 58.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 52.60%\n",
      "0m 6s (- 2m 3s) (50 5%) 0.2108 \n",
      "0m 7s (- 1m 11s) (100 10%) 0.2532 \n",
      "0m 9s (- 0m 52s) (150 15%) 0.1391 \n",
      "0m 10s (- 0m 43s) (200 20%) 0.1214 \n",
      "0m 12s (- 0m 36s) (250 25%) 0.1337 \n",
      "0m 13s (- 0m 32s) (300 30%) 0.1445 \n",
      "0m 15s (- 0m 28s) (350 35%) 0.1393 \n",
      "0m 17s (- 0m 25s) (400 40%) 0.1671 \n",
      "0m 18s (- 0m 22s) (450 45%) 0.1361 \n",
      "0m 20s (- 0m 20s) (500 50%) 0.1671 54.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 24s (- 0m 19s) (550 55%) 0.2198 \n",
      "0m 25s (- 0m 17s) (600 60%) 0.2037 \n",
      "0m 27s (- 0m 14s) (650 65%) 0.1366 \n",
      "0m 29s (- 0m 12s) (700 70%) 0.1753 \n",
      "0m 30s (- 0m 10s) (750 75%) 0.1868 \n",
      "0m 32s (- 0m 8s) (800 80%) 0.1816 \n",
      "0m 33s (- 0m 5s) (850 85%) 0.1395 \n",
      "0m 35s (- 0m 3s) (900 90%) 0.1649 \n",
      "0m 37s (- 0m 1s) (950 95%) 0.1438 \n",
      "0m 38s (- 0m 0s) (1000 100%) 0.2649 43.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 53.20%\n",
      "0m 7s (- 2m 13s) (50 5%) 0.1740 \n",
      "0m 8s (- 1m 16s) (100 10%) 0.2546 \n",
      "0m 9s (- 0m 56s) (150 15%) 0.1654 \n",
      "0m 11s (- 0m 45s) (200 20%) 0.1917 \n",
      "0m 12s (- 0m 38s) (250 25%) 0.1508 \n",
      "0m 14s (- 0m 33s) (300 30%) 0.1635 \n",
      "0m 15s (- 0m 28s) (350 35%) 0.1310 \n",
      "0m 17s (- 0m 25s) (400 40%) 0.1535 \n",
      "0m 18s (- 0m 22s) (450 45%) 0.1323 \n",
      "0m 19s (- 0m 19s) (500 50%) 0.1954 45.00%\n",
      "0m 23s (- 0m 19s) (550 55%) 0.1910 \n",
      "0m 24s (- 0m 16s) (600 60%) 0.1677 \n",
      "0m 25s (- 0m 13s) (650 65%) 0.1771 \n",
      "0m 27s (- 0m 11s) (700 70%) 0.3031 \n",
      "0m 28s (- 0m 9s) (750 75%) 0.1073 \n",
      "0m 30s (- 0m 7s) (800 80%) 0.1413 \n",
      "0m 32s (- 0m 5s) (850 85%) 0.3192 \n",
      "0m 34s (- 0m 3s) (900 90%) 0.1242 \n",
      "0m 35s (- 0m 1s) (950 95%) 0.1387 \n",
      "0m 37s (- 0m 0s) (1000 100%) 0.1304 56.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 56.00%\n",
      "0m 7s (- 2m 18s) (50 5%) 0.2061 \n",
      "0m 8s (- 1m 19s) (100 10%) 0.1653 \n",
      "0m 10s (- 0m 58s) (150 15%) 0.2188 \n",
      "0m 11s (- 0m 47s) (200 20%) 0.1021 \n",
      "0m 13s (- 0m 40s) (250 25%) 0.2094 \n",
      "0m 14s (- 0m 34s) (300 30%) 0.1334 \n",
      "0m 16s (- 0m 30s) (350 35%) 0.1180 \n",
      "0m 17s (- 0m 26s) (400 40%) 0.0960 \n",
      "0m 19s (- 0m 23s) (450 45%) 0.1640 \n",
      "0m 21s (- 0m 21s) (500 50%) 0.1659 54.00%\n",
      "0m 24s (- 0m 20s) (550 55%) 0.1184 \n",
      "0m 26s (- 0m 17s) (600 60%) 0.1947 \n",
      "0m 28s (- 0m 15s) (650 65%) 0.1476 \n",
      "0m 29s (- 0m 12s) (700 70%) 0.1473 \n",
      "0m 31s (- 0m 10s) (750 75%) 0.1517 \n",
      "0m 32s (- 0m 8s) (800 80%) 0.1515 \n",
      "0m 34s (- 0m 6s) (850 85%) 0.1147 \n",
      "0m 35s (- 0m 3s) (900 90%) 0.1350 \n",
      "0m 37s (- 0m 1s) (950 95%) 0.1244 \n",
      "0m 39s (- 0m 0s) (1000 100%) 0.2621 53.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 53.60%\n",
      "0m 7s (- 2m 13s) (50 5%) 0.1741 \n",
      "0m 8s (- 1m 16s) (100 10%) 0.1457 \n",
      "0m 10s (- 0m 57s) (150 15%) 0.1384 \n",
      "0m 11s (- 0m 47s) (200 20%) 0.1708 \n",
      "0m 13s (- 0m 40s) (250 25%) 0.2015 \n",
      "0m 15s (- 0m 35s) (300 30%) 0.1181 \n",
      "0m 16s (- 0m 30s) (350 35%) 0.1169 \n",
      "0m 18s (- 0m 27s) (400 40%) 0.1192 \n",
      "0m 19s (- 0m 24s) (450 45%) 0.1581 \n",
      "0m 21s (- 0m 21s) (500 50%) 0.1294 55.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 25s (- 0m 21s) (550 55%) 0.3228 \n",
      "0m 26s (- 0m 17s) (600 60%) 0.1564 \n",
      "0m 28s (- 0m 15s) (650 65%) 0.1851 \n",
      "0m 30s (- 0m 13s) (700 70%) 0.1400 \n",
      "0m 32s (- 0m 10s) (750 75%) 0.1157 \n",
      "0m 33s (- 0m 8s) (800 80%) 0.0919 \n",
      "0m 35s (- 0m 6s) (850 85%) 0.1791 \n",
      "0m 36s (- 0m 4s) (900 90%) 0.1821 \n",
      "0m 38s (- 0m 2s) (950 95%) 0.1660 \n",
      "0m 39s (- 0m 0s) (1000 100%) 0.1515 57.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 50.60%\n",
      "0m 6s (- 1m 56s) (50 5%) 0.1681 \n",
      "0m 7s (- 1m 8s) (100 10%) 0.1616 \n",
      "0m 8s (- 0m 50s) (150 15%) 0.1589 \n",
      "0m 10s (- 0m 41s) (200 20%) 0.1567 \n",
      "0m 11s (- 0m 34s) (250 25%) 0.1570 \n",
      "0m 12s (- 0m 29s) (300 30%) 0.1293 \n",
      "0m 14s (- 0m 26s) (350 35%) 0.1187 \n",
      "0m 15s (- 0m 23s) (400 40%) 0.1470 \n",
      "0m 17s (- 0m 20s) (450 45%) 0.1623 \n",
      "0m 18s (- 0m 18s) (500 50%) 0.1285 66.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 21s (- 0m 17s) (550 55%) 0.1614 \n",
      "0m 22s (- 0m 15s) (600 60%) 0.1500 \n",
      "0m 24s (- 0m 13s) (650 65%) 0.1220 \n",
      "0m 25s (- 0m 10s) (700 70%) 0.2418 \n",
      "0m 26s (- 0m 8s) (750 75%) 0.1321 \n",
      "0m 27s (- 0m 6s) (800 80%) 0.1227 \n",
      "0m 29s (- 0m 5s) (850 85%) 0.1624 \n",
      "0m 30s (- 0m 3s) (900 90%) 0.1249 \n",
      "0m 31s (- 0m 1s) (950 95%) 0.1230 \n",
      "0m 33s (- 0m 0s) (1000 100%) 0.2338 56.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 53.80%\n",
      "0m 5s (- 1m 46s) (50 5%) 0.1352 \n",
      "0m 6s (- 1m 2s) (100 10%) 0.1649 \n",
      "0m 8s (- 0m 46s) (150 15%) 0.1558 \n",
      "0m 9s (- 0m 38s) (200 20%) 0.1909 \n",
      "0m 10s (- 0m 32s) (250 25%) 0.1292 \n",
      "0m 11s (- 0m 27s) (300 30%) 0.1942 \n",
      "0m 13s (- 0m 24s) (350 35%) 0.1828 \n",
      "0m 14s (- 0m 21s) (400 40%) 0.1905 \n",
      "0m 15s (- 0m 19s) (450 45%) 0.2455 \n",
      "0m 17s (- 0m 17s) (500 50%) 0.1309 59.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 20s (- 0m 16s) (550 55%) 0.1037 \n",
      "0m 21s (- 0m 14s) (600 60%) 0.1831 \n",
      "0m 23s (- 0m 12s) (650 65%) 0.1317 \n",
      "0m 24s (- 0m 10s) (700 70%) 0.1958 \n",
      "0m 25s (- 0m 8s) (750 75%) 0.1156 \n",
      "0m 26s (- 0m 6s) (800 80%) 0.1072 \n",
      "0m 27s (- 0m 4s) (850 85%) 0.1100 \n",
      "0m 29s (- 0m 3s) (900 90%) 0.1209 \n",
      "0m 30s (- 0m 1s) (950 95%) 0.1274 \n",
      "0m 31s (- 0m 0s) (1000 100%) 0.1627 59.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 58.00%\n",
      "0m 6s (- 4m 30s) (50 2%) 0.1182 \n",
      "0m 8s (- 2m 38s) (100 5%) 0.0996 \n",
      "0m 9s (- 1m 58s) (150 7%) 0.1333 \n",
      "0m 11s (- 1m 40s) (200 10%) 0.0956 \n",
      "0m 12s (- 1m 28s) (250 12%) 0.1649 \n",
      "0m 14s (- 1m 19s) (300 15%) 0.1581 \n",
      "0m 15s (- 1m 13s) (350 17%) 0.1896 \n",
      "0m 17s (- 1m 8s) (400 20%) 0.1788 \n",
      "0m 18s (- 1m 4s) (450 22%) 0.1043 \n",
      "0m 20s (- 1m 0s) (500 25%) 0.1785 64.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 24s (- 1m 4s) (550 27%) 0.1784 \n",
      "0m 27s (- 1m 3s) (600 30%) 0.1202 \n",
      "0m 28s (- 0m 59s) (650 32%) 0.1119 \n",
      "0m 30s (- 0m 56s) (700 35%) 0.1697 \n",
      "0m 31s (- 0m 53s) (750 37%) 0.1251 \n",
      "0m 33s (- 0m 50s) (800 40%) 0.1143 \n",
      "0m 35s (- 0m 47s) (850 42%) 0.1176 \n",
      "0m 36s (- 0m 45s) (900 45%) 0.1110 \n",
      "0m 38s (- 0m 42s) (950 47%) 0.1630 \n",
      "0m 39s (- 0m 39s) (1000 50%) 0.1164 58.00%\n",
      "0m 43s (- 0m 39s) (1050 52%) 0.1215 \n",
      "0m 45s (- 0m 37s) (1100 55%) 0.1304 \n",
      "0m 47s (- 0m 34s) (1150 57%) 0.1700 \n",
      "0m 48s (- 0m 32s) (1200 60%) 0.1142 \n",
      "0m 50s (- 0m 30s) (1250 62%) 0.1691 \n",
      "0m 51s (- 0m 27s) (1300 65%) 0.1068 \n",
      "0m 53s (- 0m 25s) (1350 67%) 0.1500 \n",
      "0m 55s (- 0m 23s) (1400 70%) 0.1670 \n",
      "0m 56s (- 0m 21s) (1450 72%) 0.0998 \n",
      "0m 58s (- 0m 19s) (1500 75%) 0.1875 57.50%\n",
      "1m 2s (- 0m 18s) (1550 77%) 0.1606 \n",
      "1m 3s (- 0m 15s) (1600 80%) 0.1142 \n",
      "1m 5s (- 0m 13s) (1650 82%) 0.1878 \n",
      "1m 6s (- 0m 11s) (1700 85%) 0.2133 \n",
      "1m 8s (- 0m 9s) (1750 87%) 0.1483 \n",
      "1m 10s (- 0m 7s) (1800 90%) 0.1498 \n",
      "1m 12s (- 0m 5s) (1850 92%) 0.1317 \n",
      "1m 13s (- 0m 3s) (1900 95%) 0.1107 \n",
      "1m 15s (- 0m 1s) (1950 97%) 0.2864 \n",
      "1m 17s (- 0m 0s) (2000 100%) 0.1719 57.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 59.60%\n",
      "0m 7s (- 4m 40s) (50 2%) 0.1336 \n",
      "0m 8s (- 2m 47s) (100 5%) 0.1352 \n",
      "0m 10s (- 2m 8s) (150 7%) 0.2674 \n",
      "0m 12s (- 1m 48s) (200 10%) 0.1543 \n",
      "0m 13s (- 1m 35s) (250 12%) 0.1080 \n",
      "0m 15s (- 1m 26s) (300 15%) 0.1191 \n",
      "0m 16s (- 1m 19s) (350 17%) 0.1157 \n",
      "0m 18s (- 1m 14s) (400 20%) 0.1292 \n",
      "0m 20s (- 1m 9s) (450 22%) 0.1310 \n",
      "0m 21s (- 1m 5s) (500 25%) 0.1737 64.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 25s (- 1m 8s) (550 27%) 0.1331 \n",
      "0m 27s (- 1m 4s) (600 30%) 0.0967 \n",
      "0m 29s (- 1m 0s) (650 32%) 0.1792 \n",
      "0m 30s (- 0m 57s) (700 35%) 0.1233 \n",
      "0m 32s (- 0m 53s) (750 37%) 0.1973 \n",
      "0m 33s (- 0m 50s) (800 40%) 0.1209 \n",
      "0m 35s (- 0m 47s) (850 42%) 0.1149 \n",
      "0m 36s (- 0m 45s) (900 45%) 0.1116 \n",
      "0m 38s (- 0m 42s) (950 47%) 0.1702 \n",
      "0m 40s (- 0m 40s) (1000 50%) 0.1111 61.00%\n",
      "0m 43s (- 0m 39s) (1050 52%) 0.2020 \n",
      "0m 45s (- 0m 37s) (1100 55%) 0.1107 \n",
      "0m 47s (- 0m 34s) (1150 57%) 0.1091 \n",
      "0m 48s (- 0m 32s) (1200 60%) 0.1524 \n",
      "0m 49s (- 0m 29s) (1250 62%) 0.1989 \n",
      "0m 51s (- 0m 27s) (1300 65%) 0.1584 \n",
      "0m 52s (- 0m 25s) (1350 67%) 0.1446 \n",
      "0m 55s (- 0m 23s) (1400 70%) 0.2084 \n",
      "0m 56s (- 0m 21s) (1450 72%) 0.1495 \n",
      "0m 58s (- 0m 19s) (1500 75%) 0.1248 59.50%\n",
      "1m 3s (- 0m 18s) (1550 77%) 0.1300 \n",
      "1m 5s (- 0m 16s) (1600 80%) 0.2201 \n",
      "1m 6s (- 0m 14s) (1650 82%) 0.1755 \n",
      "1m 9s (- 0m 12s) (1700 85%) 0.1496 \n",
      "1m 10s (- 0m 10s) (1750 87%) 0.1917 \n",
      "1m 11s (- 0m 7s) (1800 90%) 0.1535 \n",
      "1m 14s (- 0m 6s) (1850 92%) 0.2309 \n",
      "1m 16s (- 0m 4s) (1900 95%) 0.1871 \n",
      "1m 17s (- 0m 1s) (1950 97%) 0.1014 \n",
      "1m 19s (- 0m 0s) (2000 100%) 0.1135 61.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 60.20%\n",
      "0m 7s (- 2m 16s) (50 5%) 0.1058 \n",
      "0m 8s (- 1m 19s) (100 10%) 0.1121 \n",
      "0m 9s (- 0m 56s) (150 15%) 0.1149 \n",
      "0m 11s (- 0m 44s) (200 20%) 0.2265 \n",
      "0m 12s (- 0m 36s) (250 25%) 0.1737 \n",
      "0m 13s (- 0m 31s) (300 30%) 0.1746 \n",
      "0m 14s (- 0m 27s) (350 35%) 0.1108 \n",
      "0m 16s (- 0m 24s) (400 40%) 0.1897 \n",
      "0m 17s (- 0m 21s) (450 45%) 0.1772 \n",
      "0m 19s (- 0m 19s) (500 50%) 0.1176 58.50%\n",
      "0m 23s (- 0m 19s) (550 55%) 0.1222 \n",
      "0m 25s (- 0m 17s) (600 60%) 0.1084 \n",
      "0m 27s (- 0m 14s) (650 65%) 0.1299 \n",
      "0m 29s (- 0m 12s) (700 70%) 0.1034 \n",
      "0m 30s (- 0m 10s) (750 75%) 0.1028 \n",
      "0m 32s (- 0m 8s) (800 80%) 0.1743 \n",
      "0m 34s (- 0m 6s) (850 85%) 0.1880 \n",
      "0m 35s (- 0m 3s) (900 90%) 0.1671 \n",
      "0m 37s (- 0m 1s) (950 95%) 0.2005 \n",
      "0m 38s (- 0m 0s) (1000 100%) 0.1170 64.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 59.20%\n",
      "0m 8s (- 2m 44s) (50 5%) 0.1224 \n",
      "0m 10s (- 1m 35s) (100 10%) 0.1838 \n",
      "0m 12s (- 1m 10s) (150 15%) 0.1206 \n",
      "0m 14s (- 0m 56s) (200 20%) 0.1294 \n",
      "0m 15s (- 0m 46s) (250 25%) 0.1856 \n",
      "0m 17s (- 0m 39s) (300 30%) 0.1204 \n",
      "0m 18s (- 0m 35s) (350 35%) 0.1005 \n",
      "0m 20s (- 0m 30s) (400 40%) 0.1113 \n",
      "0m 22s (- 0m 27s) (450 45%) 0.1546 \n",
      "0m 23s (- 0m 23s) (500 50%) 0.1693 59.00%\n",
      "0m 27s (- 0m 22s) (550 55%) 0.1748 \n",
      "0m 29s (- 0m 19s) (600 60%) 0.1348 \n",
      "0m 31s (- 0m 16s) (650 65%) 0.1328 \n",
      "0m 32s (- 0m 14s) (700 70%) 0.1601 \n",
      "0m 34s (- 0m 11s) (750 75%) 0.1406 \n",
      "0m 36s (- 0m 9s) (800 80%) 0.1232 \n",
      "0m 37s (- 0m 6s) (850 85%) 0.1172 \n",
      "0m 39s (- 0m 4s) (900 90%) 0.1395 \n",
      "0m 41s (- 0m 2s) (950 95%) 0.1683 \n",
      "0m 42s (- 0m 0s) (1000 100%) 0.1467 61.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 63.00%\n",
      "0m 8s (- 2m 42s) (50 5%) 0.1006 \n",
      "0m 10s (- 1m 31s) (100 10%) 0.1370 \n",
      "0m 11s (- 1m 5s) (150 15%) 0.1563 \n",
      "0m 13s (- 0m 53s) (200 20%) 0.2312 \n",
      "0m 14s (- 0m 43s) (250 25%) 0.1033 \n",
      "0m 15s (- 0m 37s) (300 30%) 0.0932 \n",
      "0m 17s (- 0m 31s) (350 35%) 0.1271 \n",
      "0m 18s (- 0m 27s) (400 40%) 0.1340 \n",
      "0m 19s (- 0m 23s) (450 45%) 0.1161 \n",
      "0m 21s (- 0m 21s) (500 50%) 0.1882 53.00%\n",
      "0m 25s (- 0m 20s) (550 55%) 0.1273 \n",
      "0m 27s (- 0m 18s) (600 60%) 0.1173 \n",
      "0m 29s (- 0m 15s) (650 65%) 0.1819 \n",
      "0m 30s (- 0m 13s) (700 70%) 0.1155 \n",
      "0m 32s (- 0m 10s) (750 75%) 0.1126 \n",
      "0m 34s (- 0m 8s) (800 80%) 0.1233 \n",
      "0m 35s (- 0m 6s) (850 85%) 0.1353 \n",
      "0m 37s (- 0m 4s) (900 90%) 0.1156 \n",
      "0m 39s (- 0m 2s) (950 95%) 0.0980 \n",
      "0m 41s (- 0m 0s) (1000 100%) 0.1257 64.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 56.20%\n",
      "0m 7s (- 2m 19s) (50 5%) 0.1848 \n",
      "0m 8s (- 1m 20s) (100 10%) 0.1149 \n",
      "0m 10s (- 1m 1s) (150 15%) 0.1831 \n",
      "0m 12s (- 0m 49s) (200 20%) 0.1511 \n",
      "0m 13s (- 0m 41s) (250 25%) 0.1180 \n",
      "0m 15s (- 0m 35s) (300 30%) 0.1304 \n",
      "0m 16s (- 0m 30s) (350 35%) 0.1368 \n",
      "0m 18s (- 0m 27s) (400 40%) 0.1210 \n",
      "0m 19s (- 0m 24s) (450 45%) 0.2154 \n",
      "0m 21s (- 0m 21s) (500 50%) 0.1164 56.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 24s (- 0m 20s) (550 55%) 0.0987 \n",
      "0m 26s (- 0m 17s) (600 60%) 0.2404 \n",
      "0m 27s (- 0m 14s) (650 65%) 0.1223 \n",
      "0m 29s (- 0m 12s) (700 70%) 0.1378 \n",
      "0m 30s (- 0m 10s) (750 75%) 0.1087 \n",
      "0m 32s (- 0m 8s) (800 80%) 0.1716 \n",
      "0m 34s (- 0m 6s) (850 85%) 0.1426 \n",
      "0m 35s (- 0m 3s) (900 90%) 0.1665 \n",
      "0m 37s (- 0m 1s) (950 95%) 0.1370 \n",
      "0m 38s (- 0m 0s) (1000 100%) 0.1409 59.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60150000000000003"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "    encoder2 = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "    decoder2 = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "evaluateAccuracy(encoder2, decoder2, n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> look thrice and jump around right\n",
      "= I_LOOK I_LOOK I_LOOK I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP\n",
      "< I_LOOK I_LOOK I_LOOK I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> jump and turn right twice\n",
      "= I_JUMP I_TURN_RIGHT I_TURN_RIGHT\n",
      "< I_JUMP I_TURN_RIGHT I_TURN_RIGHT <EOS>\n",
      "\n",
      "> jump right twice and walk right twice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK <EOS>\n",
      "\n",
      "> jump right thrice and jump right twice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> jump opposite left twice after run left thrice\n",
      "= I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP\n",
      "< I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> jump opposite right twice after walk right twice\n",
      "= I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP\n",
      "< I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP <EOS>\n",
      "\n",
      "> walk around left twice after jump around right\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK <EOS>\n",
      "\n",
      "> run opposite right twice after jump around left\n",
      "= I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TURN_RIGHT I_RUN\n",
      "< I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TURN_RIGHT I_RUN <EOS>\n",
      "\n",
      "> turn right twice after jump around right\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT <EOS>\n",
      "\n",
      "> look around right twice after jump right twice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCAN Add-Prim JUMP Experiment\n",
    "*************************************************************\n",
    "\n",
    "Reference: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Python 3.6\n",
    "* PyTorch 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Device is using\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "TASK_NAME = \"addprim-jump\"\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False, trainOrtest='train'):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines        \n",
    "    lines = open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/processed/{}-{}_{}-{}.txt'.\\\n",
    "                 format(trainOrtest, TASK_NAME, lang1, lang2), encoding='utf-8').\\\n",
    "                 read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "# PRED_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 37046 sentence pairs\n",
      "Trimmed to 37046 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['walk right twice and look around right', 'I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataFrom='train'):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=False, trainOrtest=dataFrom)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('in', 'out', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "=================\n",
    "\n",
    "The model we are using is a GRU encoder-decoder seq2seq model with attention mechanism. In order to solve the zero-shot generalization task, we embed the encoder networks with pre-trained embeddings, from GloVe and Google Word2Vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_SOURCE = 'glove'\n",
    "hidden_size = 300\n",
    "\n",
    "if EMBEDDEING_SOURCE == 'google':\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_GoogleNews300Negative.pkl', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "else:\n",
    "    with open('/Users/Viola/CDS/AAI/Project/SCAN-Learn/data/emb_pretrained/embedding_raw{}d.pkl'.format(hidden_size), 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "\n",
    "pretrained_emb = np.zeros((input_lang.n_words, hidden_size))\n",
    "for k, v in input_lang.index2word.items():\n",
    "    if v == 'SOS':\n",
    "        pretrained_emb[k] = np.zeros(hidden_size)\n",
    "    elif (v == 'EOS') and (EMBEDDEING_SOURCE != 'google'):\n",
    "        pretrained_emb[k] = b['.']\n",
    "    elif (v == 'and') and (EMBEDDEING_SOURCE == 'google'):\n",
    "        pretrained_emb[k] = b['AND']\n",
    "    else:\n",
    "        pretrained_emb[k] = b[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of this seq2seq network is a GRU netword. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDEING_PRETRAINED = True\n",
    "WEIGHT_UPDATE = False\n",
    "\n",
    "MODEL_VERSION = 'T0.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        if EMBEDDEING_PRETRAINED:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "            self.embedding.weight.requires_grad = WEIGHT_UPDATE\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is a GRU network with attention mechanism that takes the last output of the encoder and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "First we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "We use teacher forcing to help converge faster with a delay fashion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, \n",
    "          max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for timing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, eval_every=1000, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "        encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "        decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "        \n",
    "    best_test_acc = evaluateAccuracy(encoder, decoder, 500)\n",
    "    print(\"Best evaluation accuracy: {0:.2f}%\".format(best_test_acc * 100))\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        \n",
    "    encoder_optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg), end=' ')\n",
    "            \n",
    "            if iter % eval_every == 0:\n",
    "                test_acc = evaluateAccuracy(encoder, decoder, 200)\n",
    "                print('{0:.2f}%'.format(test_acc * 100))\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    with open(\"saved_models/encoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(encoder, f)\n",
    "                    with open(\"saved_models/decoder_\" + MODEL_VERSION, \"wb\") as f:\n",
    "                        torch.save(decoder, f)\n",
    "                    print(\"New best test accuracy! Model Updated!\")\n",
    "                    best_test_acc = test_acc\n",
    "#                 elif test_acc < best_test_acc - 0.001:\n",
    "#                     encoder = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "#                     decoder = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "                    \n",
    "            else:\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 15412 sentence pairs\n",
      "Trimmed to 15412 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "in 15\n",
      "out 8\n",
      "['turn around right twice after jump opposite right twice', 'I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs_eval = prepareData('in', 'out', True, dataFrom='test')\n",
    "print(random.choice(pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateAccuracy(encoder, decoder, n=10):\n",
    "    ACCs = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_eval)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        \n",
    "        if output_words[-1] == '<EOS>':\n",
    "            output_words = output_words[:-1]\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        if output_sentence == pair[1]:\n",
    "            ACCs.append(1)\n",
    "        else:\n",
    "            ACCs.append(0)\n",
    "    return np.array(ACCs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initially trained with a higher teacher aid, and relatively large learning rate. Both teacher forcing effect and the learning rate decay over iterations when the model approaches the optimum.  \n",
    "\n",
    "#### The model achieves 97% accuracy rate for the best test sample evaluation, and is 94% correct on average for the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 0.00%\n",
      "0m 33s (- 55m 16s) (50 1%) 1.8531 \n",
      "0m 37s (- 30m 56s) (100 2%) 1.6460 \n",
      "0m 41s (- 22m 20s) (150 3%) 1.2627 \n",
      "0m 45s (- 18m 7s) (200 4%) 1.1962 \n",
      "0m 49s (- 15m 36s) (250 5%) 1.3049 \n",
      "0m 52s (- 13m 48s) (300 6%) 1.0255 \n",
      "0m 56s (- 12m 36s) (350 7%) 1.1453 \n",
      "1m 1s (- 11m 45s) (400 8%) 1.1230 \n",
      "1m 5s (- 10m 58s) (450 9%) 1.1119 \n",
      "1m 8s (- 10m 17s) (500 10%) 0.8724 0.00%\n",
      "1m 19s (- 10m 46s) (550 11%) 0.9449 \n",
      "1m 24s (- 10m 16s) (600 12%) 0.8409 \n",
      "1m 28s (- 9m 49s) (650 13%) 0.9497 \n",
      "1m 32s (- 9m 27s) (700 14%) 0.8867 \n",
      "1m 37s (- 9m 12s) (750 15%) 0.7976 \n",
      "1m 41s (- 8m 52s) (800 16%) 0.8399 \n",
      "1m 45s (- 8m 34s) (850 17%) 0.7813 \n",
      "1m 49s (- 8m 16s) (900 18%) 0.7319 \n",
      "1m 53s (- 8m 2s) (950 19%) 0.7370 \n",
      "1m 57s (- 7m 49s) (1000 20%) 0.6536 0.00%\n",
      "2m 8s (- 8m 5s) (1050 21%) 0.7980 \n",
      "2m 13s (- 7m 52s) (1100 22%) 0.6912 \n",
      "2m 17s (- 7m 39s) (1150 23%) 0.6949 \n",
      "2m 21s (- 7m 26s) (1200 24%) 0.6980 \n",
      "2m 25s (- 7m 16s) (1250 25%) 0.5271 \n",
      "2m 29s (- 7m 4s) (1300 26%) 0.6871 \n",
      "2m 32s (- 6m 53s) (1350 27%) 0.6417 \n",
      "2m 36s (- 6m 42s) (1400 28%) 0.5850 \n",
      "2m 40s (- 6m 33s) (1450 28%) 0.6666 \n",
      "2m 44s (- 6m 24s) (1500 30%) 0.5704 0.00%\n",
      "2m 56s (- 6m 32s) (1550 31%) 0.5841 \n",
      "3m 0s (- 6m 23s) (1600 32%) 0.6637 \n",
      "3m 4s (- 6m 14s) (1650 33%) 0.5419 \n",
      "3m 8s (- 6m 6s) (1700 34%) 0.5533 \n",
      "3m 12s (- 5m 58s) (1750 35%) 0.5736 \n",
      "3m 16s (- 5m 49s) (1800 36%) 0.4069 \n",
      "3m 20s (- 5m 41s) (1850 37%) 0.5208 \n",
      "3m 24s (- 5m 34s) (1900 38%) 0.5131 \n",
      "3m 29s (- 5m 27s) (1950 39%) 0.7005 \n",
      "3m 33s (- 5m 20s) (2000 40%) 0.5364 1.00%\n",
      "New best test accuracy! Model Updated!\n",
      "3m 42s (- 5m 20s) (2050 41%) 0.4620 \n",
      "3m 46s (- 5m 12s) (2100 42%) 0.5273 \n",
      "3m 50s (- 5m 5s) (2150 43%) 0.6624 \n",
      "3m 53s (- 4m 57s) (2200 44%) 0.5429 \n",
      "3m 57s (- 4m 50s) (2250 45%) 0.4541 \n",
      "4m 1s (- 4m 43s) (2300 46%) 0.4689 \n",
      "4m 5s (- 4m 37s) (2350 47%) 0.4411 \n",
      "4m 9s (- 4m 30s) (2400 48%) 0.4471 \n",
      "4m 13s (- 4m 24s) (2450 49%) 0.5133 \n",
      "4m 17s (- 4m 17s) (2500 50%) 0.4741 3.00%\n",
      "New best test accuracy! Model Updated!\n",
      "4m 26s (- 4m 15s) (2550 51%) 0.3800 \n",
      "4m 30s (- 4m 9s) (2600 52%) 0.3566 \n",
      "4m 34s (- 4m 3s) (2650 53%) 0.4683 \n",
      "4m 38s (- 3m 56s) (2700 54%) 0.4786 \n",
      "4m 42s (- 3m 50s) (2750 55%) 0.5232 \n",
      "4m 45s (- 3m 44s) (2800 56%) 0.3930 \n",
      "4m 49s (- 3m 38s) (2850 56%) 0.4377 \n",
      "4m 53s (- 3m 32s) (2900 57%) 0.4067 \n",
      "4m 57s (- 3m 26s) (2950 59%) 0.4310 \n",
      "5m 2s (- 3m 21s) (3000 60%) 0.3839 7.00%\n",
      "New best test accuracy! Model Updated!\n",
      "5m 10s (- 3m 18s) (3050 61%) 0.4396 \n",
      "5m 14s (- 3m 12s) (3100 62%) 0.3529 \n",
      "5m 18s (- 3m 6s) (3150 63%) 0.4340 \n",
      "5m 22s (- 3m 1s) (3200 64%) 0.4464 \n",
      "5m 25s (- 2m 55s) (3250 65%) 0.3913 \n",
      "5m 29s (- 2m 49s) (3300 66%) 0.5090 \n",
      "5m 33s (- 2m 44s) (3350 67%) 0.4030 \n",
      "5m 37s (- 2m 38s) (3400 68%) 0.3946 \n",
      "5m 41s (- 2m 33s) (3450 69%) 0.3573 \n",
      "5m 45s (- 2m 28s) (3500 70%) 0.4112 17.00%\n",
      "New best test accuracy! Model Updated!\n",
      "5m 52s (- 2m 24s) (3550 71%) 0.3142 \n",
      "5m 56s (- 2m 18s) (3600 72%) 0.4774 \n",
      "6m 0s (- 2m 13s) (3650 73%) 0.3623 \n",
      "6m 4s (- 2m 8s) (3700 74%) 0.3174 \n",
      "6m 8s (- 2m 2s) (3750 75%) 0.3649 \n",
      "6m 12s (- 1m 57s) (3800 76%) 0.4544 \n",
      "6m 16s (- 1m 52s) (3850 77%) 0.4398 \n",
      "6m 20s (- 1m 47s) (3900 78%) 0.3787 \n",
      "6m 25s (- 1m 42s) (3950 79%) 0.2906 \n",
      "6m 29s (- 1m 37s) (4000 80%) 0.3320 15.00%\n",
      "6m 37s (- 1m 33s) (4050 81%) 0.3384 \n",
      "6m 41s (- 1m 28s) (4100 82%) 0.3373 \n",
      "6m 46s (- 1m 23s) (4150 83%) 0.3676 \n",
      "6m 49s (- 1m 18s) (4200 84%) 0.3346 \n",
      "6m 53s (- 1m 12s) (4250 85%) 0.3133 \n",
      "6m 57s (- 1m 8s) (4300 86%) 0.4051 \n",
      "7m 2s (- 1m 3s) (4350 87%) 0.3357 \n",
      "7m 6s (- 0m 58s) (4400 88%) 0.4058 \n",
      "7m 10s (- 0m 53s) (4450 89%) 0.3673 \n",
      "7m 15s (- 0m 48s) (4500 90%) 0.2987 15.50%\n",
      "7m 23s (- 0m 43s) (4550 91%) 0.3358 \n",
      "7m 27s (- 0m 38s) (4600 92%) 0.2806 \n",
      "7m 32s (- 0m 34s) (4650 93%) 0.3595 \n",
      "7m 36s (- 0m 29s) (4700 94%) 0.3719 \n",
      "7m 41s (- 0m 24s) (4750 95%) 0.3124 \n",
      "7m 45s (- 0m 19s) (4800 96%) 0.3100 \n",
      "7m 49s (- 0m 14s) (4850 97%) 0.2555 \n",
      "7m 54s (- 0m 9s) (4900 98%) 0.3392 \n",
      "7m 58s (- 0m 4s) (4950 99%) 0.4312 \n",
      "8m 2s (- 0m 0s) (5000 100%) 0.3010 16.00%\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 13.60%\n",
      "0m 13s (- 4m 23s) (50 5%) 0.5547 \n",
      "0m 18s (- 2m 49s) (100 10%) 0.5065 \n",
      "0m 23s (- 2m 13s) (150 15%) 0.4760 \n",
      "0m 27s (- 1m 51s) (200 20%) 0.3585 \n",
      "0m 32s (- 1m 38s) (250 25%) 0.4330 \n",
      "0m 37s (- 1m 26s) (300 30%) 0.3983 \n",
      "0m 42s (- 1m 18s) (350 35%) 0.4234 \n",
      "0m 46s (- 1m 9s) (400 40%) 0.3870 \n",
      "0m 50s (- 1m 1s) (450 45%) 0.3770 \n",
      "0m 54s (- 0m 54s) (500 50%) 0.4302 5.50%\n",
      "1m 3s (- 0m 51s) (550 55%) 0.5413 \n",
      "1m 8s (- 0m 45s) (600 60%) 0.4528 \n",
      "1m 12s (- 0m 39s) (650 65%) 0.5108 \n",
      "1m 18s (- 0m 33s) (700 70%) 0.4360 \n",
      "1m 22s (- 0m 27s) (750 75%) 0.3269 \n",
      "1m 27s (- 0m 21s) (800 80%) 0.3529 \n",
      "1m 32s (- 0m 16s) (850 85%) 0.5202 \n",
      "1m 37s (- 0m 10s) (900 90%) 0.3848 \n",
      "1m 42s (- 0m 5s) (950 95%) 0.4396 \n",
      "1m 47s (- 0m 0s) (1000 100%) 0.5019 12.00%\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 10.60%\n",
      "0m 12s (- 4m 0s) (50 5%) 0.5040 \n",
      "0m 15s (- 2m 21s) (100 10%) 0.4421 \n",
      "0m 19s (- 1m 47s) (150 15%) 0.4616 \n",
      "0m 22s (- 1m 28s) (200 20%) 0.6345 \n",
      "0m 25s (- 1m 15s) (250 25%) 0.4463 \n",
      "0m 28s (- 1m 6s) (300 30%) 0.4816 \n",
      "0m 31s (- 0m 59s) (350 35%) 0.5444 \n",
      "0m 34s (- 0m 52s) (400 40%) 0.4340 \n",
      "0m 38s (- 0m 46s) (450 45%) 0.3638 \n",
      "0m 41s (- 0m 41s) (500 50%) 0.4350 9.00%\n",
      "0m 49s (- 0m 40s) (550 55%) 0.4330 \n",
      "0m 51s (- 0m 34s) (600 60%) 0.4733 \n",
      "0m 54s (- 0m 29s) (650 65%) 0.4028 \n",
      "0m 58s (- 0m 25s) (700 70%) 0.4733 \n",
      "1m 1s (- 0m 20s) (750 75%) 0.3322 \n",
      "1m 5s (- 0m 16s) (800 80%) 0.4213 \n",
      "1m 8s (- 0m 12s) (850 85%) 0.4040 \n",
      "1m 11s (- 0m 7s) (900 90%) 0.3955 \n",
      "1m 15s (- 0m 3s) (950 95%) 0.3665 \n",
      "1m 19s (- 0m 0s) (1000 100%) 0.4379 16.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 15.60%\n",
      "0m 12s (- 3m 58s) (50 5%) 0.4148 \n",
      "0m 15s (- 2m 20s) (100 10%) 0.3887 \n",
      "0m 18s (- 1m 46s) (150 15%) 0.3530 \n",
      "0m 22s (- 1m 28s) (200 20%) 0.4633 \n",
      "0m 25s (- 1m 16s) (250 25%) 0.4489 \n",
      "0m 28s (- 1m 6s) (300 30%) 0.3755 \n",
      "0m 31s (- 0m 58s) (350 35%) 0.3400 \n",
      "0m 34s (- 0m 51s) (400 40%) 0.3629 \n",
      "0m 38s (- 0m 46s) (450 45%) 0.4588 \n",
      "0m 41s (- 0m 41s) (500 50%) 0.4215 15.50%\n",
      "0m 47s (- 0m 38s) (550 55%) 0.3727 \n",
      "0m 51s (- 0m 34s) (600 60%) 0.4441 \n",
      "0m 54s (- 0m 29s) (650 65%) 0.3512 \n",
      "0m 57s (- 0m 24s) (700 70%) 0.3028 \n",
      "1m 1s (- 0m 20s) (750 75%) 0.3797 \n",
      "1m 4s (- 0m 16s) (800 80%) 0.3150 \n",
      "1m 7s (- 0m 11s) (850 85%) 0.4082 \n",
      "1m 11s (- 0m 7s) (900 90%) 0.3635 \n",
      "1m 14s (- 0m 3s) (950 95%) 0.3929 \n",
      "1m 18s (- 0m 0s) (1000 100%) 0.4602 9.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 12.40%\n",
      "0m 11s (- 3m 39s) (50 5%) 0.4174 \n",
      "0m 15s (- 2m 15s) (100 10%) 0.4224 \n",
      "0m 18s (- 1m 45s) (150 15%) 0.4317 \n",
      "0m 21s (- 1m 27s) (200 20%) 0.3479 \n",
      "0m 25s (- 1m 15s) (250 25%) 0.4895 \n",
      "0m 28s (- 1m 6s) (300 30%) 0.4067 \n",
      "0m 31s (- 0m 58s) (350 35%) 0.4090 \n",
      "0m 34s (- 0m 52s) (400 40%) 0.4451 \n",
      "0m 37s (- 0m 46s) (450 45%) 0.4318 \n",
      "0m 40s (- 0m 40s) (500 50%) 0.3709 17.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 47s (- 0m 38s) (550 55%) 0.4098 \n",
      "0m 50s (- 0m 33s) (600 60%) 0.4016 \n",
      "0m 53s (- 0m 28s) (650 65%) 0.3762 \n",
      "0m 56s (- 0m 24s) (700 70%) 0.3307 \n",
      "1m 0s (- 0m 20s) (750 75%) 0.3702 \n",
      "1m 4s (- 0m 16s) (800 80%) 0.4648 \n",
      "1m 7s (- 0m 11s) (850 85%) 0.3732 \n",
      "1m 11s (- 0m 7s) (900 90%) 0.3948 \n",
      "1m 14s (- 0m 3s) (950 95%) 0.3703 \n",
      "1m 18s (- 0m 0s) (1000 100%) 0.2973 18.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 19.60%\n",
      "0m 10s (- 3m 23s) (50 5%) 0.3135 \n",
      "0m 14s (- 2m 6s) (100 10%) 0.3340 \n",
      "0m 16s (- 1m 35s) (150 15%) 0.2837 \n",
      "0m 20s (- 1m 20s) (200 20%) 0.3610 \n",
      "0m 23s (- 1m 11s) (250 25%) 0.2773 \n",
      "0m 26s (- 1m 2s) (300 30%) 0.2549 \n",
      "0m 30s (- 0m 56s) (350 35%) 0.2719 \n",
      "0m 33s (- 0m 50s) (400 40%) 0.3024 \n",
      "0m 36s (- 0m 45s) (450 45%) 0.3166 \n",
      "0m 40s (- 0m 40s) (500 50%) 0.2565 19.50%\n",
      "0m 47s (- 0m 38s) (550 55%) 0.2573 \n",
      "0m 50s (- 0m 33s) (600 60%) 0.2567 \n",
      "0m 53s (- 0m 28s) (650 65%) 0.1881 \n",
      "0m 56s (- 0m 24s) (700 70%) 0.2806 \n",
      "0m 59s (- 0m 19s) (750 75%) 0.2576 \n",
      "1m 2s (- 0m 15s) (800 80%) 0.2561 \n",
      "1m 5s (- 0m 11s) (850 85%) 0.2130 \n",
      "1m 9s (- 0m 7s) (900 90%) 0.2192 \n",
      "1m 12s (- 0m 3s) (950 95%) 0.3000 \n",
      "1m 15s (- 0m 0s) (1000 100%) 0.2279 41.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 38.00%\n",
      "0m 10s (- 3m 23s) (50 5%) 0.2492 \n",
      "0m 13s (- 2m 3s) (100 10%) 0.2154 \n",
      "0m 16s (- 1m 33s) (150 15%) 0.2003 \n",
      "0m 19s (- 1m 18s) (200 20%) 0.3465 \n",
      "0m 22s (- 1m 8s) (250 25%) 0.2395 \n",
      "0m 26s (- 1m 0s) (300 30%) 0.2626 \n",
      "0m 29s (- 0m 54s) (350 35%) 0.2409 \n",
      "0m 32s (- 0m 48s) (400 40%) 0.2556 \n",
      "0m 35s (- 0m 43s) (450 45%) 0.1621 \n",
      "0m 38s (- 0m 38s) (500 50%) 0.1853 32.00%\n",
      "0m 44s (- 0m 36s) (550 55%) 0.2213 \n",
      "0m 48s (- 0m 32s) (600 60%) 0.2072 \n",
      "0m 51s (- 0m 27s) (650 65%) 0.1681 \n",
      "0m 55s (- 0m 23s) (700 70%) 0.1462 \n",
      "0m 58s (- 0m 19s) (750 75%) 0.1631 \n",
      "1m 1s (- 0m 15s) (800 80%) 0.2741 \n",
      "1m 5s (- 0m 11s) (850 85%) 0.2427 \n",
      "1m 8s (- 0m 7s) (900 90%) 0.2375 \n",
      "1m 11s (- 0m 3s) (950 95%) 0.2292 \n",
      "1m 15s (- 0m 0s) (1000 100%) 0.2078 47.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 47.00%\n",
      "0m 10s (- 3m 24s) (50 5%) 0.1710 \n",
      "0m 14s (- 2m 7s) (100 10%) 0.1293 \n",
      "0m 17s (- 1m 37s) (150 15%) 0.1526 \n",
      "0m 20s (- 1m 21s) (200 20%) 0.1156 \n",
      "0m 23s (- 1m 10s) (250 25%) 0.1291 \n",
      "0m 26s (- 1m 2s) (300 30%) 0.2149 \n",
      "0m 30s (- 0m 55s) (350 35%) 0.1689 \n",
      "0m 33s (- 0m 50s) (400 40%) 0.1578 \n",
      "0m 37s (- 0m 45s) (450 45%) 0.1050 \n",
      "0m 39s (- 0m 39s) (500 50%) 0.1290 54.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 46s (- 0m 38s) (550 55%) 0.1098 \n",
      "0m 50s (- 0m 33s) (600 60%) 0.2137 \n",
      "0m 53s (- 0m 28s) (650 65%) 0.1018 \n",
      "0m 56s (- 0m 24s) (700 70%) 0.1097 \n",
      "1m 0s (- 0m 20s) (750 75%) 0.1432 \n",
      "1m 3s (- 0m 15s) (800 80%) 0.1731 \n",
      "1m 7s (- 0m 11s) (850 85%) 0.0924 \n",
      "1m 10s (- 0m 7s) (900 90%) 0.1173 \n",
      "1m 13s (- 0m 3s) (950 95%) 0.0984 \n",
      "1m 17s (- 0m 0s) (1000 100%) 0.0882 65.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 65.60%\n",
      "0m 10s (- 3m 10s) (50 5%) 0.3040 \n",
      "0m 12s (- 1m 56s) (100 10%) 0.1473 \n",
      "0m 16s (- 1m 33s) (150 15%) 0.1476 \n",
      "0m 19s (- 1m 19s) (200 20%) 0.1238 \n",
      "0m 23s (- 1m 9s) (250 25%) 0.1289 \n",
      "0m 26s (- 1m 1s) (300 30%) 0.1352 \n",
      "0m 29s (- 0m 55s) (350 35%) 0.0878 \n",
      "0m 33s (- 0m 49s) (400 40%) 0.1366 \n",
      "0m 36s (- 0m 44s) (450 45%) 0.0922 \n",
      "0m 39s (- 0m 39s) (500 50%) 0.0961 66.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 45s (- 0m 37s) (550 55%) 0.1116 \n",
      "0m 48s (- 0m 32s) (600 60%) 0.0782 \n",
      "0m 51s (- 0m 27s) (650 65%) 0.0799 \n",
      "0m 54s (- 0m 23s) (700 70%) 0.1063 \n",
      "0m 57s (- 0m 19s) (750 75%) 0.0846 \n",
      "1m 1s (- 0m 15s) (800 80%) 0.0765 \n",
      "1m 4s (- 0m 11s) (850 85%) 0.0897 \n",
      "1m 8s (- 0m 7s) (900 90%) 0.0961 \n",
      "1m 11s (- 0m 3s) (950 95%) 0.0900 \n",
      "1m 14s (- 0m 0s) (1000 100%) 0.0931 65.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 67.80%\n",
      "0m 12s (- 4m 4s) (50 5%) 0.1109 \n",
      "0m 18s (- 2m 46s) (100 10%) 0.0572 \n",
      "0m 24s (- 2m 16s) (150 15%) 0.1216 \n",
      "0m 29s (- 1m 58s) (200 20%) 0.0911 \n",
      "0m 36s (- 1m 48s) (250 25%) 0.0792 \n",
      "0m 41s (- 1m 36s) (300 30%) 0.1087 \n",
      "0m 47s (- 1m 27s) (350 35%) 0.0978 \n",
      "0m 51s (- 1m 17s) (400 40%) 0.1062 \n",
      "0m 56s (- 1m 9s) (450 45%) 0.0618 \n",
      "1m 1s (- 1m 1s) (500 50%) 0.1198 67.00%\n",
      "1m 10s (- 0m 57s) (550 55%) 0.1716 \n",
      "1m 14s (- 0m 49s) (600 60%) 0.0631 \n",
      "1m 19s (- 0m 42s) (650 65%) 0.1456 \n",
      "1m 24s (- 0m 36s) (700 70%) 0.0785 \n",
      "1m 30s (- 0m 30s) (750 75%) 0.1053 \n",
      "1m 35s (- 0m 23s) (800 80%) 0.1415 \n",
      "1m 43s (- 0m 18s) (850 85%) 0.0777 \n",
      "1m 50s (- 0m 12s) (900 90%) 0.0749 \n",
      "1m 56s (- 0m 6s) (950 95%) 0.0810 \n",
      "2m 1s (- 0m 0s) (1000 100%) 0.0740 72.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 76.20%\n",
      "0m 10s (- 3m 14s) (50 5%) 0.0545 \n",
      "0m 13s (- 2m 3s) (100 10%) 0.0679 \n",
      "0m 16s (- 1m 35s) (150 15%) 0.0638 \n",
      "0m 20s (- 1m 20s) (200 20%) 0.1739 \n",
      "0m 23s (- 1m 9s) (250 25%) 0.0644 \n",
      "0m 26s (- 1m 2s) (300 30%) 0.0681 \n",
      "0m 30s (- 0m 56s) (350 35%) 0.0525 \n",
      "0m 33s (- 0m 50s) (400 40%) 0.0829 \n",
      "0m 37s (- 0m 45s) (450 45%) 0.0930 \n",
      "0m 40s (- 0m 40s) (500 50%) 0.0723 78.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 47s (- 0m 38s) (550 55%) 0.0406 \n",
      "0m 51s (- 0m 34s) (600 60%) 0.0512 \n",
      "0m 54s (- 0m 29s) (650 65%) 0.0586 \n",
      "0m 57s (- 0m 24s) (700 70%) 0.0619 \n",
      "1m 0s (- 0m 20s) (750 75%) 0.0688 \n",
      "1m 4s (- 0m 16s) (800 80%) 0.0478 \n",
      "1m 7s (- 0m 11s) (850 85%) 0.0619 \n",
      "1m 11s (- 0m 7s) (900 90%) 0.1190 \n",
      "1m 14s (- 0m 3s) (950 95%) 0.0672 \n",
      "1m 18s (- 0m 0s) (1000 100%) 0.0840 78.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 78.20%\n",
      "0m 10s (- 3m 21s) (50 5%) 0.0706 \n",
      "0m 13s (- 2m 5s) (100 10%) 0.0466 \n",
      "0m 17s (- 1m 37s) (150 15%) 0.1047 \n",
      "0m 20s (- 1m 21s) (200 20%) 0.0674 \n",
      "0m 23s (- 1m 11s) (250 25%) 0.0645 \n",
      "0m 26s (- 1m 2s) (300 30%) 0.0377 \n",
      "0m 30s (- 0m 55s) (350 35%) 0.1067 \n",
      "0m 33s (- 0m 50s) (400 40%) 0.0631 \n",
      "0m 37s (- 0m 45s) (450 45%) 0.0721 \n",
      "0m 40s (- 0m 40s) (500 50%) 0.0609 81.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 46s (- 0m 38s) (550 55%) 0.0748 \n",
      "0m 50s (- 0m 33s) (600 60%) 0.0523 \n",
      "0m 53s (- 0m 29s) (650 65%) 0.0454 \n",
      "0m 57s (- 0m 24s) (700 70%) 0.0466 \n",
      "1m 0s (- 0m 20s) (750 75%) 0.1134 \n",
      "1m 4s (- 0m 16s) (800 80%) 0.0669 \n",
      "1m 7s (- 0m 11s) (850 85%) 0.1493 \n",
      "1m 11s (- 0m 7s) (900 90%) 0.0557 \n",
      "1m 14s (- 0m 3s) (950 95%) 0.0483 \n",
      "1m 17s (- 0m 0s) (1000 100%) 0.0540 75.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 79.40%\n",
      "0m 11s (- 3m 45s) (50 5%) 0.0899 \n",
      "0m 15s (- 2m 18s) (100 10%) 0.0499 \n",
      "0m 18s (- 1m 44s) (150 15%) 0.0441 \n",
      "0m 21s (- 1m 27s) (200 20%) 0.0726 \n",
      "0m 25s (- 1m 17s) (250 25%) 0.1029 \n",
      "0m 29s (- 1m 8s) (300 30%) 0.0376 \n",
      "0m 32s (- 1m 0s) (350 35%) 0.0749 \n",
      "0m 35s (- 0m 53s) (400 40%) 0.0751 \n",
      "0m 39s (- 0m 48s) (450 45%) 0.0497 \n",
      "0m 42s (- 0m 42s) (500 50%) 0.0381 81.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 49s (- 0m 40s) (550 55%) 0.0521 \n",
      "0m 52s (- 0m 35s) (600 60%) 0.0504 \n",
      "0m 56s (- 0m 30s) (650 65%) 0.1254 \n",
      "0m 59s (- 0m 25s) (700 70%) 0.0463 \n",
      "1m 3s (- 0m 21s) (750 75%) 0.0389 \n",
      "1m 6s (- 0m 16s) (800 80%) 0.0448 \n",
      "1m 9s (- 0m 12s) (850 85%) 0.0664 \n",
      "1m 13s (- 0m 8s) (900 90%) 0.0465 \n",
      "1m 16s (- 0m 4s) (950 95%) 0.1186 \n",
      "1m 20s (- 0m 0s) (1000 100%) 0.0349 87.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 83.60%\n",
      "0m 11s (- 3m 46s) (50 5%) 0.1510 \n",
      "0m 17s (- 2m 37s) (100 10%) 0.0343 \n",
      "0m 23s (- 2m 10s) (150 15%) 0.0724 \n",
      "0m 27s (- 1m 50s) (200 20%) 0.0492 \n",
      "0m 33s (- 1m 39s) (250 25%) 0.0383 \n",
      "0m 38s (- 1m 29s) (300 30%) 0.0704 \n",
      "0m 43s (- 1m 20s) (350 35%) 0.0745 \n",
      "0m 49s (- 1m 14s) (400 40%) 0.0785 \n",
      "0m 54s (- 1m 6s) (450 45%) 0.0571 \n",
      "0m 57s (- 0m 57s) (500 50%) 0.0434 86.50%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 4s (- 0m 52s) (550 55%) 0.0455 \n",
      "1m 6s (- 0m 44s) (600 60%) 0.0457 \n",
      "1m 10s (- 0m 37s) (650 65%) 0.0527 \n",
      "1m 13s (- 0m 31s) (700 70%) 0.0436 \n",
      "1m 16s (- 0m 25s) (750 75%) 0.0526 \n",
      "1m 20s (- 0m 20s) (800 80%) 0.0519 \n",
      "1m 23s (- 0m 14s) (850 85%) 0.0430 \n",
      "1m 26s (- 0m 9s) (900 90%) 0.0443 \n",
      "1m 30s (- 0m 4s) (950 95%) 0.1227 \n",
      "1m 33s (- 0m 0s) (1000 100%) 0.0693 82.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 82.80%\n",
      "0m 10s (- 3m 19s) (50 5%) 0.0420 \n",
      "0m 14s (- 2m 6s) (100 10%) 0.0732 \n",
      "0m 17s (- 1m 37s) (150 15%) 0.0625 \n",
      "0m 20s (- 1m 21s) (200 20%) 0.0779 \n",
      "0m 24s (- 1m 12s) (250 25%) 0.0350 \n",
      "0m 27s (- 1m 3s) (300 30%) 0.0360 \n",
      "0m 30s (- 0m 56s) (350 35%) 0.0336 \n",
      "0m 33s (- 0m 50s) (400 40%) 0.0347 \n",
      "0m 37s (- 0m 45s) (450 45%) 0.0484 \n",
      "0m 40s (- 0m 40s) (500 50%) 0.0529 76.50%\n",
      "0m 46s (- 0m 38s) (550 55%) 0.0331 \n",
      "0m 50s (- 0m 33s) (600 60%) 0.0791 \n",
      "0m 53s (- 0m 28s) (650 65%) 0.0445 \n",
      "0m 56s (- 0m 24s) (700 70%) 0.0436 \n",
      "1m 0s (- 0m 20s) (750 75%) 0.0241 \n",
      "1m 4s (- 0m 16s) (800 80%) 0.0468 \n",
      "1m 10s (- 0m 12s) (850 85%) 0.0320 \n",
      "1m 16s (- 0m 8s) (900 90%) 0.0414 \n",
      "1m 22s (- 0m 4s) (950 95%) 0.0460 \n",
      "1m 27s (- 0m 0s) (1000 100%) 0.0685 85.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 85.20%\n",
      "0m 11s (- 3m 30s) (50 5%) 0.0499 \n",
      "0m 15s (- 2m 20s) (100 10%) 0.0294 \n",
      "0m 18s (- 1m 47s) (150 15%) 0.0349 \n",
      "0m 22s (- 1m 29s) (200 20%) 0.0820 \n",
      "0m 25s (- 1m 17s) (250 25%) 0.0819 \n",
      "0m 29s (- 1m 8s) (300 30%) 0.0286 \n",
      "0m 32s (- 1m 0s) (350 35%) 0.0530 \n",
      "0m 35s (- 0m 53s) (400 40%) 0.0284 \n",
      "0m 39s (- 0m 48s) (450 45%) 0.0458 \n",
      "0m 43s (- 0m 43s) (500 50%) 0.0331 84.50%\n",
      "0m 49s (- 0m 40s) (550 55%) 0.0513 \n",
      "0m 53s (- 0m 35s) (600 60%) 0.0320 \n",
      "0m 56s (- 0m 30s) (650 65%) 0.0196 \n",
      "0m 59s (- 0m 25s) (700 70%) 0.0287 \n",
      "1m 3s (- 0m 21s) (750 75%) 0.0323 \n",
      "1m 6s (- 0m 16s) (800 80%) 0.0356 \n",
      "1m 10s (- 0m 12s) (850 85%) 0.0590 \n",
      "1m 13s (- 0m 8s) (900 90%) 0.0200 \n",
      "1m 16s (- 0m 4s) (950 95%) 0.0235 \n",
      "1m 19s (- 0m 0s) (1000 100%) 0.0317 89.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 84.00%\n",
      "0m 11s (- 3m 44s) (50 5%) 0.0315 \n",
      "0m 15s (- 2m 23s) (100 10%) 0.0291 \n",
      "0m 19s (- 1m 50s) (150 15%) 0.0315 \n",
      "0m 23s (- 1m 35s) (200 20%) 0.0288 \n",
      "0m 27s (- 1m 22s) (250 25%) 0.0208 \n",
      "0m 30s (- 1m 12s) (300 30%) 0.0322 \n",
      "0m 34s (- 1m 3s) (350 35%) 0.0279 \n",
      "0m 37s (- 0m 56s) (400 40%) 0.0566 \n",
      "0m 41s (- 0m 50s) (450 45%) 0.0421 \n",
      "0m 44s (- 0m 44s) (500 50%) 0.0250 82.50%\n",
      "0m 51s (- 0m 41s) (550 55%) 0.0236 \n",
      "0m 54s (- 0m 36s) (600 60%) 0.0788 \n",
      "0m 57s (- 0m 31s) (650 65%) 0.0255 \n",
      "1m 1s (- 0m 26s) (700 70%) 0.0219 \n",
      "1m 4s (- 0m 21s) (750 75%) 0.0409 \n",
      "1m 7s (- 0m 16s) (800 80%) 0.0365 \n",
      "1m 11s (- 0m 12s) (850 85%) 0.0433 \n",
      "1m 14s (- 0m 8s) (900 90%) 0.1026 \n",
      "1m 18s (- 0m 4s) (950 95%) 0.0254 \n",
      "1m 21s (- 0m 0s) (1000 100%) 0.0350 89.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 86.20%\n",
      "0m 10s (- 3m 22s) (50 5%) 0.0298 \n",
      "0m 14s (- 2m 7s) (100 10%) 0.0551 \n",
      "0m 17s (- 1m 39s) (150 15%) 0.0286 \n",
      "0m 21s (- 1m 24s) (200 20%) 0.0217 \n",
      "0m 24s (- 1m 12s) (250 25%) 0.0567 \n",
      "0m 27s (- 1m 3s) (300 30%) 0.0374 \n",
      "0m 30s (- 0m 57s) (350 35%) 0.0187 \n",
      "0m 34s (- 0m 51s) (400 40%) 0.0219 \n",
      "0m 37s (- 0m 45s) (450 45%) 0.0279 \n",
      "0m 40s (- 0m 40s) (500 50%) 0.1717 89.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 47s (- 0m 38s) (550 55%) 0.0301 \n",
      "0m 50s (- 0m 33s) (600 60%) 0.0405 \n",
      "0m 53s (- 0m 28s) (650 65%) 0.0730 \n",
      "0m 56s (- 0m 24s) (700 70%) 0.0494 \n",
      "1m 0s (- 0m 20s) (750 75%) 0.0145 \n",
      "1m 3s (- 0m 15s) (800 80%) 0.0297 \n",
      "1m 6s (- 0m 11s) (850 85%) 0.0392 \n",
      "1m 10s (- 0m 7s) (900 90%) 0.1606 \n",
      "1m 13s (- 0m 3s) (950 95%) 0.0261 \n",
      "1m 16s (- 0m 0s) (1000 100%) 0.0235 92.50%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 91.20%\n",
      "0m 10s (- 3m 14s) (50 5%) 0.0297 \n",
      "0m 13s (- 2m 4s) (100 10%) 0.0380 \n",
      "0m 16s (- 1m 35s) (150 15%) 0.1087 \n",
      "0m 20s (- 1m 22s) (200 20%) 0.0187 \n",
      "0m 23s (- 1m 11s) (250 25%) 0.0653 \n",
      "0m 26s (- 1m 2s) (300 30%) 0.0328 \n",
      "0m 29s (- 0m 55s) (350 35%) 0.0186 \n",
      "0m 33s (- 0m 49s) (400 40%) 0.1069 \n",
      "0m 36s (- 0m 44s) (450 45%) 0.0283 \n",
      "0m 40s (- 0m 40s) (500 50%) 0.0357 94.50%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 46s (- 0m 37s) (550 55%) 0.0383 \n",
      "0m 49s (- 0m 33s) (600 60%) 0.0294 \n",
      "0m 53s (- 0m 28s) (650 65%) 0.0522 \n",
      "0m 56s (- 0m 24s) (700 70%) 0.0463 \n",
      "0m 59s (- 0m 19s) (750 75%) 0.0224 \n",
      "1m 2s (- 0m 15s) (800 80%) 0.0259 \n",
      "1m 6s (- 0m 11s) (850 85%) 0.0217 \n",
      "1m 9s (- 0m 7s) (900 90%) 0.0387 \n",
      "1m 12s (- 0m 3s) (950 95%) 0.0247 \n",
      "1m 16s (- 0m 0s) (1000 100%) 0.0224 88.50%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 89.00%\n",
      "0m 10s (- 3m 12s) (50 5%) 0.0151 \n",
      "0m 13s (- 2m 2s) (100 10%) 0.0146 \n",
      "0m 17s (- 1m 38s) (150 15%) 0.0244 \n",
      "0m 20s (- 1m 23s) (200 20%) 0.0313 \n",
      "0m 24s (- 1m 12s) (250 25%) 0.0262 \n",
      "0m 27s (- 1m 4s) (300 30%) 0.0173 \n",
      "0m 31s (- 0m 57s) (350 35%) 0.0223 \n",
      "0m 34s (- 0m 51s) (400 40%) 0.0244 \n",
      "0m 37s (- 0m 45s) (450 45%) 0.0184 \n",
      "0m 41s (- 0m 41s) (500 50%) 0.0348 91.00%\n",
      "New best test accuracy! Model Updated!\n",
      "0m 47s (- 0m 38s) (550 55%) 0.0186 \n",
      "0m 50s (- 0m 33s) (600 60%) 0.0150 \n",
      "0m 54s (- 0m 29s) (650 65%) 0.0235 \n",
      "0m 57s (- 0m 24s) (700 70%) 0.0515 \n",
      "1m 1s (- 0m 20s) (750 75%) 0.0264 \n",
      "1m 4s (- 0m 16s) (800 80%) 0.0243 \n",
      "1m 7s (- 0m 11s) (850 85%) 0.0266 \n",
      "1m 11s (- 0m 7s) (900 90%) 0.0883 \n",
      "1m 14s (- 0m 3s) (950 95%) 0.0199 \n",
      "1m 18s (- 0m 0s) (1000 100%) 0.0186 95.00%\n",
      "New best test accuracy! Model Updated!\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 1000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation accuracy: 94.20%\n",
      "0m 17s (- 11m 23s) (50 2%) 0.0659 \n",
      "0m 22s (- 7m 12s) (100 5%) 0.0167 \n",
      "0m 27s (- 5m 42s) (150 7%) 0.0387 \n",
      "0m 31s (- 4m 45s) (200 10%) 0.0241 \n",
      "0m 35s (- 4m 6s) (250 12%) 0.0190 \n",
      "0m 38s (- 3m 37s) (300 15%) 0.0128 \n",
      "0m 42s (- 3m 18s) (350 17%) 0.0161 \n",
      "0m 45s (- 3m 2s) (400 20%) 0.0135 \n",
      "0m 49s (- 2m 49s) (450 22%) 0.0215 \n",
      "0m 52s (- 2m 38s) (500 25%) 0.0359 95.00%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 2s (- 2m 45s) (550 27%) 0.0276 \n",
      "1m 8s (- 2m 40s) (600 30%) 0.0195 \n",
      "1m 13s (- 2m 32s) (650 32%) 0.0350 \n",
      "1m 17s (- 2m 23s) (700 35%) 0.0115 \n",
      "1m 22s (- 2m 17s) (750 37%) 0.0160 \n",
      "1m 26s (- 2m 9s) (800 40%) 0.0248 \n",
      "1m 31s (- 2m 3s) (850 42%) 0.0615 \n",
      "1m 35s (- 1m 57s) (900 45%) 0.0832 \n",
      "1m 40s (- 1m 50s) (950 47%) 0.0383 \n",
      "1m 45s (- 1m 45s) (1000 50%) 0.0250 97.00%\n",
      "New best test accuracy! Model Updated!\n",
      "1m 54s (- 1m 43s) (1050 52%) 0.0843 \n",
      "1m 58s (- 1m 36s) (1100 55%) 0.0236 \n",
      "2m 2s (- 1m 30s) (1150 57%) 0.0241 \n",
      "2m 8s (- 1m 25s) (1200 60%) 0.0171 \n",
      "2m 12s (- 1m 19s) (1250 62%) 0.0183 \n",
      "2m 15s (- 1m 13s) (1300 65%) 0.0132 \n",
      "2m 18s (- 1m 6s) (1350 67%) 0.0299 \n",
      "2m 22s (- 1m 1s) (1400 70%) 0.0160 \n",
      "2m 25s (- 0m 55s) (1450 72%) 0.0141 \n",
      "2m 29s (- 0m 49s) (1500 75%) 0.0152 94.50%\n",
      "2m 37s (- 0m 45s) (1550 77%) 0.0134 \n",
      "2m 41s (- 0m 40s) (1600 80%) 0.0248 \n",
      "2m 45s (- 0m 35s) (1650 82%) 0.0158 \n",
      "2m 48s (- 0m 29s) (1700 85%) 0.0165 \n",
      "2m 52s (- 0m 24s) (1750 87%) 0.0345 \n",
      "2m 55s (- 0m 19s) (1800 90%) 0.0168 \n",
      "2m 59s (- 0m 14s) (1850 92%) 0.0492 \n",
      "3m 2s (- 0m 9s) (1900 95%) 0.0174 \n",
      "3m 5s (- 0m 4s) (1950 97%) 0.0316 \n",
      "3m 9s (- 0m 0s) (2000 100%) 0.0316 93.00%\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 2000, print_every=50, eval_every=500, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93999999999999995"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"saved_models/encoder_\" + MODEL_VERSION):\n",
    "    encoder2 = torch.load(\"saved_models/encoder_\" + MODEL_VERSION)\n",
    "    decoder2 = torch.load(\"saved_models/decoder_\" + MODEL_VERSION)\n",
    "evaluateAccuracy(encoder2, decoder2, n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> look opposite right twice and jump opposite left twice\n",
      "= I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP\n",
      "< I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_JUMP <EOS>\n",
      "\n",
      "> turn around right twice after jump left\n",
      "= I_TURN_LEFT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT\n",
      "< I_TURN_LEFT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT <EOS>\n",
      "\n",
      "> jump left twice and look opposite right thrice\n",
      "= I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_LOOK\n",
      "< I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_LOOK <EOS>\n",
      "\n",
      "> turn opposite left twice after jump twice\n",
      "= I_JUMP I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT\n",
      "< I_JUMP I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT <EOS>\n",
      "\n",
      "> walk opposite left after jump thrice\n",
      "= I_JUMP I_JUMP I_JUMP I_TURN_LEFT I_TURN_LEFT I_WALK\n",
      "< I_JUMP I_JUMP I_JUMP I_TURN_LEFT I_TURN_LEFT I_WALK <EOS>\n",
      "\n",
      "> walk opposite left twice after jump opposite left\n",
      "= I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_WALK I_TURN_LEFT I_TURN_LEFT I_WALK\n",
      "< I_TURN_LEFT I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_WALK I_TURN_LEFT I_TURN_LEFT I_WALK <EOS>\n",
      "\n",
      "> look opposite left after jump left thrice\n",
      "= I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_LOOK\n",
      "< I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_TURN_LEFT I_LOOK <EOS>\n",
      "\n",
      "> jump around right and walk around right thrice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK <EOS>\n",
      "\n",
      "> jump around right thrice and look around right twice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK <EOS>\n",
      "\n",
      "> jump around right and turn opposite left thrice\n",
      "= I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT\n",
      "< I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
